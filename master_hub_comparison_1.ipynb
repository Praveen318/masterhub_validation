{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90698c26-0416-41da-acb1-fc0e6546f5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 09:59:34,040 - INFO - Slack client initialized successfully.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "2024-11-28 09:59:34,599 - INFO - BigQuery client initialized successfully.\n",
      "2024-11-28 09:59:35,816 - INFO - Found 174 tables in dataset 'Impetus_dev_prod'.\n",
      "2024-11-28 09:59:35,818 - INFO - Identified 2 common base names with 'master_hub_' and other specified prefixes.\n",
      "2024-11-28 09:59:35,818 - INFO - Starting comparison for base table 'supplier': 'master_hub_supplier' vs 'procuro_supplier'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-28 09:59:37,401 - INFO - Loaded data from table 'master_hub_supplier' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1650\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \u001b[38;5;66;03m# Optionally, exit the script with a non-zero status\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m         sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1650\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[33], line 1565\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1562\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget table with prefix \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for base table \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found. Skipping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 1565\u001b[0m comparison_result \u001b[38;5;241m=\u001b[39m compare_tables(\n\u001b[1;32m   1566\u001b[0m     client, \n\u001b[1;32m   1567\u001b[0m     DATASET_ID, \n\u001b[1;32m   1568\u001b[0m     base_name, \n\u001b[1;32m   1569\u001b[0m     master_table, \n\u001b[1;32m   1570\u001b[0m     target_table, \n\u001b[1;32m   1571\u001b[0m     master_key, \n\u001b[1;32m   1572\u001b[0m     target_key,  \u001b[38;5;66;03m# Pass the correct target_key per prefix\u001b[39;00m\n\u001b[1;32m   1573\u001b[0m     column_mapping  \u001b[38;5;66;03m# Pass the column_mapping\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m )\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m comparison_result:\n\u001b[1;32m   1576\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mappend(comparison_result)\n",
      "Cell \u001b[0;32mIn[33], line 1222\u001b[0m, in \u001b[0;36mcompare_tables\u001b[0;34m(client, dataset_name, base_name, master_table, target_table, master_key, target_key, column_mapping)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m df_master \u001b[38;5;241m=\u001b[39m load_table_from_bigquery(client, dataset_name, master_table)\n\u001b[0;32m-> 1222\u001b[0m df_target \u001b[38;5;241m=\u001b[39m load_table_from_bigquery(client, dataset_name, target_table)\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;66;03m# Apply standardization\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m df_master \u001b[38;5;241m=\u001b[39m standardize_dataframe(df_master, exclude_columns\u001b[38;5;241m=\u001b[39m[master_key, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_active\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[33], line 261\u001b[0m, in \u001b[0;36mload_table_from_bigquery\u001b[0;34m(client, dataset_name, table_name)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 261\u001b[0m     df \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mquery(query)\u001b[38;5;241m.\u001b[39mto_dataframe()\n\u001b[1;32m    262\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded data from table \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m into DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:2058\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m \n\u001b[1;32m   1853\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;124;03m        :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m query_result \u001b[38;5;241m=\u001b[39m wait_for_query(\u001b[38;5;28mself\u001b[39m, progress_bar_type, max_results\u001b[38;5;241m=\u001b[39mmax_results)\n\u001b[0;32m-> 2058\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\u001b[38;5;241m.\u001b[39mto_dataframe(\n\u001b[1;32m   2059\u001b[0m     bqstorage_client\u001b[38;5;241m=\u001b[39mbqstorage_client,\n\u001b[1;32m   2060\u001b[0m     dtypes\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[1;32m   2061\u001b[0m     progress_bar_type\u001b[38;5;241m=\u001b[39mprogress_bar_type,\n\u001b[1;32m   2062\u001b[0m     create_bqstorage_client\u001b[38;5;241m=\u001b[39mcreate_bqstorage_client,\n\u001b[1;32m   2063\u001b[0m     geography_as_object\u001b[38;5;241m=\u001b[39mgeography_as_object,\n\u001b[1;32m   2064\u001b[0m     bool_dtype\u001b[38;5;241m=\u001b[39mbool_dtype,\n\u001b[1;32m   2065\u001b[0m     int_dtype\u001b[38;5;241m=\u001b[39mint_dtype,\n\u001b[1;32m   2066\u001b[0m     float_dtype\u001b[38;5;241m=\u001b[39mfloat_dtype,\n\u001b[1;32m   2067\u001b[0m     string_dtype\u001b[38;5;241m=\u001b[39mstring_dtype,\n\u001b[1;32m   2068\u001b[0m     date_dtype\u001b[38;5;241m=\u001b[39mdate_dtype,\n\u001b[1;32m   2069\u001b[0m     datetime_dtype\u001b[38;5;241m=\u001b[39mdatetime_dtype,\n\u001b[1;32m   2070\u001b[0m     time_dtype\u001b[38;5;241m=\u001b[39mtime_dtype,\n\u001b[1;32m   2071\u001b[0m     timestamp_dtype\u001b[38;5;241m=\u001b[39mtimestamp_dtype,\n\u001b[1;32m   2072\u001b[0m     range_date_dtype\u001b[38;5;241m=\u001b[39mrange_date_dtype,\n\u001b[1;32m   2073\u001b[0m     range_datetime_dtype\u001b[38;5;241m=\u001b[39mrange_datetime_dtype,\n\u001b[1;32m   2074\u001b[0m     range_timestamp_dtype\u001b[38;5;241m=\u001b[39mrange_timestamp_dtype,\n\u001b[1;32m   2075\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:2379\u001b[0m, in \u001b[0;36mRowIterator.to_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   2376\u001b[0m     create_bqstorage_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2377\u001b[0m     bqstorage_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2379\u001b[0m record_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_arrow(\n\u001b[1;32m   2380\u001b[0m     progress_bar_type\u001b[38;5;241m=\u001b[39mprogress_bar_type,\n\u001b[1;32m   2381\u001b[0m     bqstorage_client\u001b[38;5;241m=\u001b[39mbqstorage_client,\n\u001b[1;32m   2382\u001b[0m     create_bqstorage_client\u001b[38;5;241m=\u001b[39mcreate_bqstorage_client,\n\u001b[1;32m   2383\u001b[0m )\n\u001b[1;32m   2385\u001b[0m \u001b[38;5;66;03m# Default date dtype is `db_dtypes.DateDtype()` that could cause out of bounds error,\u001b[39;00m\n\u001b[1;32m   2386\u001b[0m \u001b[38;5;66;03m# when pyarrow converts date values to nanosecond precision. To avoid the error, we\u001b[39;00m\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;66;03m# set the date_as_object parameter to True, if necessary.\u001b[39;00m\n\u001b[1;32m   2388\u001b[0m date_as_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1946\u001b[0m, in \u001b[0;36mRowIterator.to_arrow\u001b[0;34m(self, progress_bar_type, bqstorage_client, create_bqstorage_client)\u001b[0m\n\u001b[1;32m   1941\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m get_progress_bar(\n\u001b[1;32m   1942\u001b[0m     progress_bar_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_rows, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1943\u001b[0m )\n\u001b[1;32m   1945\u001b[0m record_batches \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1946\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_arrow_iterable(\n\u001b[1;32m   1947\u001b[0m     bqstorage_client\u001b[38;5;241m=\u001b[39mbqstorage_client\n\u001b[1;32m   1948\u001b[0m ):\n\u001b[1;32m   1949\u001b[0m     record_batches\u001b[38;5;241m.\u001b[39mappend(record_batch)\n\u001b[1;32m   1951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1952\u001b[0m         \u001b[38;5;66;03m# In some cases, the number of total rows is not populated\u001b[39;00m\n\u001b[1;32m   1953\u001b[0m         \u001b[38;5;66;03m# until the first page of rows is fetched. Update the\u001b[39;00m\n\u001b[1;32m   1954\u001b[0m         \u001b[38;5;66;03m# progress bar's total to keep an accurate count.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1809\u001b[0m, in \u001b[0;36mRowIterator._to_page_iterable\u001b[0;34m(self, bqstorage_download, tabledata_list_download, bqstorage_client)\u001b[0m\n\u001b[1;32m   1802\u001b[0m     bqstorage_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1804\u001b[0m result_pages \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1805\u001b[0m     bqstorage_download()\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bqstorage_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1807\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m tabledata_list_download()\n\u001b[1;32m   1808\u001b[0m )\n\u001b[0;32m-> 1809\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m result_pages\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/_pandas_helpers.py:732\u001b[0m, in \u001b[0;36mdownload_arrow_row_iterator\u001b[0;34m(pages, bq_schema)\u001b[0m\n\u001b[1;32m    729\u001b[0m column_names \u001b[38;5;241m=\u001b[39m bq_to_arrow_schema(bq_schema) \u001b[38;5;129;01mor\u001b[39;00m [field\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m bq_schema]\n\u001b[1;32m    730\u001b[0m arrow_types \u001b[38;5;241m=\u001b[39m [bq_to_arrow_data_type(field) \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m bq_schema]\n\u001b[0;32m--> 732\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pages:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m _row_iterator_page_to_arrow(page, column_names, arrow_types)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/page_iterator.py:244\u001b[0m, in \u001b[0;36mIterator._page_iter\u001b[0;34m(self, increment)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_page_iter\u001b[39m(\u001b[38;5;28mself\u001b[39m, increment):\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generator of pages of API responses.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m        Page: each page of items from the API.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_page()\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m page \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/page_iterator.py:373\u001b[0m, in \u001b[0;36mHTTPIterator._next_page\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the next page in the iterator.\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    Optional[Page]: The next page in the iterator or :data:`None` if\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m        there are no pages left.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_next_page():\n\u001b[0;32m--> 373\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_page_response()\n\u001b[1;32m    374\u001b[0m     items \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items_key, ())\n\u001b[1;32m    375\u001b[0m     page \u001b[38;5;241m=\u001b[39m Page(\u001b[38;5;28mself\u001b[39m, items, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_to_value, raw_page\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1763\u001b[0m, in \u001b[0;36mRowIterator._get_next_page_response\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_number \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1763\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_request(\n\u001b[1;32m   1764\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_HTTP_METHOD, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, query_params\u001b[38;5;241m=\u001b[39mparams\n\u001b[1;32m   1765\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/client.py:835\u001b[0m, in \u001b[0;36mClient._call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[1;32m    831\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[1;32m    832\u001b[0m     ):\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m call()\n\u001b[0;32m--> 835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[1;32m    294\u001b[0m     target,\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[1;32m    296\u001b[0m     sleep_generator,\n\u001b[1;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[1;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[1;32m    299\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m target()\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/cloud/_http/__init__.py:482\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    479\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(data)\n\u001b[1;32m    480\u001b[0m     content_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    485\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    486\u001b[0m     content_type\u001b[38;5;241m=\u001b[39mcontent_type,\n\u001b[1;32m    487\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    488\u001b[0m     target_object\u001b[38;5;241m=\u001b[39m_target_object,\n\u001b[1;32m    489\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/cloud/_http/__init__.py:341\u001b[0m, in \u001b[0;36mJSONConnection._make_request\u001b[0;34m(self, method, url, data, content_type, headers, target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    338\u001b[0m     headers[CLIENT_INFO_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent\n\u001b[1;32m    339\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_request(\n\u001b[1;32m    342\u001b[0m     method, url, headers, data, target_object, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    343\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/cloud/_http/__init__.py:379\u001b[0m, in \u001b[0;36mJSONConnection._do_request\u001b[0;34m(self, method, url, headers, data, target_object, timeout)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_request\u001b[39m(\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, url, headers, data, target_object, timeout\u001b[38;5;241m=\u001b[39m_DEFAULT_TIMEOUT\n\u001b[1;32m    347\u001b[0m ):  \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Low-level helper:  perform the actual API request over HTTP.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    Allows batch context managers to override and defer a request.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m    :returns: The HTTP response.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    380\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, method\u001b[38;5;241m=\u001b[39mmethod, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    381\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/auth/transport/requests.py:538\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[0;32m--> 538\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(AuthorizedSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    539\u001b[0m         method,\n\u001b[1;32m    540\u001b[0m         url,\n\u001b[1;32m    541\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    542\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest_headers,\n\u001b[1;32m    543\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    545\u001b[0m     )\n\u001b[1;32m    546\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#imp and non imp column added\n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "from docx import Document\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = 'fynd-jio-impetus-prod'       # Replace with your project ID\n",
    "DATASET_ID = 'Impetus_dev_prod'                 # Replace with your dataset ID\n",
    "PREFIXES = ['procuro_', 'costing_engine_', 'scan_pack_', 'pigeon_']  # Define your prefixes\n",
    "# PROJECT_ID = 'fynd-jio-impetus-non-prod'       # Replace with your project ID\n",
    "# DATASET_ID = 'Impetus_dev_sit'                 # Replace with your dataset ID\n",
    "# PREFIXES = ['procuro_', 'costing_engine_', 'scan_pack_', 'pigeon_']  # Define your prefixes\n",
    "\n",
    "\n",
    "\n",
    "# Error log list\n",
    "ERROR_LOG_M = []\n",
    "\n",
    "# Mapping of base table names to their key columns in master and target tables\n",
    "BASE_TABLES = {\n",
    "    # 'brand': {\n",
    "    #     'master_key': 'code',\n",
    "    #     'targets': {\n",
    "    #         'procuro_': 'code',\n",
    "    #         'costing_engine_': 'code'\n",
    "    #     },\n",
    "    #     'active_filter': {\n",
    "    #         'column': 'is_active',\n",
    "    #         'value': True\n",
    "    #     },\n",
    "    #     'perform_checks': True  # Default behavior\n",
    "    # },\n",
    "    # 'brand_pm_mapping': {\n",
    "    #     'master_key': 'pm_id',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'pm_id'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    # 'brick': {\n",
    "    #     'master_key': 'brick_code',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'code'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    # 'coe_bom_element_type_mapping': {\n",
    "    #     'master_key': 'coe_name',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'coe_name'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    # 'event_log': {\n",
    "    #     'master_key': 'user_id',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'user_id'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    'supplier': {\n",
    "        'master_key': 'supplier_code',\n",
    "        'targets': {\n",
    "            'procuro_': 'supplier_code',\n",
    "            'costing_engine_': 'supplier_code'\n",
    "        },\n",
    "        'active_filter': {  # Apply active filter\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'vendor_details': {  # Newly added entry\n",
    "        'master_key': 'supplier_code',  # Using supplier_code as the key\n",
    "        'master_table': 'master_hub_supplier',  # Specify the master table explicitly\n",
    "        'targets': {\n",
    "            'scan_pack_': 'vendor_code'\n",
    "        },\n",
    "        'active_filter': {  # Apply active filter\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    # 'hsn_tax_mapping': {  # Newly added base table for HSN Codes\n",
    "    #     'master_key': 'hsn_code',  # Assuming 'hsn_code' is the key column\n",
    "    #     'master_table': 'master_hub_hsn',\n",
    "    #     'targets': {\n",
    "    #         'procuro_': 'hsn_code',\n",
    "    #     },\n",
    "    #     'perform_checks': False  # Only perform key comparisons\n",
    "    # },\n",
    "    # 'config_buyer_brand_mapping': {  # Updated entry\n",
    "    #     'master_key': 'id',\n",
    "    #     'master_table': 'master_hub_buyer_brand_mapping',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'id'\n",
    "    #     },\n",
    "    #     'column_mapping': {  # Mapping of master columns to target columns\n",
    "    #         'brand_id': 'brand_code'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },   \n",
    "}\n",
    "\n",
    "# Define Non-Important Columns\n",
    "non_imp_columns = {\n",
    "    'supplier': ['id', '_id', 'updated_at', 'created_at'],\n",
    "    'vendor_details': ['id', '_id', 'updated_at', 'created_at']  # Add if applicable\n",
    "}\n",
    "\n",
    "# Define Important Columns\n",
    "imp_columns = {\n",
    "    'brand': ['name', 'id', 'slug', 'code'],\n",
    "    'brick': ['name', 'id', 'brick_code', 'description', 'class_code'],\n",
    "    'config_buyer_brand_mapping': ['buyer_email', 'brand_code', 'id', 'buyer_id', 'is_active', 'buyer_name'],\n",
    "    'brand_pm_mapping': ['pm_id', 'brand_code', 'pm_email', 'is_active', 'pm_name', 'id'],\n",
    "    'coe_bom_element_type_mapping': ['is_active','coe_id','id','coe_name','coe_approver_email','element_type']\n",
    "    # Add more base tables and their important columns as needed\n",
    "}\n",
    "\n",
    "# Slack configuration\n",
    "SLACK_TOKEN = \"xoxb-2151238541-7946286860052-5FCcfqBPem0xKigGlIcKdLgX\"  # Replace with your Slack token\n",
    "# SLACK_CHANNEL = \"C07UN19ETK5\"  # Replace with your Slack channel ID\n",
    "SLACK_CHANNEL = \"C08310RS2PK\"\n",
    "\n",
    "\n",
    "# Initialize Slack client\n",
    "if SLACK_TOKEN and SLACK_CHANNEL:\n",
    "    slack_client = WebClient(token=SLACK_TOKEN)\n",
    "    logging.info(\"Slack client initialized successfully.\")\n",
    "else:\n",
    "    slack_client = None\n",
    "    logging.warning(\"Slack token or channel not found. Slack notifications will be disabled.\")\n",
    "\n",
    "def get_bigquery_client(project_id):\n",
    "    \"\"\"\n",
    "    Initialize and return a BigQuery client.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID.\n",
    "\n",
    "    Returns:\n",
    "        bigquery.Client: An initialized BigQuery client.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        logging.info(\"BigQuery client initialized successfully.\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize BigQuery client: {e}\")\n",
    "        raise\n",
    "\n",
    "def find_common_tables_with_master_hub(client, dataset_name, prefixes, base_tables):\n",
    "    \"\"\"\n",
    "    Find tables in the specified dataset that share the same base name after removing the 'master_hub_' prefix\n",
    "    and exist with other given prefixes.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset to search within.\n",
    "        prefixes (list): List of prefixes to compare with 'master_hub_'.\n",
    "        base_tables (dict): The BASE_TABLES dictionary containing base table configurations.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are base names and values are dictionaries showing which prefixes have tables.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Reference the dataset\n",
    "        dataset_ref = client.dataset(dataset_name)\n",
    "\n",
    "        # List all tables in the dataset\n",
    "        tables = client.list_tables(dataset_ref)\n",
    "        table_names = [table.table_id for table in tables]\n",
    "        logging.info(f\"Found {len(table_names)} tables in dataset '{dataset_name}'.\")\n",
    "\n",
    "        # Dictionary to hold base names and their corresponding tables\n",
    "        common_tables = {}\n",
    "        for base_name, config in base_tables.items():\n",
    "            # Determine the master table\n",
    "            master_table = config.get('master_table', f'master_hub_{base_name}')\n",
    "            if master_table in table_names:\n",
    "                common_tables[base_name] = {'master_hub_': master_table}\n",
    "                # Check for target tables with specified prefixes\n",
    "                for prefix, target_key in config.get('targets', {}).items():\n",
    "                    target_table = f\"{prefix}{base_name}\"\n",
    "                    if target_table in table_names:\n",
    "                        common_tables[base_name][prefix] = target_table\n",
    "            else:\n",
    "                logging.warning(f\"Master table '{master_table}' for base '{base_name}' not found in dataset.\")\n",
    "\n",
    "        # Filter out base names that only have 'master_hub_' but no other matching prefixes\n",
    "        common_tables_with_prefixes = {base_name: tables for base_name, tables in common_tables.items() if len(tables) > 1}\n",
    "\n",
    "        logging.info(f\"Identified {len(common_tables_with_prefixes)} common base names with 'master_hub_' and other specified prefixes.\")\n",
    "        return common_tables_with_prefixes\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Google API Error: {e.message}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return {}\n",
    "\n",
    "def get_table_schema(client, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Retrieve the schema of a specified BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        table_name (str): The name of the table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping column names to their data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table_ref = client.dataset(dataset_name).table(table_name)\n",
    "        table = client.get_table(table_ref)\n",
    "        schema = {field.name: field.field_type for field in table.schema}\n",
    "        logging.info(f\"Retrieved schema for table '{table_name}'.\")\n",
    "        return schema\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Failed to retrieve schema for table '{table_name}': {e.message}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while retrieving schema for table '{table_name}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def load_table_from_bigquery(client, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Load a table from BigQuery into a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        table_name (str): The name of the table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the table data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f\"SELECT * FROM `{PROJECT_ID}.{dataset_name}.{table_name}`\"\n",
    "        df = client.query(query).to_dataframe()\n",
    "        logging.info(f\"Loaded data from table '{table_name}' into DataFrame.\")\n",
    "        return df\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Failed to load table '{table_name}': {e.message}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while loading table '{table_name}': {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def standardize_dataframe(df, exclude_columns=[]):\n",
    "    \"\"\"\n",
    "    Standardize string columns in the DataFrame by stripping whitespace and converting to lowercase,\n",
    "    excluding specified columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to standardize.\n",
    "        exclude_columns (list): Columns to exclude from standardization.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Standardized DataFrame.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in df_copy.columns:\n",
    "        if col in exclude_columns:\n",
    "            continue  # Skip standardizing this column\n",
    "        # if pd.api.types.is_string_dtype(df_copy[col]):\n",
    "        df_copy[col] = df_copy[col].astype(str).str.strip().str.lower()\n",
    "    logging.info(\"Standardized DataFrame for comparison.\")\n",
    "    return df_copy\n",
    "\n",
    "def find_common_and_non_common_columns(df1, df2):\n",
    "    \"\"\"\n",
    "    Identify common and unique columns between two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df1 (pd.DataFrame): First DataFrame.\n",
    "        df2 (pd.DataFrame): Second DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (common_columns, df1_unique_columns, df2_unique_columns)\n",
    "    \"\"\"\n",
    "    common_columns = list(set(df1.columns).intersection(set(df2.columns)))\n",
    "    df1_unique_columns = list(set(df1.columns) - set(df2.columns))\n",
    "    df2_unique_columns = list(set(df2.columns) - set(df1.columns))\n",
    "    logging.info(f\"Found {len(common_columns)} common columns, {len(df1_unique_columns)} unique to first table, {len(df2_unique_columns)} unique to second table.\")\n",
    "    return common_columns, df1_unique_columns, df2_unique_columns\n",
    "\n",
    "def find_mismatches(df_master, df_target, columns_to_check, master_key, target_key, table1, table2, duplicates_master, duplicates_target, non_imp_columns, column_mapping=None):\n",
    "    \"\"\"\n",
    "    Identify mismatches between two DataFrames based on specified columns and key columns.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "        df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "        columns_to_check (list): List of columns to apply mismatch checks.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        table1 (str): Name of the source table.\n",
    "        table2 (str): Name of the target table.\n",
    "        duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "        duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "        non_imp_columns (list): List of non-important columns to exclude.\n",
    "        column_mapping (dict, optional): Mapping of master columns to target columns. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mismatches, error_logs_m)\n",
    "    \"\"\"\n",
    "    mismatches = []\n",
    "    error_logs_m = []\n",
    "    # Ensure key columns are present in both DataFrames\n",
    "    if master_key not in df_master.columns or target_key not in df_target.columns:\n",
    "        logging.error(f\"Key columns '{master_key}' or '{target_key}' not found in the respective tables.\")\n",
    "        return mismatches, error_logs_m\n",
    "\n",
    "    # Rename target key to match master key for easier comparison\n",
    "    df_target_renamed = df_target.rename(columns={target_key: master_key})\n",
    "\n",
    "    # Merge DataFrames on the master_key, excluding duplicates\n",
    "    merged_df = pd.merge(\n",
    "        df_master.drop_duplicates(subset=master_key),\n",
    "        df_target_renamed.drop_duplicates(subset=master_key),\n",
    "        on=master_key,\n",
    "        suffixes=(f'_{table1}', f'_{table2}'),\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Merged DataFrame has {len(merged_df)} records for mismatch comparison.\")\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        key = row[master_key]\n",
    "        for master_col in columns_to_check:\n",
    "            if master_col.startswith('_boltic_') or master_col in non_imp_columns:\n",
    "                continue  # Skip columns starting with '_boltic_' or non-important columns\n",
    "\n",
    "            # Determine corresponding target column\n",
    "            target_col = master_col  # Default: same name\n",
    "            if column_mapping and master_col in column_mapping:\n",
    "                target_col = column_mapping[master_col]\n",
    "\n",
    "            # Check if both columns exist in the merged DataFrame\n",
    "            master_value_col = f\"{master_col}_{table1}\"\n",
    "            target_value_col = f\"{target_col}_{table2}\"\n",
    "            if master_value_col not in row or target_value_col not in row:\n",
    "                continue  # Skip if columns not present\n",
    "\n",
    "            val_master = row.get(master_value_col)\n",
    "            val_target = row.get(target_value_col)\n",
    "\n",
    "            # Handle NaN values in comparison\n",
    "            if pd.isna(val_master) and pd.isna(val_target):\n",
    "                continue  # Both are NaN, treat as equal\n",
    "            elif pd.isna(val_master) or pd.isna(val_target) or val_master != val_target:\n",
    "                mismatch_detail = {\n",
    "                    master_key: key,\n",
    "                    'column': master_col,  # Report master column name\n",
    "                    f'{table1}_value': val_master,\n",
    "                    f'{table2}_value': val_target\n",
    "                }\n",
    "                mismatches.append(mismatch_detail)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'mismatch',\n",
    "                    'error_message': '',\n",
    "                    'source_table': table1,\n",
    "                    'target_table': table2,\n",
    "                    'issue_column': master_col,\n",
    "                    'unique_identifier': f'{master_key}: {key}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(mismatches)} mismatches between '{table1}' and '{table2}'.\")\n",
    "    return mismatches, error_logs_m\n",
    "\n",
    "def find_duplicates(df, key_column, table_name):\n",
    "    \"\"\"\n",
    "    Detect duplicate key_column entries in the DataFrame and identify differences.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to check.\n",
    "        key_column (str): The key column to check for duplicates.\n",
    "        table_name (str): Name of the table being checked.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (duplicate_records_df, error_logs_m)\n",
    "    \"\"\"\n",
    "    if key_column not in df.columns:\n",
    "        logging.error(f\"Key column '{key_column}' not found in DataFrame.\")\n",
    "        return pd.DataFrame(), []\n",
    "\n",
    "    # Get all duplicate entries (keep=False to get all duplicates)\n",
    "    duplicates_df = df[df.duplicated(subset=key_column, keep=False)]\n",
    "\n",
    "    # Group by key_column\n",
    "    grouped = duplicates_df.groupby(key_column)\n",
    "\n",
    "    duplicate_records = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    for key, group in grouped:\n",
    "        if len(group) <= 1:\n",
    "            continue  # Not a duplicate\n",
    "\n",
    "        # Drop key_column and any columns starting with '_boltic_'\n",
    "        group_non_key = group.drop(columns=[key_column] + [col for col in group.columns if col.startswith('_boltic_')])\n",
    "\n",
    "        # Check if all rows are identical\n",
    "        if group_non_key.nunique().sum() == 0:\n",
    "            difference = \"No difference exists\"\n",
    "        else:\n",
    "            # Find which columns have differences\n",
    "            cols_with_diff = group_non_key.columns[group_non_key.nunique() > 1].tolist()\n",
    "            difference = \"Difference in value of columns: \" + ', '.join(cols_with_diff)\n",
    "\n",
    "        duplicate_records.append({\n",
    "            key_column: key,\n",
    "            'Difference in value': difference\n",
    "        })\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'duplicate',\n",
    "            'error_message': f'{difference}',\n",
    "            'source_table': f'{table_name}',\n",
    "            'target_table': '',\n",
    "            'issue_column': '',\n",
    "            'unique_identifier': f'{key_column}: {key}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(duplicate_records)} duplicate entries based on '{key_column}'.\")\n",
    "    return pd.DataFrame(duplicate_records), error_logs_m\n",
    "\n",
    "def validate_data_types(schema_master, schema_target, master_key, table1_name, table2_name, columns_to_check):\n",
    "    \"\"\"\n",
    "    Compare data types of specified columns between master and target schemas.\n",
    "\n",
    "    Args:\n",
    "        schema_master (dict): Schema of the master table.\n",
    "        schema_target (dict): Schema of the target table.\n",
    "        master_key (str): The key column for reference.\n",
    "        table1_name (str): Name of the first table.\n",
    "        table2_name (str): Name of the second table.\n",
    "        columns_to_check (list): List of columns to validate data types.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (data_type_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "    data_type_issues = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    # Identify common columns to check\n",
    "    common_columns = set(columns_to_check).intersection(set(schema_master.keys()), set(schema_target.keys()))\n",
    "\n",
    "    for column in common_columns:\n",
    "        type_master = schema_master[column]\n",
    "        type_target = schema_target[column]\n",
    "        if type_master != type_target:\n",
    "            data_type_issues.append({\n",
    "                'column_name': column,\n",
    "                f'{table1_name}_data_type': type_master,\n",
    "                f'{table2_name}_data_type': type_target\n",
    "            })\n",
    "            error_detail = {\n",
    "                'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'data_type_issues',\n",
    "                'error_message': f'{table1_name}_data_type: {type_master} , {table2_name}_data_type: {type_target}',\n",
    "                'source_table': table1_name,\n",
    "                'target_table': table2_name,\n",
    "                'issue_column': column,\n",
    "                'unique_identifier': ''\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(data_type_issues)} data type issues.\")\n",
    "    return pd.DataFrame(data_type_issues), error_logs_m\n",
    "\n",
    "def validate_formats(df_master, df_target, key_column, target_key, target_table, master_table, columns_to_check):\n",
    "    \"\"\"\n",
    "    Validate specific column formats using regular expressions and include corresponding target table values.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "        master_table (str): The name of the master table.\n",
    "        columns_to_check (list): List of columns to validate formats.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (format_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "    format_issues = pd.DataFrame(columns=[key_column, 'column', 'value', 'issue', f'{target_table}_value'])\n",
    "    error_logs_m = []\n",
    "\n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "\n",
    "        # GSTIN format validation\n",
    "        if 'gstin' in columns_to_check and 'gstin' in df_master.columns:\n",
    "            gstin = str(row['gstin']).strip()\n",
    "            if not re.match(r'^[0-9]{2}[A-Z]{5}[0-9]{4}[A-Z]{1}[A-Z0-9]{3}$', gstin):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['gstin'] if 'gstin' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'gstin',\n",
    "                    'value': row['gstin'],\n",
    "                    'issue': 'Invalid GSTIN format',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Invalid GSTIN format',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'gstin',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "        # Email format validation\n",
    "        if 'email' in columns_to_check and 'email' in df_master.columns:\n",
    "            email = str(row['email']).strip()\n",
    "            if not re.match(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', email):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['email'] if 'email' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'email',\n",
    "                    'value': row['email'],\n",
    "                    'issue': 'Invalid email format',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Invalid email format',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'email',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "        # Pincode format validation\n",
    "        if 'pincode' in columns_to_check and 'pincode' in df_master.columns:\n",
    "            pincode = str(row['pincode']).strip()\n",
    "            if not re.match(r'^\\d{6}$', pincode):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['pincode'] if 'pincode' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'pincode',\n",
    "                    'value': row['pincode'],\n",
    "                    'issue': 'Pincode must be exactly 6 digits',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Pincode must be exactly 6 digits',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'pincode',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "        # Address length validation\n",
    "        if 'address' in columns_to_check and 'address' in df_master.columns:\n",
    "            address = str(row['address']).strip()\n",
    "            if len(address) > 100:\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['address'] if 'address' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'address',\n",
    "                    'value': address,\n",
    "                    'issue': 'Address exceeds 100 characters after stripping',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Address exceeds 100 characters',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'address',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(format_issues)} format issues.\")\n",
    "    return format_issues, error_logs_m\n",
    "\n",
    "\n",
    "def create_table(doc, data, column_names):\n",
    "    \"\"\"\n",
    "    Helper function to create a table in a docx document from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        data (list or list of dict): Data to populate the table.\n",
    "        column_names (list): List of column names for the table headers.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    table = doc.add_table(rows=1, cols=len(column_names))\n",
    "    table.style = 'Light List Accent 1'\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for i, col_name in enumerate(column_names):\n",
    "        hdr_cells[i].text = col_name\n",
    "\n",
    "    for row_data in data:\n",
    "        row_cells = table.add_row().cells\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            cell_value = str(row_data.get(col_name, '')).strip()\n",
    "            row_cells[i].text = cell_value\n",
    "    logging.info(\"Added table to the Word document.\")\n",
    "\n",
    "\n",
    "def add_non_matching_keys_section(doc, df1_only_keys, table1_name, df2_only_keys, table2_name, key_column_master, key_column_target):\n",
    "    \"\"\"\n",
    "    Add a section in the Word document for non-matching keys between two tables.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        df1_only_keys (list): Keys present only in table1.\n",
    "        table1_name (str): Name of the first table.\n",
    "        df2_only_keys (list): Keys present only in table2.\n",
    "        table2_name (str): Name of the second table.\n",
    "        key_column_master (str): The key column in the master table.\n",
    "        key_column_target (str): The key column in the target table.\n",
    "    \"\"\"\n",
    "    if df1_only_keys or df2_only_keys:\n",
    "        if df1_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_master}' present only in '{table1_name}' and not in '{table2_name}' ({len(df1_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_master: key[key_column_master]} for key in df1_only_keys], [key_column_master])\n",
    "        if df2_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_target}' present only in '{table2_name}' and not in '{table1_name}' ({len(df2_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_target: key[key_column_target]} for key in df2_only_keys], [key_column_target])\n",
    "    else:\n",
    "        doc.add_paragraph(\"No non-matching keys found.\")\n",
    "\n",
    "def add_table_of_contents(doc):\n",
    "    \"\"\"\n",
    "    Adds a Table of Contents to the Word document.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "    \"\"\"\n",
    "    paragraph = doc.add_paragraph()\n",
    "    run = paragraph.add_run()\n",
    "    fldChar_begin = OxmlElement('w:fldChar')  # creates a new element\n",
    "    fldChar_begin.set(qn('w:fldCharType'), 'begin')  # sets attribute on element\n",
    "    instrText = OxmlElement('w:instrText')\n",
    "    instrText.set(qn('xml:space'), 'preserve')  # sets attribute on element\n",
    "    instrText.text = 'TOC \\\\o \"1-2\" \\\\h \\\\z \\\\u'  # change to what you need\n",
    "    fldChar_separate = OxmlElement('w:fldChar')\n",
    "    fldChar_separate.set(qn('w:fldCharType'), 'separate')\n",
    "    fldChar_end = OxmlElement('w:fldChar')\n",
    "    fldChar_end.set(qn('w:fldCharType'), 'end')\n",
    "    run._r.append(fldChar_begin)\n",
    "    run._r.append(instrText)\n",
    "    run._r.append(fldChar_separate)\n",
    "    run._r.append(fldChar_end)\n",
    "    logging.info(\"Added Table of Contents to the Word document.\")\n",
    "\n",
    "def create_aggregated_document(all_results, base_name):\n",
    "    \"\"\"\n",
    "    Creates a single Word document that presents all comparison results for a base table.\n",
    "\n",
    "    Args:\n",
    "        all_results (list): List of comparison result dictionaries.\n",
    "        base_name (str): The base name of the table.\n",
    "\n",
    "    Returns:\n",
    "        str: The filepath of the saved report.\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "    doc.add_heading(f'{base_name.capitalize()} Tables Comparison Report', level=0)\n",
    "    doc.add_paragraph(f'Report generated on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "    # Add Instruction for TOC Update\n",
    "    doc.add_paragraph(\n",
    "        \"📌 **Note:** To update the Table of Contents and make the links clickable, go to the ‘References’ tab and click ‘Update Table’ or press F9 in Windows and Fn+F9 in Mac after opening this document in Microsoft Word.\",\n",
    "        style='Intense Quote'\n",
    "    )\n",
    "\n",
    "    # Add Table of Contents\n",
    "    doc.add_heading('Table of Contents', level=1)\n",
    "    add_table_of_contents(doc)\n",
    "    doc.add_page_break()\n",
    "\n",
    "    for result in all_results:\n",
    "        table1_name = result['table1_name']\n",
    "        table2_name = result['table2_name']\n",
    "        key_column_master = result['key_column_master']\n",
    "        key_column_target = result['key_column_target']\n",
    "        doc.add_heading(f'Comparison: {table1_name} vs {table2_name}', level=1)\n",
    "\n",
    "        # Mismatches\n",
    "        if result['mismatches']:\n",
    "            doc.add_heading(f'Mismatches ({len(result[\"mismatches\"])})', level=2)\n",
    "            column_names = [key_column_master, 'column', f'{table1_name}_value', f'{table2_name}_value']\n",
    "            create_table(doc, result['mismatches'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No mismatches found.\", level=2)\n",
    "\n",
    "        # Null values in master table\n",
    "        if not result['null_values_master'].empty:\n",
    "            count_null_master = len(result['null_values_master'])\n",
    "            doc.add_heading(f'Null values in {table1_name} ({count_null_master})', level=2)\n",
    "            column_names = [key_column_master, 'column', table2_name]\n",
    "            create_table(doc, result['null_values_master'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table1_name}.\", level=2)\n",
    "\n",
    "        # Null values in target table\n",
    "        if not result['null_values_target'].empty:\n",
    "            count_null_target = len(result['null_values_target'])\n",
    "            doc.add_heading(f'Null values in {table2_name} ({count_null_target})', level=2)\n",
    "            column_names = [key_column_target, 'column', table1_name]\n",
    "            create_table(doc, result['null_values_target'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Duplicate keys in master table\n",
    "        if not result['duplicates_master'].empty:\n",
    "            count_dup_master = len(result['duplicates_master'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table1_name} ({count_dup_master})', level=2)\n",
    "            create_table(doc, result['duplicates_master'].to_dict('records'), [key_column_master, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(\"No duplicate keys found in master table.\", level=2)\n",
    "\n",
    "        # Duplicate keys in target table\n",
    "        if not result['duplicates_target'].empty:\n",
    "            count_dup_target = len(result['duplicates_target'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table2_name} ({count_dup_target})', level=2)\n",
    "            create_table(doc, result['duplicates_target'].to_dict('records'), [key_column_target, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(f\"No duplicate keys found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Data type issues\n",
    "        if not result['data_type_issues'].empty:\n",
    "            count_data_type_issues = len(result['data_type_issues'])\n",
    "            doc.add_heading(f'Data Type Issues ({count_data_type_issues})', level=2)\n",
    "            column_names = ['column_name', f'{table1_name}_data_type', f'{table2_name}_data_type']\n",
    "            create_table(doc, result['data_type_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No data type issues found.\", level=2)\n",
    "\n",
    "        # Format issues in master table with target values\n",
    "        if not result['format_issues_master'].empty:\n",
    "            count_format_issues_master = len(result['format_issues_master'])\n",
    "            doc.add_heading(f'Format Issues in {table1_name} ({count_format_issues_master})', level=2)\n",
    "            column_names_master = [key_column_master, 'column', 'value', 'issue', f'{table2_name}_value']\n",
    "            create_table(doc, result['format_issues_master'].to_dict('records'), column_names_master)\n",
    "        else:\n",
    "            doc.add_heading(f\"No format issues found in {table1_name}.\", level=2)\n",
    "\n",
    "        # Pincode Mapping Issues with target details\n",
    "        if not result['pincode_mapping_issues'].empty:\n",
    "            count_pincode_issues = len(result['pincode_mapping_issues'])\n",
    "            doc.add_heading(f'Pincode Mapping Issues in {table1_name} ({count_pincode_issues})', level=2)\n",
    "            column_names = [\n",
    "                key_column_master, 'pincode', 'state', 'city', 'issue',\n",
    "                f'{table2_name}_details'\n",
    "            ]\n",
    "            create_table(doc, result['pincode_mapping_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No pincode mapping issues found.\", level=2)\n",
    "\n",
    "        # Non-matching keys in master DataFrame\n",
    "        if result['df_master_only_keys']:\n",
    "            count_master_only = len(result['df_master_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table1_name} ({count_master_only})', level=2)\n",
    "            column_names = [key_column_master]\n",
    "            create_table(doc, result['df_master_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table1_name}.\", level=2)\n",
    "\n",
    "        # Non-matching keys in target DataFrame\n",
    "        if result['df_target_only_keys']:\n",
    "            count_target_only = len(result['df_target_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table2_name} ({count_target_only})', level=2)\n",
    "            column_names = [key_column_target]\n",
    "            create_table(doc, result['df_target_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table2_name}.\", level=2)\n",
    "\n",
    "        doc.add_page_break()  # Optional: Add a page break between comparisons\n",
    "\n",
    "    # Save the aggregated document to the current directory\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_filename = f\"{base_name}_comparison_report_aggregated_{timestamp}.docx\"\n",
    "    doc.save(report_filename)\n",
    "    logging.info(f\"Saved aggregated comparison report as '{report_filename}'.\")\n",
    "\n",
    "    return report_filename  # Return the filename for further processing\n",
    "\n",
    "def send_slack_alert(message):\n",
    "    \"\"\"\n",
    "    Send a message to a specified Slack channel.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to send.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping Slack notification.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = slack_client.chat_postMessage(\n",
    "            channel=SLACK_CHANNEL,\n",
    "            text=message\n",
    "        )\n",
    "        logging.info(f\"Message sent to {SLACK_CHANNEL}: {response['ts']}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Error sending message to Slack: {e.response['error']}\")\n",
    "\n",
    "def upload_file_to_slack(filepath, title=None):\n",
    "    \"\"\"\n",
    "    Upload a file to the specified Slack channel using files_upload_v2.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the file to upload.\n",
    "        title (str, optional): The title for the uploaded file. Defaults to the file's basename.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping file upload.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            response = slack_client.files_upload_v2(\n",
    "                channel=SLACK_CHANNEL,\n",
    "                file=f,\n",
    "                filename=os.path.basename(filepath),  # Explicitly set the filename with extension\n",
    "                title=title if title else os.path.basename(filepath),  # Set the title\n",
    "                initial_comment=title if title else \"File uploaded.\"  # Optional: Add an initial comment\n",
    "            )\n",
    "\n",
    "        # Verify if the upload was successful\n",
    "        if response.get('ok'):\n",
    "            file_permalink = response['file']['permalink']\n",
    "            logging.info(f\"File uploaded to Slack channel '{SLACK_CHANNEL}': {file_permalink}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to upload file to Slack: {response}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Slack API Error during file upload: {e.response['error']}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during file upload: {e}\")\n",
    "\n",
    "def find_non_matching_keys(df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table):\n",
    "    \"\"\"\n",
    "    Identify keys present in df_master but not in df_target and vice versa, including duplicates.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame.\n",
    "        df_target (pd.DataFrame): Target DataFrame.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "        duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "        master_table (str): Name of the master table.\n",
    "        target_table (str): Name of the target table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (master_only_keys, target_only_keys, error_logs_m)\n",
    "    \"\"\"\n",
    "    error_logs_m = []\n",
    "    # Include all keys, including duplicates\n",
    "    keys_master = set(df_master[master_key].astype(str).str.strip())\n",
    "    keys_target = set(df_target[target_key].astype(str).str.strip())\n",
    "\n",
    "    # Keys present only in master\n",
    "    master_only = keys_master - keys_target\n",
    "    # Keys present only in target\n",
    "    target_only = keys_target - keys_master\n",
    "\n",
    "    logging.info(f\"Found {len(master_only)} keys in source not in target and {len(target_only)} keys in target not in source.\")\n",
    "\n",
    "    # Convert to list of dictionaries for consistency\n",
    "    master_only_keys = [{master_key: key} for key in master_only]\n",
    "    target_only_keys = [{target_key: key} for key in target_only]\n",
    "\n",
    "    # Log errors for keys only in master\n",
    "    for key in master_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{master_key}' with value '{key}' is present only in '{master_table}' and missing in '{target_table}'.\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table, \n",
    "            'issue_column': master_key,\n",
    "            'unique_identifier': f\"{master_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    # Log errors for keys only in target\n",
    "    for key in target_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{target_key}' with value '{key}' is present only in '{target_table}' and missing in '{master_table}'.\",\n",
    "            'source_table': target_table,\n",
    "            'target_table': master_table,\n",
    "            'issue_column': target_key,\n",
    "            'unique_identifier': f\"{target_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    return master_only_keys, target_only_keys, error_logs_m\n",
    "\n",
    "def find_detailed_nulls(df_master, df_target, master_key, target_key, master_table, target_table, columns_to_check):\n",
    "    \"\"\"\n",
    "    Identify null values in both master and target tables for specified columns and fetch corresponding values or indicate missing keys.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "        df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        master_table (str): Name of the master table.\n",
    "        target_table (str): Name of the target table.\n",
    "        columns_to_check (list): List of columns to check for null values.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (null_values_master, null_values_target, error_logs_m)\n",
    "    \"\"\"\n",
    "    null_values_master = []\n",
    "    null_values_target = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    # Find nulls in master\n",
    "    null_master = df_master[df_master[columns_to_check].isnull().any(axis=1)]\n",
    "    for idx, row in null_master.iterrows():\n",
    "        key_value = str(row[master_key]).strip()\n",
    "        for column in columns_to_check:\n",
    "            if column == master_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and non-important columns\n",
    "            if column not in row:\n",
    "                continue  # Skip if column is not in the row\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row[column] if column in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                null_record = {\n",
    "                    master_key: key_value,\n",
    "                    'column': column,\n",
    "                    target_table: target_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'null',\n",
    "                    'error_message': 'Null in columns',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': column,\n",
    "                    'unique_identifier': f'{master_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_master.append(null_record)\n",
    "\n",
    "    # Find nulls in target\n",
    "    null_target = df_target[df_target[columns_to_check].isnull().any(axis=1)]\n",
    "    for idx, row in null_target.iterrows():\n",
    "        key_value = str(row[target_key]).strip()\n",
    "        for column in columns_to_check:\n",
    "            if column == target_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and non-important columns\n",
    "            if column not in row:\n",
    "                continue  # Skip if column is not in the row\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_master[master_key].astype(str).str.strip().values:\n",
    "                    master_row = df_master[df_master[master_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    master_value = master_row[column] if column in master_row else \"Column not present\"\n",
    "                else:\n",
    "                    master_value = f\"'{master_key}' not present\"\n",
    "                null_record = {\n",
    "                    target_key: key_value,\n",
    "                    'column': column,\n",
    "                    master_table: master_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'null',\n",
    "                    'error_message': 'Null in columns',\n",
    "                    'source_table': target_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': column,\n",
    "                    'unique_identifier': f'{target_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_target.append(null_record)\n",
    "\n",
    "    logging.info(f\"Found {len(null_values_master)} null values in master table '{master_table}'.\")\n",
    "    logging.info(f\"Found {len(null_values_target)} null values in target table '{target_table}'.\")\n",
    "    return null_values_master, null_values_target, error_logs_m\n",
    "\n",
    "def validate_pincode_mapping(df_master, df_target, key_column, target_key, target_table, client, master_table):\n",
    "    \"\"\"\n",
    "    Validate pincode mapping by comparing with the all_india_PO_list reference table.\n",
    "    If a pincode issue is found in the master table, then check the corresponding pincode in the target table.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        master_table (str): Name of the master table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pincode_mapping_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "\n",
    "    error_logs_m = []\n",
    "    # Read the reference table from Analytics dataset\n",
    "    try:\n",
    "        reference_table = \"all_india_po_list\"\n",
    "        reference_dataset = \"analytics_data\"\n",
    "        query = f\"SELECT pincode, city, state FROM `{PROJECT_ID}.{reference_dataset}.{reference_table}`\"\n",
    "        reference_df = client.query(query).to_dataframe()\n",
    "        reference_df['pincode'] = reference_df['pincode'].astype(str).str.strip()\n",
    "        reference_df['city'] = reference_df['city'].astype(str).str.strip().str.lower()\n",
    "        reference_df['state'] = reference_df['state'].astype(str).str.strip().str.lower()\n",
    "        logging.info(f\"Loaded reference pincode mapping from '{reference_table}' in '{reference_dataset}' dataset.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load reference pincode mapping: {e}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Check if df_master has 'pincode', 'city', 'state' columns\n",
    "    required_columns = {'pincode', 'city', 'state'}\n",
    "    if not required_columns.issubset(df_master.columns):\n",
    "        logging.info(f\"DataFrame does not have required columns for pincode mapping validation: {required_columns}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Initialize the issues DataFrame with a single target table details column\n",
    "    pincode_mapping_issues = pd.DataFrame(columns=[\n",
    "        key_column, 'pincode', 'state', 'city', 'issue',\n",
    "        f'{target_table}_details'\n",
    "    ])\n",
    "\n",
    "    # Iterate over each row in df_master to validate pincode mapping\n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "        pincode = str(row['pincode']).strip()\n",
    "        city = str(row['city']).strip().lower()\n",
    "        state = str(row['state']).strip().lower()\n",
    "\n",
    "        # Fetch corresponding target row if exists\n",
    "        target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value]\n",
    "        if not target_row.empty:\n",
    "            target_row = target_row.iloc[0]\n",
    "            target_pincode = target_row['pincode'] if 'pincode' in target_row and pd.notnull(target_row['pincode']) else \"Pincode missing\"\n",
    "            target_state = target_row['state'] if 'state' in target_row and pd.notnull(target_row['state']) else \"State missing\"\n",
    "            target_city = target_row['city'] if 'city' in target_row and pd.notnull(target_row['city']) else \"City missing\"\n",
    "            target_details = f\"Pincode: {target_pincode}, State: {target_state}, City: {target_city}\"\n",
    "        else:\n",
    "            target_details = f\"Key '{key_column}' with value '{key_value}' not present in target table '{target_table}'.\"\n",
    "\n",
    "        # Check if pincode exists in reference\n",
    "        ref_matches = reference_df[reference_df['pincode'] == pincode]\n",
    "        if ref_matches.empty:\n",
    "            issue = f\"Invalid pincode ({pincode}).\"\n",
    "            pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "                key_column: key_value,\n",
    "                'pincode': pincode,\n",
    "                'state': state,\n",
    "                'city': city,\n",
    "                'issue': issue,\n",
    "                f'{target_table}_details': target_details\n",
    "            }])], ignore_index=True)\n",
    "            error_detail = {\n",
    "                'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'pincode_mapping',\n",
    "                'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "                'source_table': master_table,\n",
    "                'target_table': target_table,\n",
    "                'issue_column': 'pincode',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "            continue\n",
    "\n",
    "        # Check if any of the reference entries match both the city and state\n",
    "        exact_match = ref_matches[\n",
    "            (ref_matches['city'] == city) & (ref_matches['state'] == state)\n",
    "        ]\n",
    "        if not exact_match.empty:\n",
    "            continue  # No issue, mapping is correct\n",
    "\n",
    "        # Check for state mismatch\n",
    "        state_matches = ref_matches[ref_matches['state'] == state]\n",
    "\n",
    "        # Check for city mismatch\n",
    "        city_matches = ref_matches[ref_matches['city'] == city]\n",
    "\n",
    "        if state_matches.empty and city_matches.empty:\n",
    "            # Both state and city do not match\n",
    "            expected_entries = ref_matches[['state', 'city']].drop_duplicates()\n",
    "            expected_states = expected_entries['state'].tolist()\n",
    "            expected_cities = expected_entries['city'].tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}' and city '{city}'. Expected states: {expected_states_str}; Expected cities: {expected_cities_str}.\"\n",
    "        elif state_matches.empty:\n",
    "            # State does not match\n",
    "            expected_states = ref_matches['state'].unique().tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}'. Expected states: {expected_states_str}.\"\n",
    "        elif city_matches.empty:\n",
    "            # City does not match\n",
    "            expected_cities = state_matches['city'].unique().tolist()\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match city '{city}'. Expected cities: {expected_cities_str}.\"\n",
    "        else:\n",
    "            # Other cases\n",
    "            issue = f\"Pincode {pincode} has a mapping inconsistency.\"\n",
    "\n",
    "        pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "            key_column: key_value,\n",
    "            'pincode': pincode,\n",
    "            'state': state,\n",
    "            'city': city,\n",
    "            'issue': issue,\n",
    "            f'{target_table}_details': target_details\n",
    "        }])], ignore_index=True)\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'pincode_mapping',\n",
    "            'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table,\n",
    "            'issue_column': 'pincode',\n",
    "            'unique_identifier': f'{key_column}: {key_value}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(pincode_mapping_issues)} pincode mapping issues in master table '{master_table}'.\")\n",
    "    return pincode_mapping_issues, error_logs_m\n",
    "\n",
    "def compare_tables(client, dataset_name, base_name, master_table, target_table, master_key, target_key, column_mapping=None):\n",
    "    \"\"\"\n",
    "    Compare two tables and generate a report.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        base_name (str): The base name of the table.\n",
    "        master_table (str): Name of the master_hub_ table.\n",
    "        target_table (str): Name of the target prefixed table.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing all comparison results.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting comparison for base table '{base_name}': '{master_table}' vs '{target_table}'.\")\n",
    "\n",
    "    # Initialize comparison results\n",
    "    mismatches = []\n",
    "    null_values_master = []\n",
    "    null_values_target = []\n",
    "    data_type_issues = pd.DataFrame()\n",
    "    format_issues_master = pd.DataFrame()\n",
    "    pincode_mapping_issues = pd.DataFrame()\n",
    "    duplicates_master = pd.DataFrame()\n",
    "    duplicates_target = pd.DataFrame()\n",
    "    master_only_keys = []\n",
    "    target_only_keys = []\n",
    "\n",
    "    # Load data\n",
    "    df_master = load_table_from_bigquery(client, dataset_name, master_table)\n",
    "    df_target = load_table_from_bigquery(client, dataset_name, target_table)\n",
    "\n",
    "    # Apply standardization\n",
    "    df_master = standardize_dataframe(df_master, exclude_columns=[master_key, 'is_active'])\n",
    "    df_target = standardize_dataframe(df_target, exclude_columns=[target_key, 'is_active'])\n",
    "\n",
    "    # # # Debugging: Log unique 'pan' values after standardization\n",
    "    # if 'is_active' in df_master.columns:\n",
    "    #     logging.info(f\"Master 'pan' unique values: {df_master['is_active'].unique()}\")\n",
    "    # if 'is_active' in df_target.columns:\n",
    "    #     logging.info(f\"Target 'pan' unique values: {df_target['is_active'].unique()}\")\n",
    "\n",
    "    \n",
    "    # Apply active filter if defined\n",
    "    base_table_info = BASE_TABLES.get(base_name, {})\n",
    "    active_filter = base_table_info.get('active_filter')\n",
    "    perform_checks = base_table_info.get('perform_checks', True)\n",
    "\n",
    "    if active_filter:\n",
    "        column = active_filter.get('column')\n",
    "        value = active_filter.get('value')\n",
    "        if column and column in df_master.columns:\n",
    "            initial_count = len(df_master)\n",
    "            df_master = df_master[df_master[column] == value]\n",
    "            filtered_count = len(df_master)\n",
    "            logging.info(f\"Filtered '{base_name}' master table: {initial_count - filtered_count} records excluded based on {column} = {value}.\")\n",
    "        else:\n",
    "            logging.warning(f\"Active filter specified but column '{column}' not found in master table '{master_table}'.\")\n",
    "\n",
    "    # Proceed with filtering\n",
    "    # if active_filter:\n",
    "    #     column = active_filter.get('column')\n",
    "    #     value = active_filter.get('value')\n",
    "    #     if column:\n",
    "    #         if column == 'is_active':\n",
    "    #             df_master = df_master[df_master[column].str.lower() == 'true']  # Updated to string comparison\n",
    "    #             logging.info(f\"Filtered '{base_name}' master table based on {column} = 'true'.\")\n",
    "    #         else:\n",
    "    #             df_master = df_master[df_master[column] == value]\n",
    "    #             logging.info(f\"Filtered '{base_name}' master table based on {column} = {value}.\")\n",
    "    \n",
    "    \n",
    "    if df_master.empty or df_target.empty:\n",
    "        logging.warning(f\"One of the tables '{master_table}' or '{target_table}' is empty. Skipping comparison.\")\n",
    "        return None\n",
    "\n",
    "    # Identify BigNumeric columns in master and target tables\n",
    "    schema_master = get_table_schema(client, dataset_name, master_table)\n",
    "    schema_target = get_table_schema(client, dataset_name, target_table)\n",
    "    bignumeric_columns_master = [col for col, dtype in schema_master.items() if dtype == 'BIGNUMERIC']\n",
    "    bignumeric_columns_target = [col for col, dtype in schema_target.items() if dtype == 'BIGNUMERIC']\n",
    "    \n",
    "    # # Format BigNumeric columns in master table\n",
    "    # for col in bignumeric_columns_master:\n",
    "    #     if col in df_master.columns:\n",
    "    #         df_master[col] = df_master[col].apply(lambda x: format(x, '.0f') if pd.notnull(x) else x)\n",
    "\n",
    "    # # Format BigNumeric columns in target table\n",
    "    # for col in bignumeric_columns_target:\n",
    "    #     if col in df_target.columns:\n",
    "    #         df_target[col] = df_target[col].apply(lambda x: format(x, '.0f') if pd.notnull(x) else x)\n",
    "\n",
    "    # # Get imp_columns and non_imp_columns\n",
    "    # imp_columns = Imp_columns.get(base_name, None)\n",
    "    # non_imp_columns = Non_imp_columns.get(base_name, [])\n",
    "\n",
    "    # Identify common columns\n",
    "    common_columns, master_unique_cols, target_unique_cols = find_common_and_non_common_columns(df_master, df_target)\n",
    "\n",
    "    if not common_columns:\n",
    "        logging.warning(f\"No common columns found between '{master_table}' and '{target_table}'. Skipping comparison.\")\n",
    "        return None\n",
    "\n",
    "    # Apply column mapping if provided\n",
    "    if column_mapping:\n",
    "        # Rename master columns to target columns for comparison\n",
    "        for master_col, target_col in column_mapping.items():\n",
    "            if master_col in df_master.columns and target_col in df_target.columns:\n",
    "                # Ensure both columns are standardized\n",
    "                df_master[master_col] = df_master[master_col].astype(str).str.strip().str.lower()\n",
    "                df_target[target_col] = df_target[target_col].astype(str).str.strip().str.lower()\n",
    "        # Adjust common_columns by replacing master_col with target_col\n",
    "        adjusted_common_columns = set(common_columns)\n",
    "        for master_col, target_col in column_mapping.items():\n",
    "            if master_col in adjusted_common_columns and target_col in adjusted_common_columns:\n",
    "                adjusted_common_columns.remove(master_col)\n",
    "                adjusted_common_columns.remove(target_col)\n",
    "                adjusted_common_columns.add(master_col)  # Use master_col as the unified name\n",
    "        common_columns = list(adjusted_common_columns)\n",
    "\n",
    "    # Determine columns to check based on imp_columns\n",
    "    if imp_columns:\n",
    "        columns_to_check = [col for col in imp_columns if col in common_columns]\n",
    "        logging.info(f\"Important columns defined for '{base_name}': {columns_to_check}\")\n",
    "    else:\n",
    "        columns_to_check = [col for col in common_columns if col not in non_imp_columns]\n",
    "        logging.info(f\"No important columns defined for '{base_name}'. Applying checks to all columns except non_imp_columns: {columns_to_check}\")\n",
    "\n",
    "    if perform_checks:\n",
    "        # Find duplicates in both tables\n",
    "        duplicates_master, error_logs_m = find_duplicates(df_master, master_key, master_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "        duplicates_target, error_logs_m = find_duplicates(df_target, target_key, target_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    if not duplicates_master.empty:\n",
    "        logging.warning(f\"Duplicate keys found in source table '{master_table}'. These will be reported but not used in mismatch comparison.\")\n",
    "    if not duplicates_target.empty:\n",
    "        logging.warning(f\"Duplicate keys found in target table '{target_table}'. These will be reported but not used in mismatch comparison.\")\n",
    "\n",
    "    # Perform mismatch comparison\n",
    "    if perform_checks:\n",
    "        mismatches, error_logs_m = find_mismatches(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            columns_to_check,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            duplicates_master,\n",
    "            duplicates_target,\n",
    "            non_imp_columns,\n",
    "            column_mapping  # Pass column_mapping to handle differently named columns\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Find detailed null values in both tables\n",
    "    if perform_checks:\n",
    "        null_values_master, null_values_target, error_logs_m = find_detailed_nulls(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            columns_to_check  # Pass columns_to_check\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Validate data types between master and target schemas\n",
    "    if perform_checks:\n",
    "        data_type_issues, error_logs_m = validate_data_types(\n",
    "            schema_master,\n",
    "            schema_target,\n",
    "            master_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            columns_to_check  # Pass columns_to_check\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Validate formats in master table only\n",
    "    if perform_checks:\n",
    "        format_issues_master, error_logs_m = validate_formats(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            target_table,\n",
    "            master_table,\n",
    "            columns_to_check  # Pass columns_to_check\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "        # Validate pincode mapping if applicable\n",
    "        pincode_mapping_issues = pd.DataFrame()\n",
    "        if {'pincode', 'city', 'state'}.issubset(df_master.columns):\n",
    "            pincode_mapping_issues, error_logs_m = validate_pincode_mapping(\n",
    "                df_master,\n",
    "                df_target,\n",
    "                master_key,\n",
    "                target_key,\n",
    "                target_table,\n",
    "                client,\n",
    "                master_table\n",
    "            )\n",
    "            ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Find non-matching keys\n",
    "    master_only_keys, target_only_keys, error_logs_m = find_non_matching_keys(\n",
    "        df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table\n",
    "    )\n",
    "    ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        'mismatches': mismatches,\n",
    "        'null_values_master': pd.DataFrame(null_values_master),\n",
    "        'null_values_target': pd.DataFrame(null_values_target),\n",
    "        'duplicates_master': duplicates_master,\n",
    "        'duplicates_target': duplicates_target,\n",
    "        'data_type_issues': data_type_issues,\n",
    "        'format_issues_master': format_issues_master,\n",
    "        'pincode_mapping_issues': pincode_mapping_issues,\n",
    "        'key_column_master': master_key,\n",
    "        'key_column_target': target_key,\n",
    "        'df_master_only_keys': master_only_keys,\n",
    "        'df_target_only_keys': target_only_keys,\n",
    "        'table1_name': master_table,\n",
    "        'table2_name': target_table,\n",
    "        'df_master': df_master,\n",
    "        'df_target': df_target\n",
    "    }\n",
    "\n",
    "    logging.info(f\"Completed comparison for '{master_table}' vs '{target_table}'.\")\n",
    "    return results\n",
    "\n",
    "def generate_string_schema(df):\n",
    "    \"\"\"\n",
    "    Generates a BigQuery schema with all fields as STRING.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame for which to generate the schema.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of SchemaField objects with type STRING.\n",
    "    \"\"\"\n",
    "    schema = [SchemaField(column, \"STRING\", mode=\"NULLABLE\") for column in df.columns]\n",
    "    return schema\n",
    "\n",
    "def _upload_dataframe_to_bigquery(client, analytics_dataset, table_name, df):\n",
    "    \"\"\"\n",
    "    Helper function to upload a DataFrame to BigQuery.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        analytics_dataset (str): The name of the Analytics dataset.\n",
    "        table_name (str): The name of the table to upload.\n",
    "        df (pd.DataFrame): The DataFrame to upload.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logging.info(f\"No data to upload for '{table_name}'. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Convert all columns to string type\n",
    "    df = df.astype(str)\n",
    "\n",
    "    # Generate BigQuery schema with all fields as STRING\n",
    "    schema = generate_string_schema(df)\n",
    "\n",
    "    # Ensure table name doesn't exceed BigQuery's maximum length (1,024 characters)\n",
    "    if len(table_name) > 1024:\n",
    "        original_table_name = table_name\n",
    "        table_name = table_name[:1021] + '...'\n",
    "        logging.warning(f\"Table name truncated from '{original_table_name}' to '{table_name}' due to length constraints.\")\n",
    "\n",
    "    # Define the full table ID\n",
    "    table_id = f\"{client.project}.{analytics_dataset}.{table_name}\"\n",
    "\n",
    "    # Upload the DataFrame to BigQuery\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(\n",
    "            df,\n",
    "            table_id,\n",
    "            job_config=bigquery.LoadJobConfig(\n",
    "                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "                schema=schema  # Using the provided schema with all fields as STRING\n",
    "            )\n",
    "        )\n",
    "        job.result()  # Wait for the job to complete\n",
    "        logging.info(f\"Successfully uploaded '{table_id}' with {len(df)} records.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to upload '{table_id}' to BigQuery: {e}\")\n",
    "\n",
    "def upload_comparison_results_to_bigquery(client, analytics_dataset, ERROR_LOG_M):\n",
    "    \"\"\"\n",
    "    Uploads the ERROR_LOG_M to BigQuery as a separate table in the Analytics dataset.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        analytics_dataset (str): The name of the Analytics dataset.\n",
    "        ERROR_LOG_M (list): The error log data as a list of dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Handle ERROR_LOG separately\n",
    "    if ERROR_LOG_M is not None:\n",
    "        # Determine the DataFrame to upload\n",
    "        if isinstance(ERROR_LOG_M, pd.DataFrame):\n",
    "            error_df = ERROR_LOG_M\n",
    "        elif isinstance(ERROR_LOG_M, list):\n",
    "            error_df = pd.DataFrame(ERROR_LOG_M)\n",
    "        else:\n",
    "            logging.warning(\"Unsupported data type for ERROR_LOG. Skipping upload.\")\n",
    "            error_df = None\n",
    "\n",
    "        if error_df is not None and not error_df.empty:\n",
    "            _upload_dataframe_to_bigquery(client, analytics_dataset, \"error_logs_master_hub\", error_df)\n",
    "        else:\n",
    "            logging.info(\"No error logs to upload.\")\n",
    "    else:\n",
    "        logging.info(\"No error logs present.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the comparison of multiple base tables against their master_hub_ counterparts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize BigQuery client\n",
    "        try:\n",
    "            client = get_bigquery_client(PROJECT_ID)\n",
    "        except Exception:\n",
    "            logging.error(\"Exiting due to BigQuery client initialization failure.\")\n",
    "            return\n",
    "\n",
    "        # Find common tables with 'master_hub_' and other prefixes, passing BASE_TABLES\n",
    "        common_tables = find_common_tables_with_master_hub(client, DATASET_ID, PREFIXES, BASE_TABLES)\n",
    "\n",
    "        if not common_tables:\n",
    "            logging.info(\"No common tables found with 'master_hub_' and the specified prefixes.\")\n",
    "            return\n",
    "\n",
    "        # Iterate over each base table and perform comparisons\n",
    "        for base_name, tables in common_tables.items():\n",
    "            base_table_info = BASE_TABLES.get(base_name)\n",
    "            if not base_table_info:\n",
    "                logging.warning(f\"No configuration found for base table '{base_name}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            master_key = base_table_info.get('master_key')\n",
    "            target_tables = base_table_info.get('targets', {})\n",
    "            column_mapping = base_table_info.get('column_mapping', {})\n",
    "            \n",
    "\n",
    "            master_table = tables.get('master_hub_')\n",
    "            if not master_table:\n",
    "                logging.warning(f\"Master table 'master_hub_{base_name}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            all_results = []\n",
    "\n",
    "            # Iterate through each prefix and its corresponding target_key\n",
    "            for prefix, target_key in target_tables.items():\n",
    "                target_table = tables.get(prefix)\n",
    "                if not target_table:\n",
    "                    logging.warning(f\"Target table with prefix '{prefix}' for base table '{base_name}' not found. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                comparison_result = compare_tables(\n",
    "                    client, \n",
    "                    DATASET_ID, \n",
    "                    base_name, \n",
    "                    master_table, \n",
    "                    target_table, \n",
    "                    master_key, \n",
    "                    target_key,  # Pass the correct target_key per prefix\n",
    "                    column_mapping  # Pass the column_mapping\n",
    "                )\n",
    "                if comparison_result:\n",
    "                    all_results.append(comparison_result)\n",
    "\n",
    "                    # Prepare and send a separate Slack message for each comparison\n",
    "                    total_mismatches = len(comparison_result['mismatches'])\n",
    "                    total_nulls_master = len(comparison_result['null_values_master'])\n",
    "                    total_nulls_target = len(comparison_result['null_values_target'])\n",
    "                    total_dup_master = len(comparison_result['duplicates_master'])\n",
    "                    total_dup_target = len(comparison_result['duplicates_target'])\n",
    "                    total_data_type_issues = len(comparison_result['data_type_issues'])\n",
    "                    total_format_issues_master = len(comparison_result['format_issues_master'])\n",
    "                    total_pincode_issues = len(comparison_result['pincode_mapping_issues'])\n",
    "                    total_non_matching_source = len(comparison_result.get('df_master_only_keys', []))\n",
    "                    total_non_matching_target = len(comparison_result.get('df_target_only_keys', []))\n",
    "\n",
    "                    message = (\n",
    "                        f\"✅ *Comparison Report Generated for `{base_name}`*\\n\"\n",
    "                        f\"*Tables Compared: `{comparison_result['table1_name']}` vs `{comparison_result['table2_name']}`*\\n\"\n",
    "                        f\"- *Total Mismatches between values of same column name of both tables : `{total_mismatches}`*\\n\"\n",
    "                        f\"- *Total Null Values in `{comparison_result['table1_name']}`: `{total_nulls_master}`*\\n\"\n",
    "                        f\"- *Total Null Values in `{comparison_result['table2_name']}`: `{total_nulls_target}`*\\n\"\n",
    "                        f\"- *Duplicate `{master_key}` in `{comparison_result['table1_name']}`: `{total_dup_master}`*\\n\"\n",
    "                        f\"- *Duplicate `{target_key}` in `{comparison_result['table2_name']}`: `{total_dup_target}`*\\n\"\n",
    "                        f\"- *Total Data Type Issues(mismatch between datatype in columns with same name of both tables): `{total_data_type_issues}`*\\n\"\n",
    "                        f\"- *Total Format/Value Issues(gstin, email, pincode) in `{comparison_result['table1_name']}`: `{total_format_issues_master}`*\\n\"\n",
    "                        f\"- *Total Pincode Mapping Issues in `{comparison_result['table1_name']}`: `{total_pincode_issues}`*\\n\"\n",
    "                         \"- *Non-Matching Keys*:\\n\"\n",
    "                        f\"--*`{master_key}` only in `{comparison_result['table1_name']}` and not in `{comparison_result['table2_name']}`:`{total_non_matching_source}`,*\\n\"\n",
    "                        f\"--*`{target_key}` only in `{comparison_result['table2_name']}` and not in `{comparison_result['table1_name']}`:`{total_non_matching_target}`*\"\n",
    "                    )\n",
    "\n",
    "                    send_slack_alert(message)\n",
    "            \n",
    "            if all_results:\n",
    "                # Generate aggregated report for the base name and get the filepath\n",
    "                report_filepath = create_aggregated_document(all_results, base_name)\n",
    "                \n",
    "                # Upload the report to Slack using the updated function\n",
    "                upload_file_to_slack(report_filepath, title=f\"{base_name.capitalize()} Comparison Report\")\n",
    "                \n",
    "                # Remove the local report file after successful upload\n",
    "                try:\n",
    "                    os.remove(report_filepath)\n",
    "                    logging.info(f\"Removed local report file '{report_filepath}'.\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to remove local report file '{report_filepath}': {e}\")\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                logging.info(f\"No comparison results to report for base name '{base_name}'.\")\n",
    "               \n",
    "\n",
    "        # Upload error logs to BigQuery after all comparisons\n",
    "        upload_comparison_results_to_bigquery(\n",
    "            client, \n",
    "            'analytics_data',\n",
    "            ERROR_LOG_M\n",
    "            )\n",
    "\n",
    "        logging.info(\"All comparisons completed.\")\n",
    "    except Exception as e:\n",
    "        # Capture the full traceback\n",
    "        tb = traceback.format_exc()\n",
    "        logging.error(\"An unexpected error occurred in the main process.\", exc_info=True)\n",
    "\n",
    "        # Prepare a detailed error message for Slack\n",
    "        error_message = (\n",
    "            f\"❌ *Comparison Process Failed*\\n\"\n",
    "            f\"*Error:* {str(e)}\\n\"\n",
    "            f\"*Traceback:*\\n```{tb}```\"\n",
    "        )\n",
    "        send_slack_alert(error_message)\n",
    "\n",
    "        # Optionally, exit the script with a non-zero status\n",
    "        sys.exit(1)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2592094-1286-4285-8833-5d92dc8825c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_master' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(df_master\u001b[38;5;241m.\u001b[39mloc[df_master[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupplier_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m32021006\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpan\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(df_target\u001b[38;5;241m.\u001b[39mloc[df_target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupplier_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m32021006\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpan\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_master' is not defined"
     ]
    }
   ],
   "source": [
    "# print(repr(df_master.loc[df_master['supplier_code'] == '32021006', 'pan'].iloc[0]))\n",
    "# print(repr(df_target.loc[df_target['supplier_code'] == '32021006', 'pan'].iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41968bd-ace5-4ec6-84f7-24844fddcce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 17:23:42,532 - INFO - Slack client initialized successfully.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "2024-11-27 17:23:42,991 - INFO - BigQuery client initialized successfully.\n",
      "2024-11-27 17:23:44,005 - INFO - Found 173 tables in dataset 'Impetus_dev_prod'.\n",
      "2024-11-27 17:23:44,006 - INFO - Identified 2 common base names with 'master_hub_' and other specified prefixes.\n",
      "2024-11-27 17:23:44,007 - INFO - Starting comparison for base table 'supplier': 'master_hub_supplier' vs 'procuro_supplier'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 17:23:45,246 - INFO - Loaded data from table 'master_hub_supplier' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 17:23:46,086 - INFO - Loaded data from table 'procuro_supplier' into DataFrame.\n",
      "2024-11-27 17:23:46,099 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 17:23:46,108 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 17:23:46,111 - INFO - Filtered 'supplier' master table: 0 records excluded based on is_active = True.\n",
      "2024-11-27 17:23:46,267 - INFO - Retrieved schema for table 'master_hub_supplier'.\n",
      "2024-11-27 17:23:46,490 - INFO - Retrieved schema for table 'procuro_supplier'.\n",
      "2024-11-27 17:23:46,502 - INFO - Found 18 common columns, 16 unique to first table, 11 unique to second table.\n",
      "2024-11-27 17:23:46,504 - INFO - No important columns defined for 'supplier'. Applying checks to all columns except non_imp_columns: ['_boltic_pipe_id', '_boltic_ingested_at', 'pan', 'pincode', 'is_msme', 'state', '_boltic_mark_deleted', 'address', 'supplier_code', '_boltic_updated_at', '_boltic_merged', 'gstin', '_boltic_id', '_boltic_meta_id', 'name', 'city']\n",
      "2024-11-27 17:23:46,514 - INFO - Found 0 duplicate entries based on 'supplier_code'.\n",
      "2024-11-27 17:23:46,516 - INFO - Found 0 duplicate entries based on 'supplier_code'.\n",
      "2024-11-27 17:23:46,526 - INFO - Merged DataFrame has 822 records for mismatch comparison.\n",
      "2024-11-27 17:23:46,580 - INFO - Found 1833 mismatches between 'master_hub_supplier' and 'procuro_supplier'.\n",
      "2024-11-27 17:23:46,717 - INFO - Found 0 null values in master table 'master_hub_supplier'.\n",
      "2024-11-27 17:23:46,717 - INFO - Found 155 null values in target table 'procuro_supplier'.\n",
      "2024-11-27 17:23:46,717 - INFO - Found 0 data type issues.\n",
      "2024-11-27 17:23:47,399 - INFO - Found 923 format issues.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 17:23:51,607 - INFO - Loaded reference pincode mapping from 'all_india_po_list' in 'analytics_data' dataset.\n",
      "2024-11-27 17:23:57,249 - INFO - Found 226 pincode mapping issues in master table 'master_hub_supplier'.\n",
      "2024-11-27 17:23:57,253 - INFO - Found 99 keys in source not in target and 0 keys in target not in source.\n",
      "2024-11-27 17:23:57,254 - INFO - Completed comparison for 'master_hub_supplier' vs 'procuro_supplier'.\n",
      "2024-11-27 17:23:57,721 - INFO - Message sent to C08310RS2PK: 1732708437.668709\n",
      "2024-11-27 17:23:57,722 - INFO - Starting comparison for base table 'supplier': 'master_hub_supplier' vs 'costing_engine_supplier'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 17:23:58,850 - INFO - Loaded data from table 'master_hub_supplier' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 17:23:59,753 - INFO - Loaded data from table 'costing_engine_supplier' into DataFrame.\n",
      "2024-11-27 17:23:59,760 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 17:23:59,763 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 17:23:59,765 - INFO - Filtered 'supplier' master table: 0 records excluded based on is_active = True.\n",
      "2024-11-27 17:23:59,970 - INFO - Retrieved schema for table 'master_hub_supplier'.\n",
      "2024-11-27 17:24:00,182 - INFO - Retrieved schema for table 'costing_engine_supplier'.\n",
      "2024-11-27 17:24:00,201 - INFO - Found 25 common columns, 9 unique to first table, 2 unique to second table.\n",
      "2024-11-27 17:24:00,202 - INFO - No important columns defined for 'supplier'. Applying checks to all columns except non_imp_columns: ['is_msme', 'state', 'email', '_boltic_id', '_boltic_meta_id', 'city', '_boltic_pipe_id', '_boltic_ingested_at', 'is_sample_supplier', 'country_of_origin', 'name', 'vendor_status', '_boltic_mark_deleted', '_boltic_updated_at', 'pan', 'pincode', 'address', 'supplier_code', '_boltic_merged', 'is_active', 'gstin']\n",
      "2024-11-27 17:24:00,203 - INFO - Found 0 duplicate entries based on 'supplier_code'.\n",
      "2024-11-27 17:24:00,957 - INFO - Found 757 duplicate entries based on 'supplier_code'.\n",
      "2024-11-27 17:24:00,958 - WARNING - Duplicate keys found in target table 'costing_engine_supplier'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 17:24:00,961 - INFO - Merged DataFrame has 921 records for mismatch comparison.\n",
      "2024-11-27 17:24:01,035 - INFO - Found 8081 mismatches between 'master_hub_supplier' and 'costing_engine_supplier'.\n",
      "2024-11-27 17:24:01,291 - INFO - Found 0 null values in master table 'master_hub_supplier'.\n",
      "2024-11-27 17:24:01,291 - INFO - Found 1310 null values in target table 'costing_engine_supplier'.\n",
      "2024-11-27 17:24:01,292 - INFO - Found 0 data type issues.\n",
      "2024-11-27 17:24:02,188 - INFO - Found 923 format issues.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 17:24:05,393 - INFO - Loaded reference pincode mapping from 'all_india_po_list' in 'analytics_data' dataset.\n",
      "2024-11-27 17:24:11,303 - INFO - Found 226 pincode mapping issues in master table 'master_hub_supplier'.\n",
      "2024-11-27 17:24:11,308 - INFO - Found 0 keys in source not in target and 4 keys in target not in source.\n",
      "2024-11-27 17:24:11,309 - INFO - Completed comparison for 'master_hub_supplier' vs 'costing_engine_supplier'.\n",
      "2024-11-27 17:24:11,738 - INFO - Message sent to C08310RS2PK: 1732708451.672579\n",
      "2024-11-27 17:24:11,769 - INFO - Added Table of Contents to the Word document.\n",
      "2024-11-27 17:24:12,421 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:24:12,465 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:24:12,844 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:24:12,958 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:24:12,969 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:24:15,915 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:24:16,257 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:24:16,392 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:24:16,769 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:24:16,882 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:24:16,884 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:24:16,986 - INFO - Saved aggregated comparison report as 'supplier_comparison_report_aggregated_20241127_172416.docx'.\n",
      "2024-11-27 17:24:18,343 - INFO - File uploaded to Slack channel 'C08310RS2PK': https://gofynd.slack.com/files/U07TU8ERA1J/F083CN6D9H6/supplier_comparison_report_aggregated_20241127_172416.docx\n",
      "2024-11-27 17:24:18,344 - INFO - Removed local report file 'supplier_comparison_report_aggregated_20241127_172416.docx'.\n",
      "2024-11-27 17:24:48,361 - INFO - Starting comparison for base table 'vendor_details': 'master_hub_supplier' vs 'scan_pack_vendor_details'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 17:24:49,637 - INFO - Loaded data from table 'master_hub_supplier' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 17:24:50,167 - INFO - Loaded data from table 'scan_pack_vendor_details' into DataFrame.\n",
      "2024-11-27 17:24:50,186 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 17:24:50,192 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 17:24:50,195 - INFO - Filtered 'vendor_details' master table: 0 records excluded based on is_active = True.\n",
      "2024-11-27 17:24:50,404 - INFO - Retrieved schema for table 'master_hub_supplier'.\n",
      "2024-11-27 17:24:50,573 - INFO - Retrieved schema for table 'scan_pack_vendor_details'.\n",
      "2024-11-27 17:24:50,584 - INFO - Found 17 common columns, 17 unique to first table, 9 unique to second table.\n",
      "2024-11-27 17:24:50,585 - INFO - No important columns defined for 'vendor_details'. Applying checks to all columns except non_imp_columns: ['_boltic_pipe_id', '_boltic_ingested_at', 'pan', 'pincode', 'state', '_boltic_mark_deleted', 'address', '_boltic_merged', '_boltic_updated_at', '_boltic_id', '_boltic_meta_id', 'name', 'city']\n",
      "2024-11-27 17:24:50,592 - INFO - Found 0 duplicate entries based on 'supplier_code'.\n",
      "2024-11-27 17:24:50,596 - INFO - Found 0 duplicate entries based on 'vendor_code'.\n",
      "2024-11-27 17:24:50,606 - INFO - Merged DataFrame has 96 records for mismatch comparison.\n",
      "2024-11-27 17:24:50,616 - INFO - Found 479 mismatches between 'master_hub_supplier' and 'scan_pack_vendor_details'.\n",
      "2024-11-27 17:24:50,893 - INFO - Found 0 null values in master table 'master_hub_supplier'.\n",
      "2024-11-27 17:24:50,893 - INFO - Found 431 null values in target table 'scan_pack_vendor_details'.\n",
      "2024-11-27 17:24:50,893 - INFO - Found 0 data type issues.\n",
      "2024-11-27 17:24:50,910 - INFO - Found 2 format issues.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 17:24:54,151 - INFO - Loaded reference pincode mapping from 'all_india_po_list' in 'analytics_data' dataset.\n",
      "2024-11-27 17:25:00,121 - INFO - Found 226 pincode mapping issues in master table 'master_hub_supplier'.\n",
      "2024-11-27 17:25:00,127 - INFO - Found 825 keys in source not in target and 0 keys in target not in source.\n",
      "2024-11-27 17:25:00,130 - INFO - Completed comparison for 'master_hub_supplier' vs 'scan_pack_vendor_details'.\n",
      "2024-11-27 17:25:00,627 - INFO - Message sent to C08310RS2PK: 1732708500.634349\n",
      "2024-11-27 17:25:00,646 - INFO - Added Table of Contents to the Word document.\n",
      "2024-11-27 17:25:00,817 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:25:00,937 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:25:00,943 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:25:01,084 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:25:01,164 - INFO - Added table to the Word document.\n",
      "2024-11-27 17:25:01,183 - INFO - Saved aggregated comparison report as 'vendor_details_comparison_report_aggregated_20241127_172501.docx'.\n",
      "2024-11-27 17:25:02,844 - INFO - File uploaded to Slack channel 'C08310RS2PK': https://gofynd.slack.com/files/U07TU8ERA1J/F082HA5P5FY/vendor_details_comparison_report_aggregated_20241127_172501.docx\n",
      "2024-11-27 17:25:02,846 - INFO - Removed local report file 'vendor_details_comparison_report_aggregated_20241127_172501.docx'.\n",
      "2024-11-27 17:25:33,987 - ERROR - Failed to upload 'fynd-jio-impetus-prod.analytics_data.error_logs_master_hub' to BigQuery: 403 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/fynd-jio-impetus-prod/jobs?uploadType=multipart: Access Denied: Table fynd-jio-impetus-prod:analytics_data.error_logs_master_hub: Permission bigquery.tables.update denied on table fynd-jio-impetus-prod:analytics_data.error_logs_master_hub (or it may not exist).\n",
      "2024-11-27 17:25:33,991 - INFO - All comparisons completed.\n"
     ]
    }
   ],
   "source": [
    "#imp and non imp column added\n",
    "# f\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "from docx import Document\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = 'fynd-jio-impetus-prod'       # Replace with your project ID\n",
    "DATASET_ID = 'Impetus_dev_prod'                 # Replace with your dataset ID\n",
    "PREFIXES = ['procuro_', 'costing_engine_', 'scan_pack_', 'pigeon_']  # Define your prefixes\n",
    "# PROJECT_ID = 'fynd-jio-impetus-non-prod'       # Replace with your project ID\n",
    "# DATASET_ID = 'Impetus_dev_sit'                 # Replace with your dataset ID\n",
    "# PREFIXES = ['procuro_', 'costing_engine_', 'scan_pack_', 'pigeon_']  # Define your prefixes\n",
    "\n",
    "\n",
    "\n",
    "# Error log list\n",
    "ERROR_LOG_M = []\n",
    "\n",
    "# Mapping of base table names to their key columns in master and target tables\n",
    "BASE_TABLES = {\n",
    "    # 'brand': {\n",
    "    #     'master_key': 'code',\n",
    "    #     'targets': {\n",
    "    #         'procuro_': 'code',\n",
    "    #         'costing_engine_': 'code'\n",
    "    #     },\n",
    "    #     'active_filter': {\n",
    "    #         'column': 'is_active',\n",
    "    #         'value': True\n",
    "    #     },\n",
    "    #     'perform_checks': True  # Default behavior\n",
    "    # },\n",
    "    # 'brand_pm_mapping': {\n",
    "    #     'master_key': 'pm_id',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'pm_id'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    # 'brick': {\n",
    "    #     'master_key': 'brick_code',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'code'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    # 'coe_bom_element_type_mapping': {\n",
    "    #     'master_key': 'coe_name',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'coe_name'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    # 'event_log': {\n",
    "    #     'master_key': 'user_id',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'user_id'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    'supplier': {\n",
    "        'master_key': 'supplier_code',\n",
    "        'targets': {\n",
    "            'procuro_': 'supplier_code',\n",
    "            'costing_engine_': 'supplier_code'\n",
    "        },\n",
    "        'active_filter': {  # Apply active filter\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'vendor_details': {  # Newly added entry\n",
    "        'master_key': 'supplier_code',  # Using supplier_code as the key\n",
    "        'master_table': 'master_hub_supplier',  # Specify the master table explicitly\n",
    "        'targets': {\n",
    "            'scan_pack_': 'vendor_code'\n",
    "        },\n",
    "        'active_filter': {  # Apply active filter\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    # 'hsn_tax_mapping': {  # Newly added base table for HSN Codes\n",
    "    #     'master_key': 'hsn_code',  # Assuming 'hsn_code' is the key column\n",
    "    #     'master_table': 'master_hub_hsn',\n",
    "    #     'targets': {\n",
    "    #         'procuro_': 'hsn_code',\n",
    "    #     },\n",
    "    #     'perform_checks': False  # Only perform key comparisons\n",
    "    # },\n",
    "    # 'config_buyer_brand_mapping': {  # Updated entry\n",
    "    #     'master_key': 'id',\n",
    "    #     'master_table': 'master_hub_buyer_brand_mapping',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'id'\n",
    "    #     },\n",
    "    #     'column_mapping': {  # Mapping of master columns to target columns\n",
    "    #         'brand_id': 'brand_code'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },   \n",
    "}\n",
    "\n",
    "# Define Non-Important Columns\n",
    "Non_imp_columns = {\n",
    "    'supplier': ['id', '_id', 'updated_at', 'created_at'],\n",
    "    'vendor_details': ['id', '_id', 'updated_at', 'created_at']  # Add if applicable\n",
    "}\n",
    "\n",
    "# Define Important Columns\n",
    "Imp_columns = {\n",
    "    'brand': ['name', 'id', 'slug', 'code'],\n",
    "    'brick': ['name', 'id', 'brick_code', 'description', 'class_code'],\n",
    "    'config_buyer_brand_mapping': ['buyer_email', 'brand_code', 'id', 'buyer_id', 'is_active', 'buyer_name'],\n",
    "    'brand_pm_mapping': ['pm_id', 'brand_code', 'pm_email', 'is_active', 'pm_name', 'id'],\n",
    "    'config_buyer_brand_mapping': ['is_active','coe_id','id','coe_name','coe_approver_email','element_type']\n",
    "    # Add more base tables and their important columns as needed\n",
    "}\n",
    "\n",
    "# Slack configuration\n",
    "SLACK_TOKEN = \"xoxb-2151238541-7946286860052-5FCcfqBPem0xKigGlIcKdLgX\"  # Replace with your Slack token\n",
    "SLACK_CHANNEL = \"C08310RS2PK\"  # Replace with your Slack channel ID\n",
    "\n",
    "# Initialize Slack client\n",
    "if SLACK_TOKEN and SLACK_CHANNEL:\n",
    "    slack_client = WebClient(token=SLACK_TOKEN)\n",
    "    logging.info(\"Slack client initialized successfully.\")\n",
    "else:\n",
    "    slack_client = None\n",
    "    logging.warning(\"Slack token or channel not found. Slack notifications will be disabled.\")\n",
    "\n",
    "def get_bigquery_client(project_id):\n",
    "    \"\"\"\n",
    "    Initialize and return a BigQuery client.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID.\n",
    "\n",
    "    Returns:\n",
    "        bigquery.Client: An initialized BigQuery client.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        logging.info(\"BigQuery client initialized successfully.\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize BigQuery client: {e}\")\n",
    "        raise\n",
    "\n",
    "def find_common_tables_with_master_hub(client, dataset_name, prefixes, base_tables):\n",
    "    \"\"\"\n",
    "    Find tables in the specified dataset that share the same base name after removing the 'master_hub_' prefix\n",
    "    and exist with other given prefixes.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset to search within.\n",
    "        prefixes (list): List of prefixes to compare with 'master_hub_'.\n",
    "        base_tables (dict): The BASE_TABLES dictionary containing base table configurations.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are base names and values are dictionaries showing which prefixes have tables.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Reference the dataset\n",
    "        dataset_ref = client.dataset(dataset_name)\n",
    "\n",
    "        # List all tables in the dataset\n",
    "        tables = client.list_tables(dataset_ref)\n",
    "        table_names = [table.table_id for table in tables]\n",
    "        logging.info(f\"Found {len(table_names)} tables in dataset '{dataset_name}'.\")\n",
    "\n",
    "        # Dictionary to hold base names and their corresponding tables\n",
    "        common_tables = {}\n",
    "        for base_name, config in base_tables.items():\n",
    "            # Determine the master table\n",
    "            master_table = config.get('master_table', f'master_hub_{base_name}')\n",
    "            if master_table in table_names:\n",
    "                common_tables[base_name] = {'master_hub_': master_table}\n",
    "                # Check for target tables with specified prefixes\n",
    "                for prefix, target_key in config.get('targets', {}).items():\n",
    "                    target_table = f\"{prefix}{base_name}\"\n",
    "                    if target_table in table_names:\n",
    "                        common_tables[base_name][prefix] = target_table\n",
    "            else:\n",
    "                logging.warning(f\"Master table '{master_table}' for base '{base_name}' not found in dataset.\")\n",
    "\n",
    "        # Filter out base names that only have 'master_hub_' but no other matching prefixes\n",
    "        common_tables_with_prefixes = {base_name: tables for base_name, tables in common_tables.items() if len(tables) > 1}\n",
    "\n",
    "        logging.info(f\"Identified {len(common_tables_with_prefixes)} common base names with 'master_hub_' and other specified prefixes.\")\n",
    "        return common_tables_with_prefixes\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Google API Error: {e.message}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return {}\n",
    "\n",
    "def get_table_schema(client, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Retrieve the schema of a specified BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        table_name (str): The name of the table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping column names to their data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table_ref = client.dataset(dataset_name).table(table_name)\n",
    "        table = client.get_table(table_ref)\n",
    "        schema = {field.name: field.field_type for field in table.schema}\n",
    "        logging.info(f\"Retrieved schema for table '{table_name}'.\")\n",
    "        return schema\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Failed to retrieve schema for table '{table_name}': {e.message}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while retrieving schema for table '{table_name}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def load_table_from_bigquery(client, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Load a table from BigQuery into a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        table_name (str): The name of the table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the table data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f\"SELECT * FROM `{PROJECT_ID}.{dataset_name}.{table_name}`\"\n",
    "        df = client.query(query).to_dataframe()\n",
    "        logging.info(f\"Loaded data from table '{table_name}' into DataFrame.\")\n",
    "        return df\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Failed to load table '{table_name}': {e.message}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while loading table '{table_name}': {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def standardize_dataframe(df, exclude_columns=[]):\n",
    "    \"\"\"\n",
    "    Standardize string columns in the DataFrame by stripping whitespace and converting to lowercase,\n",
    "    excluding specified columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to standardize.\n",
    "        exclude_columns (list): Columns to exclude from standardization.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Standardized DataFrame.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in df_copy.columns:\n",
    "        if col in exclude_columns:\n",
    "            continue  # Skip standardizing this column\n",
    "        if pd.api.types.is_string_dtype(df_copy[col]):\n",
    "            df_copy[col] = df_copy[col].astype(str).str.strip().str.lower()\n",
    "    logging.info(\"Standardized DataFrame for comparison.\")\n",
    "    return df_copy\n",
    "\n",
    "def find_common_and_non_common_columns(df1, df2):\n",
    "    \"\"\"\n",
    "    Identify common and unique columns between two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df1 (pd.DataFrame): First DataFrame.\n",
    "        df2 (pd.DataFrame): Second DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (common_columns, df1_unique_columns, df2_unique_columns)\n",
    "    \"\"\"\n",
    "    common_columns = list(set(df1.columns).intersection(set(df2.columns)))\n",
    "    df1_unique_columns = list(set(df1.columns) - set(df2.columns))\n",
    "    df2_unique_columns = list(set(df2.columns) - set(df1.columns))\n",
    "    logging.info(f\"Found {len(common_columns)} common columns, {len(df1_unique_columns)} unique to first table, {len(df2_unique_columns)} unique to second table.\")\n",
    "    return common_columns, df1_unique_columns, df2_unique_columns\n",
    "\n",
    "def find_mismatches(df_master, df_target, columns_to_check, master_key, target_key, table1, table2, duplicates_master, duplicates_target, non_imp_columns, column_mapping=None):\n",
    "    \"\"\"\n",
    "    Identify mismatches between two DataFrames based on specified columns and key columns.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "        df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "        columns_to_check (list): List of columns to apply mismatch checks.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        table1 (str): Name of the source table.\n",
    "        table2 (str): Name of the target table.\n",
    "        duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "        duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "        non_imp_columns (list): List of non-important columns to exclude.\n",
    "        column_mapping (dict, optional): Mapping of master columns to target columns. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mismatches, error_logs_m)\n",
    "    \"\"\"\n",
    "    mismatches = []\n",
    "    error_logs_m = []\n",
    "    # Ensure key columns are present in both DataFrames\n",
    "    if master_key not in df_master.columns or target_key not in df_target.columns:\n",
    "        logging.error(f\"Key columns '{master_key}' or '{target_key}' not found in the respective tables.\")\n",
    "        return mismatches, error_logs_m\n",
    "\n",
    "    # Rename target key to match master key for easier comparison\n",
    "    df_target_renamed = df_target.rename(columns={target_key: master_key})\n",
    "\n",
    "    # Merge DataFrames on the master_key, excluding duplicates\n",
    "    merged_df = pd.merge(\n",
    "        df_master.drop_duplicates(subset=master_key),\n",
    "        df_target_renamed.drop_duplicates(subset=master_key),\n",
    "        on=master_key,\n",
    "        suffixes=(f'_{table1}', f'_{table2}'),\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Merged DataFrame has {len(merged_df)} records for mismatch comparison.\")\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        key = row[master_key]\n",
    "        for master_col in columns_to_check:\n",
    "            if master_col.startswith('_boltic_') or master_col in non_imp_columns:\n",
    "                continue  # Skip columns starting with '_boltic_' or non-important columns\n",
    "\n",
    "            # Determine corresponding target column\n",
    "            target_col = master_col  # Default: same name\n",
    "            if column_mapping and master_col in column_mapping:\n",
    "                target_col = column_mapping[master_col]\n",
    "\n",
    "            # Check if both columns exist in the merged DataFrame\n",
    "            master_value_col = f\"{master_col}_{table1}\"\n",
    "            target_value_col = f\"{target_col}_{table2}\"\n",
    "            if master_value_col not in row or target_value_col not in row:\n",
    "                continue  # Skip if columns not present\n",
    "\n",
    "            val_master = row.get(master_value_col)\n",
    "            val_target = row.get(target_value_col)\n",
    "\n",
    "            # Handle NaN values in comparison\n",
    "            if pd.isna(val_master) and pd.isna(val_target):\n",
    "                continue  # Both are NaN, treat as equal\n",
    "            elif pd.isna(val_master) or pd.isna(val_target) or val_master != val_target:\n",
    "                mismatch_detail = {\n",
    "                    master_key: key,\n",
    "                    'column': master_col,  # Report master column name\n",
    "                    f'{table1}_value': val_master,\n",
    "                    f'{table2}_value': val_target\n",
    "                }\n",
    "                mismatches.append(mismatch_detail)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'mismatch',\n",
    "                    'error_message': '',\n",
    "                    'source_table': table1,\n",
    "                    'target_table': table2,\n",
    "                    'issue_column': master_col,\n",
    "                    'unique_identifier': f'{master_key}: {key}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(mismatches)} mismatches between '{table1}' and '{table2}'.\")\n",
    "    return mismatches, error_logs_m\n",
    "\n",
    "def find_duplicates(df, key_column, table_name):\n",
    "    \"\"\"\n",
    "    Detect duplicate key_column entries in the DataFrame and identify differences.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to check.\n",
    "        key_column (str): The key column to check for duplicates.\n",
    "        table_name (str): Name of the table being checked.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (duplicate_records_df, error_logs_m)\n",
    "    \"\"\"\n",
    "    if key_column not in df.columns:\n",
    "        logging.error(f\"Key column '{key_column}' not found in DataFrame.\")\n",
    "        return pd.DataFrame(), []\n",
    "\n",
    "    # Get all duplicate entries (keep=False to get all duplicates)\n",
    "    duplicates_df = df[df.duplicated(subset=key_column, keep=False)]\n",
    "\n",
    "    # Group by key_column\n",
    "    grouped = duplicates_df.groupby(key_column)\n",
    "\n",
    "    duplicate_records = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    for key, group in grouped:\n",
    "        if len(group) <= 1:\n",
    "            continue  # Not a duplicate\n",
    "\n",
    "        # Drop key_column and any columns starting with '_boltic_'\n",
    "        group_non_key = group.drop(columns=[key_column] + [col for col in group.columns if col.startswith('_boltic_')])\n",
    "\n",
    "        # Check if all rows are identical\n",
    "        if group_non_key.nunique().sum() == 0:\n",
    "            difference = \"No difference exists\"\n",
    "        else:\n",
    "            # Find which columns have differences\n",
    "            cols_with_diff = group_non_key.columns[group_non_key.nunique() > 1].tolist()\n",
    "            difference = \"Difference in value of columns: \" + ', '.join(cols_with_diff)\n",
    "\n",
    "        duplicate_records.append({\n",
    "            key_column: key,\n",
    "            'Difference in value': difference\n",
    "        })\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'duplicate',\n",
    "            'error_message': f'{difference}',\n",
    "            'source_table': f'{table_name}',\n",
    "            'target_table': '',\n",
    "            'issue_column': '',\n",
    "            'unique_identifier': f'{key_column}: {key}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(duplicate_records)} duplicate entries based on '{key_column}'.\")\n",
    "    return pd.DataFrame(duplicate_records), error_logs_m\n",
    "\n",
    "def validate_data_types(schema_master, schema_target, master_key, table1_name, table2_name, columns_to_check):\n",
    "    \"\"\"\n",
    "    Compare data types of specified columns between master and target schemas.\n",
    "\n",
    "    Args:\n",
    "        schema_master (dict): Schema of the master table.\n",
    "        schema_target (dict): Schema of the target table.\n",
    "        master_key (str): The key column for reference.\n",
    "        table1_name (str): Name of the first table.\n",
    "        table2_name (str): Name of the second table.\n",
    "        columns_to_check (list): List of columns to validate data types.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (data_type_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "    data_type_issues = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    # Identify common columns to check\n",
    "    common_columns = set(columns_to_check).intersection(set(schema_master.keys()), set(schema_target.keys()))\n",
    "\n",
    "    for column in common_columns:\n",
    "        type_master = schema_master[column]\n",
    "        type_target = schema_target[column]\n",
    "        if type_master != type_target:\n",
    "            data_type_issues.append({\n",
    "                'column_name': column,\n",
    "                f'{table1_name}_data_type': type_master,\n",
    "                f'{table2_name}_data_type': type_target\n",
    "            })\n",
    "            error_detail = {\n",
    "                'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'data_type_issues',\n",
    "                'error_message': f'{table1_name}_data_type: {type_master} , {table2_name}_data_type: {type_target}',\n",
    "                'source_table': table1_name,\n",
    "                'target_table': table2_name,\n",
    "                'issue_column': column,\n",
    "                'unique_identifier': ''\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(data_type_issues)} data type issues.\")\n",
    "    return pd.DataFrame(data_type_issues), error_logs_m\n",
    "\n",
    "def validate_formats(df_master, df_target, key_column, target_key, target_table, master_table, columns_to_check):\n",
    "    \"\"\"\n",
    "    Validate specific column formats using regular expressions and include corresponding target table values.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "        master_table (str): The name of the master table.\n",
    "        columns_to_check (list): List of columns to validate formats.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (format_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "    format_issues = pd.DataFrame(columns=[key_column, 'column', 'value', 'issue', f'{target_table}_value'])\n",
    "    error_logs_m = []\n",
    "\n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "\n",
    "        # GSTIN format validation\n",
    "        if 'gstin' in columns_to_check and 'gstin' in df_master.columns:\n",
    "            gstin = str(row['gstin']).strip()\n",
    "            if not re.match(r'^[0-9]{2}[A-Z]{5}[0-9]{4}[A-Z]{1}[A-Z0-9]{3}$', gstin):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['gstin'] if 'gstin' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'gstin',\n",
    "                    'value': row['gstin'],\n",
    "                    'issue': 'Invalid GSTIN format',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Invalid GSTIN format',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'gstin',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "        # Email format validation\n",
    "        if 'email' in columns_to_check and 'email' in df_master.columns:\n",
    "            email = str(row['email']).strip()\n",
    "            if not re.match(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', email):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['email'] if 'email' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'email',\n",
    "                    'value': row['email'],\n",
    "                    'issue': 'Invalid email format',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Invalid email format',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'email',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "        # Pincode format validation\n",
    "        if 'pincode' in columns_to_check and 'pincode' in df_master.columns:\n",
    "            pincode = str(row['pincode']).strip()\n",
    "            if not re.match(r'^\\d{6}$', pincode):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['pincode'] if 'pincode' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'pincode',\n",
    "                    'value': row['pincode'],\n",
    "                    'issue': 'Pincode must be exactly 6 digits',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Pincode must be exactly 6 digits',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'pincode',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "        # Address length validation\n",
    "        if 'address' in columns_to_check and 'address' in df_master.columns:\n",
    "            address = str(row['address']).strip()\n",
    "            if len(address) > 100:\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['address'] if 'address' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'address',\n",
    "                    'value': address,\n",
    "                    'issue': 'Address exceeds 100 characters after stripping',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Address exceeds 100 characters',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'address',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(format_issues)} format issues.\")\n",
    "    return format_issues, error_logs_m\n",
    "\n",
    "# def validate_pincode_mapping(df_master, df_target, key_column, target_key, target_table, client, master_table):\n",
    "#     \"\"\"\n",
    "#     Validate pincode mapping by comparing with the all_india_PO_list reference table.\n",
    "#     If a pincode issue is found in the master table, then check the corresponding pincode in the target table.\n",
    "\n",
    "#     Args:\n",
    "#         df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "#         df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "#         key_column (str): The key column in the master DataFrame.\n",
    "#         target_key (str): The key column in the target DataFrame.\n",
    "#         target_table (str): The name of the target table.\n",
    "#         client (bigquery.Client): Initialized BigQuery client.\n",
    "#         master_table (str): Name of the master table.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (pincode_mapping_issues_df, error_logs_m)\n",
    "#     \"\"\"\n",
    "\n",
    "#     error_logs_m = []\n",
    "#     # Read the reference table from Analytics dataset\n",
    "#     try:\n",
    "#         reference_table = \"all_india_PO_list\"\n",
    "#         reference_dataset = \"analytics_data\"\n",
    "#         query = f\"SELECT pincode, city, state FROM `{PROJECT_ID}.{reference_dataset}.{reference_table}`\"\n",
    "#         reference_df = client.query(query).to_dataframe()\n",
    "#         reference_df['pincode'] = reference_df['pincode'].astype(str).str.strip()\n",
    "#         reference_df['city'] = reference_df['city'].astype(str).str.strip().str.lower()\n",
    "#         reference_df['state'] = reference_df['state'].astype(str).str.strip().str.lower()\n",
    "#         logging.info(f\"Loaded reference pincode mapping from '{reference_table}' in '{reference_dataset}' dataset.\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to load reference pincode mapping: {e}\")\n",
    "#         return pd.DataFrame(), error_logs_m\n",
    "\n",
    "#     # Check if df_master has 'pincode', 'city', 'state' columns\n",
    "#     required_columns = {'pincode', 'city', 'state'}\n",
    "#     if not required_columns.issubset(df_master.columns):\n",
    "#         logging.info(f\"DataFrame does not have required columns for pincode mapping validation: {required_columns}\")\n",
    "#         return pd.DataFrame(), error_logs_m\n",
    "\n",
    "#     # Initialize the issues DataFrame with a single target table details column\n",
    "#     pincode_mapping_issues = pd.DataFrame(columns=[\n",
    "#         key_column, 'pincode', 'state', 'city', 'issue',\n",
    "#         f'{target_table}_details'\n",
    "#     ])\n",
    "\n",
    "#     # Iterate over each row in df_master to validate pincode mapping\n",
    "#     for idx, row in df_master.iterrows():\n",
    "#         key_value = str(row[key_column]).strip()\n",
    "#         pincode = str(row['pincode']).strip()\n",
    "#         city = str(row['city']).strip().lower()\n",
    "#         state = str(row['state']).strip().lower()\n",
    "\n",
    "#         # Fetch corresponding target row if exists\n",
    "#         target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value]\n",
    "#         if not target_row.empty:\n",
    "#             target_row = target_row.iloc[0]\n",
    "#             target_pincode = target_row['pincode'] if 'pincode' in target_row and pd.notnull(target_row['pincode']) else \"Pincode missing\"\n",
    "#             target_state = target_row['state'] if 'state' in target_row and pd.notnull(target_row['state']) else \"State missing\"\n",
    "#             target_city = target_row['city'] if 'city' in target_row and pd.notnull(target_row['city']) else \"City missing\"\n",
    "#             target_details = f\"Pincode: {target_pincode}, State: {target_state}, City: {target_city}\"\n",
    "#         else:\n",
    "#             target_details = f\"Key '{key_column}' with value '{key_value}' not present in target table '{target_table}'.\"\n",
    "\n",
    "#         # Check if pincode exists in reference\n",
    "#         ref_matches = reference_df[reference_df['pincode'] == pincode]\n",
    "#         if ref_matches.empty:\n",
    "#             issue = f\"Invalid pincode ({pincode}).\"\n",
    "#             pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "#                 key_column: key_value,\n",
    "#                 'pincode': pincode,\n",
    "#                 'state': state,\n",
    "#                 'city': city,\n",
    "#                 'issue': issue,\n",
    "#                 f'{target_table}_details': target_details\n",
    "#             }])], ignore_index=True)\n",
    "#             error_detail = {\n",
    "#                 'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "#                 'issue': 'pincode_mapping',\n",
    "#                 'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "#                 'source_table': master_table,\n",
    "#                 'target_table': target_table,\n",
    "#                 'issue_column': 'pincode',\n",
    "#                 'unique_identifier': f'{key_column}: {key_value}'\n",
    "#             }\n",
    "#             error_logs_m.append(error_detail)\n",
    "#             continue\n",
    "\n",
    "#         # Check if any of the reference entries match both the city and state\n",
    "#         exact_match = ref_matches[\n",
    "#             (ref_matches['city'] == city) & (ref_matches['state'] == state)\n",
    "#         ]\n",
    "#         if not exact_match.empty:\n",
    "#             continue  # No issue, mapping is correct\n",
    "\n",
    "#         # Check for state mismatch\n",
    "#         state_matches = ref_matches[ref_matches['state'] == state]\n",
    "\n",
    "#         # Check for city mismatch\n",
    "#         city_matches = ref_matches[ref_matches['city'] == city]\n",
    "\n",
    "#         if state_matches.empty and city_matches.empty:\n",
    "#             # Both state and city do not match\n",
    "#             expected_entries = ref_matches[['state', 'city']].drop_duplicates()\n",
    "#             expected_states = expected_entries['state'].tolist()\n",
    "#             expected_cities = expected_entries['city'].tolist()\n",
    "#             expected_states_str = ', '.join(expected_states)\n",
    "#             expected_cities_str = ', '.join(expected_cities)\n",
    "#             issue = f\"Pincode {pincode} does not match state '{state}' and city '{city}'. Expected states: {expected_states_str}; Expected cities: {expected_cities_str}.\"\n",
    "#         elif state_matches.empty:\n",
    "#             # State does not match\n",
    "#             expected_states = ref_matches['state'].unique().tolist()\n",
    "#             expected_states_str = ', '.join(expected_states)\n",
    "#             issue = f\"Pincode {pincode} does not match state '{state}'. Expected states: {expected_states_str}.\"\n",
    "#         elif city_matches.empty:\n",
    "#             # City does not match\n",
    "#             expected_cities = state_matches['city'].unique().tolist()\n",
    "#             expected_cities_str = ', '.join(expected_cities)\n",
    "#             issue = f\"Pincode {pincode} does not match city '{city}'. Expected cities: {expected_cities_str}.\"\n",
    "#         else:\n",
    "#             # Other cases\n",
    "#             issue = f\"Pincode {pincode} has a mapping inconsistency.\"\n",
    "\n",
    "#         pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "#             key_column: key_value,\n",
    "#             'pincode': pincode,\n",
    "#             'state': state,\n",
    "#             'city': city,\n",
    "#             'issue': issue,\n",
    "#             f'{target_table}_details': target_details\n",
    "#         }])], ignore_index=True)\n",
    "#         error_detail = {\n",
    "#             'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "#             'issue': 'pincode_mapping',\n",
    "#             'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "#             'source_table': master_table,\n",
    "#             'target_table': target_table,\n",
    "#             'issue_column': 'pincode',\n",
    "#             'unique_identifier': f'{key_column}: {key_value}'\n",
    "#         }\n",
    "#         error_logs_m.append(error_detail)\n",
    "\n",
    "#     logging.info(f\"Found {len(pincode_mapping_issues)} pincode mapping issues in master table '{master_table}'.\")\n",
    "#     return pincode_mapping_issues, error_logs_m\n",
    "\n",
    "def create_table(doc, data, column_names):\n",
    "    \"\"\"\n",
    "    Helper function to create a table in a docx document from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        data (list or list of dict): Data to populate the table.\n",
    "        column_names (list): List of column names for the table headers.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    table = doc.add_table(rows=1, cols=len(column_names))\n",
    "    table.style = 'Light List Accent 1'\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for i, col_name in enumerate(column_names):\n",
    "        hdr_cells[i].text = col_name\n",
    "\n",
    "    for row_data in data:\n",
    "        row_cells = table.add_row().cells\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            cell_value = str(row_data.get(col_name, '')).strip()\n",
    "            row_cells[i].text = cell_value\n",
    "    logging.info(\"Added table to the Word document.\")\n",
    "\n",
    "# def add_non_matching_keys_section(doc, df1_only_keys, table1_name, df2_only_keys, table2_name, key_column_master, key_column_target):\n",
    "#     \"\"\"\n",
    "#     Add a section in the Word document for non-matching keys between two tables.\n",
    "\n",
    "#     Args:\n",
    "#         doc (Document): The Word document object.\n",
    "#         df1_only_keys (list): Keys present only in table1.\n",
    "#         table1_name (str): Name of the first table.\n",
    "#         df2_only_keys (list): Keys present only in table2.\n",
    "#         table2_name (str): Name of the second table.\n",
    "#         key_column_master (str): The key column in the master table.\n",
    "#         key_column_target (str): The key column in the target table.\n",
    "#     \"\"\"\n",
    "#     if df1_only_keys or df2_only_keys:\n",
    "#         if df1_only_keys:\n",
    "#             doc.add_heading(f\"'{key_column_master}' present only in '{table1_name}' and not in '{table2_name}' ({len(df1_only_keys)})\", level=2)\n",
    "#             create_table(doc, [{key_column_master: key[key_column_master]} for key in df1_only_keys], [key_column_master])\n",
    "#         if df2_only_keys:\n",
    "#             doc.add_heading(f\"'{key_column_target}' present only in '{table2_name}' and not in '{table1_name}' ({len(df2_only_keys)})\", level=2)\n",
    "#             create_table(doc, [{key_column_target: key[key_column_target]} for key in df2_only_keys], [key_column_target])\n",
    "#     else:\n",
    "#         doc.add_paragraph(\"No non-matching keys found.\")\n",
    "\n",
    "# def add_table_of_contents(doc):\n",
    "#     \"\"\"\n",
    "#     Adds a Table of Contents to the Word document.\n",
    "\n",
    "#     Args:\n",
    "#         doc (Document): The Word document object.\n",
    "#     \"\"\"\n",
    "#     paragraph = doc.add_paragraph()\n",
    "#     run = paragraph.add_run()\n",
    "#     fldChar_begin = OxmlElement('w:fldChar')  # creates a new element\n",
    "#     fldChar_begin.set(qn('w:fldCharType'), 'begin')  # sets attribute on element\n",
    "#     instrText = OxmlElement('w:instrText')\n",
    "#     instrText.set(qn('xml:space'), 'preserve')  # sets attribute on element\n",
    "#     instrText.text = 'TOC \\\\o \"1-2\" \\\\h \\\\z \\\\u'  # change to what you need\n",
    "#     fldChar_separate = OxmlElement('w:fldChar')\n",
    "#     fldChar_separate.set(qn('w:fldCharType'), 'separate')\n",
    "#     fldChar_end = OxmlElement('w:fldChar')\n",
    "#     fldChar_end.set(qn('w:fldCharType'), 'end')\n",
    "#     run._r.append(fldChar_begin)\n",
    "#     run._r.append(instrText)\n",
    "#     run._r.append(fldChar_separate)\n",
    "#     run._r.append(fldChar_end)\n",
    "#     logging.info(\"Added Table of Contents to the Word document.\")\n",
    "\n",
    "# def create_aggregated_document(all_results, base_name):\n",
    "#     \"\"\"\n",
    "#     Creates a single Word document that presents all comparison results for a base table.\n",
    "\n",
    "#     Args:\n",
    "#         all_results (list): List of comparison result dictionaries.\n",
    "#         base_name (str): The base name of the table.\n",
    "\n",
    "#     Returns:\n",
    "#         str: The filepath of the saved report.\n",
    "#     \"\"\"\n",
    "#     doc = Document()\n",
    "#     doc.add_heading(f'{base_name.capitalize()} Tables Comparison Report', level=0)\n",
    "#     doc.add_paragraph(f'Report generated on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "#     # Add Instruction for TOC Update\n",
    "#     doc.add_paragraph(\n",
    "#         \"📌 **Note:** To update the Table of Contents and make the links clickable, go to the ‘References’ tab and click ‘Update Table’ or press F9 in Windows and Fn+F9 in Mac after opening this document in Microsoft Word.\",\n",
    "#         style='Intense Quote'\n",
    "#     )\n",
    "\n",
    "#     # Add Table of Contents\n",
    "#     doc.add_heading('Table of Contents', level=1)\n",
    "#     add_table_of_contents(doc)\n",
    "#     doc.add_page_break()\n",
    "\n",
    "#     for result in all_results:\n",
    "#         table1_name = result['table1_name']\n",
    "#         table2_name = result['table2_name']\n",
    "#         key_column_master = result['key_column_master']\n",
    "#         key_column_target = result['key_column_target']\n",
    "#         doc.add_heading(f'Comparison: {table1_name} vs {table2_name}', level=1)\n",
    "\n",
    "#         # Mismatches\n",
    "#         if result['mismatches']:\n",
    "#             doc.add_heading(f'Mismatches ({len(result[\"mismatches\"])})', level=2)\n",
    "#             column_names = [key_column_master, 'column', f'{table1_name}_value', f'{table2_name}_value']\n",
    "#             create_table(doc, result['mismatches'], column_names)\n",
    "#         else:\n",
    "#             doc.add_heading(\"No mismatches found.\", level=2)\n",
    "\n",
    "#         # Null values in master table\n",
    "#         if not result['null_values_master'].empty:\n",
    "#             count_null_master = len(result['null_values_master'])\n",
    "#             doc.add_heading(f'Null values in {table1_name} ({count_null_master})', level=2)\n",
    "#             column_names = [key_column_master, 'column', table2_name]\n",
    "#             create_table(doc, result['null_values_master'].to_dict('records'), column_names)\n",
    "#         else:\n",
    "#             doc.add_heading(f\"No null values found in {table1_name}.\", level=2)\n",
    "\n",
    "#         # Null values in target table\n",
    "#         if not result['null_values_target'].empty:\n",
    "#             count_null_target = len(result['null_values_target'])\n",
    "#             doc.add_heading(f'Null values in {table2_name} ({count_null_target})', level=2)\n",
    "#             column_names = [key_column_target, 'column', table1_name]\n",
    "#             create_table(doc, result['null_values_target'].to_dict('records'), column_names)\n",
    "#         else:\n",
    "#             doc.add_heading(f\"No null values found in {table2_name}.\", level=2)\n",
    "\n",
    "#         # Duplicate keys in master table\n",
    "#         if not result['duplicates_master'].empty:\n",
    "#             count_dup_master = len(result['duplicates_master'])\n",
    "#             doc.add_heading(f'Duplicate Keys in {table1_name} ({count_dup_master})', level=2)\n",
    "#             create_table(doc, result['duplicates_master'].to_dict('records'), [key_column_master, 'Difference in value'])\n",
    "#         else:\n",
    "#             doc.add_heading(\"No duplicate keys found in master table.\", level=2)\n",
    "\n",
    "#         # Duplicate keys in target table\n",
    "#         if not result['duplicates_target'].empty:\n",
    "#             count_dup_target = len(result['duplicates_target'])\n",
    "#             doc.add_heading(f'Duplicate Keys in {table2_name} ({count_dup_target})', level=2)\n",
    "#             create_table(doc, result['duplicates_target'].to_dict('records'), [key_column_target, 'Difference in value'])\n",
    "#         else:\n",
    "#             doc.add_heading(f\"No duplicate keys found in {table2_name}.\", level=2)\n",
    "\n",
    "#         # Data type issues\n",
    "#         if not result['data_type_issues'].empty:\n",
    "#             count_data_type_issues = len(result['data_type_issues'])\n",
    "#             doc.add_heading(f'Data Type Issues ({count_data_type_issues})', level=2)\n",
    "#             column_names = ['column_name', f'{table1_name}_data_type', f'{table2_name}_data_type']\n",
    "#             create_table(doc, result['data_type_issues'].to_dict('records'), column_names)\n",
    "#         else:\n",
    "#             doc.add_heading(\"No data type issues found.\", level=2)\n",
    "\n",
    "#         # Format issues in master table with target values\n",
    "#         if not result['format_issues_master'].empty:\n",
    "#             count_format_issues_master = len(result['format_issues_master'])\n",
    "#             doc.add_heading(f'Format Issues in {table1_name} ({count_format_issues_master})', level=2)\n",
    "#             column_names_master = [key_column_master, 'column', 'value', 'issue', f'{table2_name}_value']\n",
    "#             create_table(doc, result['format_issues_master'].to_dict('records'), column_names_master)\n",
    "#         else:\n",
    "#             doc.add_heading(f\"No format issues found in {table1_name}.\", level=2)\n",
    "\n",
    "#         # Pincode Mapping Issues with target details\n",
    "#         if not result['pincode_mapping_issues'].empty:\n",
    "#             count_pincode_issues = len(result['pincode_mapping_issues'])\n",
    "#             doc.add_heading(f'Pincode Mapping Issues in {table1_name} ({count_pincode_issues})', level=2)\n",
    "#             column_names = [\n",
    "#                 key_column_master, 'pincode', 'state', 'city', 'issue',\n",
    "#                 f'{table2_name}_details'\n",
    "#             ]\n",
    "#             create_table(doc, result['pincode_mapping_issues'].to_dict('records'), column_names)\n",
    "#         else:\n",
    "#             doc.add_heading(\"No pincode mapping issues found.\", level=2)\n",
    "\n",
    "#         # Non-matching keys in master DataFrame\n",
    "#         if result['df_master_only_keys']:\n",
    "#             count_master_only = len(result['df_master_only_keys'])\n",
    "#             doc.add_heading(f'Keys only in {table1_name} ({count_master_only})', level=2)\n",
    "#             column_names = [key_column_master]\n",
    "#             create_table(doc, result['df_master_only_keys'], column_names)\n",
    "#         else:\n",
    "#             doc.add_heading(f\"No keys found only in {table1_name}.\", level=2)\n",
    "\n",
    "#         # Non-matching keys in target DataFrame\n",
    "#         if result['df_target_only_keys']:\n",
    "#             count_target_only = len(result['df_target_only_keys'])\n",
    "#             doc.add_heading(f'Keys only in {table2_name} ({count_target_only})', level=2)\n",
    "#             column_names = [key_column_target]\n",
    "#             create_table(doc, result['df_target_only_keys'], column_names)\n",
    "#         else:\n",
    "#             doc.add_heading(f\"No keys found only in {table2_name}.\", level=2)\n",
    "\n",
    "#         doc.add_page_break()  # Optional: Add a page break between comparisons\n",
    "\n",
    "#     # Save the aggregated document to the current directory\n",
    "#     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#     report_filename = f\"{base_name}_comparison_report_aggregated_{timestamp}.docx\"\n",
    "#     doc.save(report_filename)\n",
    "#     logging.info(f\"Saved aggregated comparison report as '{report_filename}'.\")\n",
    "\n",
    "#     return report_filename  # Return the filename for further processing\n",
    "\n",
    "# def send_slack_alert(message):\n",
    "#     \"\"\"\n",
    "#     Send a message to a specified Slack channel.\n",
    "\n",
    "#     Args:\n",
    "#         message (str): The message to send.\n",
    "#     \"\"\"\n",
    "#     if not slack_client:\n",
    "#         logging.warning(\"Slack client is not initialized. Skipping Slack notification.\")\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         response = slack_client.chat_postMessage(\n",
    "#             channel=SLACK_CHANNEL,\n",
    "#             text=message\n",
    "#         )\n",
    "#         logging.info(f\"Message sent to {SLACK_CHANNEL}: {response['ts']}\")\n",
    "#     except SlackApiError as e:\n",
    "#         logging.error(f\"Error sending message to Slack: {e.response['error']}\")\n",
    "\n",
    "# def upload_file_to_slack(filepath, title=None):\n",
    "#     \"\"\"\n",
    "#     Upload a file to the specified Slack channel using files_upload_v2.\n",
    "\n",
    "#     Args:\n",
    "#         filepath (str): The path to the file to upload.\n",
    "#         title (str, optional): The title for the uploaded file. Defaults to the file's basename.\n",
    "#     \"\"\"\n",
    "#     if not slack_client:\n",
    "#         logging.warning(\"Slack client is not initialized. Skipping file upload.\")\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         with open(filepath, 'rb') as f:\n",
    "#             response = slack_client.files_upload_v2(\n",
    "#                 channel=SLACK_CHANNEL,\n",
    "#                 file=f,\n",
    "#                 filename=os.path.basename(filepath),  # Explicitly set the filename with extension\n",
    "#                 title=title if title else os.path.basename(filepath),  # Set the title\n",
    "#                 initial_comment=title if title else \"File uploaded.\"  # Optional: Add an initial comment\n",
    "#             )\n",
    "\n",
    "#         # Verify if the upload was successful\n",
    "#         if response.get('ok'):\n",
    "#             file_permalink = response['file']['permalink']\n",
    "#             logging.info(f\"File uploaded to Slack channel '{SLACK_CHANNEL}': {file_permalink}\")\n",
    "#         else:\n",
    "#             logging.error(f\"Failed to upload file to Slack: {response}\")\n",
    "#     except SlackApiError as e:\n",
    "#         logging.error(f\"Slack API Error during file upload: {e.response['error']}\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Unexpected error during file upload: {e}\")\n",
    "\n",
    "# def find_non_matching_keys(df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table):\n",
    "#     \"\"\"\n",
    "#     Identify keys present in df_master but not in df_target and vice versa, including duplicates.\n",
    "\n",
    "#     Args:\n",
    "#         df_master (pd.DataFrame): Source DataFrame.\n",
    "#         df_target (pd.DataFrame): Target DataFrame.\n",
    "#         master_key (str): The key column in the master table.\n",
    "#         target_key (str): The key column in the target table.\n",
    "#         duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "#         duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "#         master_table (str): Name of the master table.\n",
    "#         target_table (str): Name of the target table.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (master_only_keys, target_only_keys, error_logs_m)\n",
    "#     \"\"\"\n",
    "#     error_logs_m = []\n",
    "#     # Include all keys, including duplicates\n",
    "#     keys_master = set(df_master[master_key].astype(str).str.strip())\n",
    "#     keys_target = set(df_target[target_key].astype(str).str.strip())\n",
    "\n",
    "#     # Keys present only in master\n",
    "#     master_only = keys_master - keys_target\n",
    "#     # Keys present only in target\n",
    "#     target_only = keys_target - keys_master\n",
    "\n",
    "#     logging.info(f\"Found {len(master_only)} keys in source not in target and {len(target_only)} keys in target not in source.\")\n",
    "\n",
    "#     # Convert to list of dictionaries for consistency\n",
    "#     master_only_keys = [{master_key: key} for key in master_only]\n",
    "#     target_only_keys = [{target_key: key} for key in target_only]\n",
    "\n",
    "#     # Log errors for keys only in master\n",
    "#     for key in master_only:\n",
    "#         error_detail = {\n",
    "#             'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "#             'issue': 'missing_key',\n",
    "#             'error_message': f\"Key '{master_key}' with value '{key}' is present only in '{master_table}' and missing in '{target_table}'.\",\n",
    "#             'source_table': master_table,\n",
    "#             'target_table': target_table, \n",
    "#             'issue_column': master_key,\n",
    "#             'unique_identifier': f\"{master_key}: {key}\"\n",
    "#         }\n",
    "#         error_logs_m.append(error_detail)\n",
    "\n",
    "#     # Log errors for keys only in target\n",
    "#     for key in target_only:\n",
    "#         error_detail = {\n",
    "#             'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "#             'issue': 'missing_key',\n",
    "#             'error_message': f\"Key '{target_key}' with value '{key}' is present only in '{target_table}' and missing in '{master_table}'.\",\n",
    "#             'source_table': target_table,\n",
    "#             'target_table': master_table,\n",
    "#             'issue_column': target_key,\n",
    "#             'unique_identifier': f\"{target_key}: {key}\"\n",
    "#         }\n",
    "#         error_logs_m.append(error_detail)\n",
    "\n",
    "#     return master_only_keys, target_only_keys, error_logs_m\n",
    "\n",
    "# def find_detailed_nulls(df_master, df_target, master_key, target_key, master_table, target_table, columns_to_check):\n",
    "#     \"\"\"\n",
    "#     Identify null values in both master and target tables for specified columns and fetch corresponding values or indicate missing keys.\n",
    "\n",
    "#     Args:\n",
    "#         df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "#         df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "#         master_key (str): The key column in the master table.\n",
    "#         target_key (str): The key column in the target table.\n",
    "#         master_table (str): Name of the master table.\n",
    "#         target_table (str): Name of the target table.\n",
    "#         columns_to_check (list): List of columns to check for null values.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (null_values_master, null_values_target, error_logs_m)\n",
    "#     \"\"\"\n",
    "#     null_values_master = []\n",
    "#     null_values_target = []\n",
    "#     error_logs_m = []\n",
    "\n",
    "#     # Find nulls in master\n",
    "#     null_master = df_master[df_master[columns_to_check].isnull().any(axis=1)]\n",
    "#     for idx, row in null_master.iterrows():\n",
    "#         key_value = str(row[master_key]).strip()\n",
    "#         for column in columns_to_check:\n",
    "#             if column == master_key or column.startswith('_boltic_'):\n",
    "#                 continue  # Skip key column and non-important columns\n",
    "#             if column not in row:\n",
    "#                 continue  # Skip if column is not in the row\n",
    "#             if pd.isnull(row[column]):\n",
    "#                 if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "#                     target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "#                     target_value = target_row[column] if column in target_row else \"Column not present\"\n",
    "#                 else:\n",
    "#                     target_value = f\"'{target_key}' not present\"\n",
    "#                 null_record = {\n",
    "#                     master_key: key_value,\n",
    "#                     'column': column,\n",
    "#                     target_table: target_value\n",
    "#                 }\n",
    "#                 error_detail = {\n",
    "#                     'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "#                     'issue': 'null',\n",
    "#                     'error_message': 'Null in columns',\n",
    "#                     'source_table': master_table,\n",
    "#                     'target_table': '',\n",
    "#                     'issue_column': column,\n",
    "#                     'unique_identifier': f'{master_key} : {key_value}'\n",
    "#                 }\n",
    "#                 error_logs_m.append(error_detail)\n",
    "#                 null_values_master.append(null_record)\n",
    "\n",
    "#     # Find nulls in target\n",
    "#     null_target = df_target[df_target[columns_to_check].isnull().any(axis=1)]\n",
    "#     for idx, row in null_target.iterrows():\n",
    "#         key_value = str(row[target_key]).strip()\n",
    "#         for column in columns_to_check:\n",
    "#             if column == target_key or column.startswith('_boltic_'):\n",
    "#                 continue  # Skip key column and non-important columns\n",
    "#             if column not in row:\n",
    "#                 continue  # Skip if column is not in the row\n",
    "#             if pd.isnull(row[column]):\n",
    "#                 if key_value in df_master[master_key].astype(str).str.strip().values:\n",
    "#                     master_row = df_master[df_master[master_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "#                     master_value = master_row[column] if column in master_row else \"Column not present\"\n",
    "#                 else:\n",
    "#                     master_value = f\"'{master_key}' not present\"\n",
    "#                 null_record = {\n",
    "#                     target_key: key_value,\n",
    "#                     'column': column,\n",
    "#                     master_table: master_value\n",
    "#                 }\n",
    "#                 error_detail = {\n",
    "#                     'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "#                     'issue': 'null',\n",
    "#                     'error_message': 'Null in columns',\n",
    "#                     'source_table': target_table,\n",
    "#                     'target_table': '',\n",
    "#                     'issue_column': column,\n",
    "#                     'unique_identifier': f'{target_key} : {key_value}'\n",
    "#                 }\n",
    "#                 error_logs_m.append(error_detail)\n",
    "#                 null_values_target.append(null_record)\n",
    "\n",
    "#     logging.info(f\"Found {len(null_values_master)} null values in master table '{master_table}'.\")\n",
    "#     logging.info(f\"Found {len(null_values_target)} null values in target table '{target_table}'.\")\n",
    "#     return null_values_master, null_values_target, error_logs_m\n",
    "\n",
    "def validate_pincode_mapping(df_master, df_target, key_column, target_key, target_table, client, master_table):\n",
    "    \"\"\"\n",
    "    Validate pincode mapping by comparing with the all_india_PO_list reference table.\n",
    "    If a pincode issue is found in the master table, then check the corresponding pincode in the target table.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        master_table (str): Name of the master table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pincode_mapping_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "\n",
    "    error_logs_m = []\n",
    "    # Read the reference table from Analytics dataset\n",
    "    try:\n",
    "        reference_table = \"all_india_po_list\"\n",
    "        reference_dataset = \"analytics_data\"\n",
    "        query = f\"SELECT pincode, city, state FROM `{PROJECT_ID}.{reference_dataset}.{reference_table}`\"\n",
    "        reference_df = client.query(query).to_dataframe()\n",
    "        reference_df['pincode'] = reference_df['pincode'].astype(str).str.strip()\n",
    "        reference_df['city'] = reference_df['city'].astype(str).str.strip().str.lower()\n",
    "        reference_df['state'] = reference_df['state'].astype(str).str.strip().str.lower()\n",
    "        logging.info(f\"Loaded reference pincode mapping from '{reference_table}' in '{reference_dataset}' dataset.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load reference pincode mapping: {e}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Check if df_master has 'pincode', 'city', 'state' columns\n",
    "    required_columns = {'pincode', 'city', 'state'}\n",
    "    if not required_columns.issubset(df_master.columns):\n",
    "        logging.info(f\"DataFrame does not have required columns for pincode mapping validation: {required_columns}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Initialize the issues DataFrame with a single target table details column\n",
    "    pincode_mapping_issues = pd.DataFrame(columns=[\n",
    "        key_column, 'pincode', 'state', 'city', 'issue',\n",
    "        f'{target_table}_details'\n",
    "    ])\n",
    "\n",
    "    # Iterate over each row in df_master to validate pincode mapping\n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "        pincode = str(row['pincode']).strip()\n",
    "        city = str(row['city']).strip().lower()\n",
    "        state = str(row['state']).strip().lower()\n",
    "\n",
    "        # Fetch corresponding target row if exists\n",
    "        target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value]\n",
    "        if not target_row.empty:\n",
    "            target_row = target_row.iloc[0]\n",
    "            target_pincode = target_row['pincode'] if 'pincode' in target_row and pd.notnull(target_row['pincode']) else \"Pincode missing\"\n",
    "            target_state = target_row['state'] if 'state' in target_row and pd.notnull(target_row['state']) else \"State missing\"\n",
    "            target_city = target_row['city'] if 'city' in target_row and pd.notnull(target_row['city']) else \"City missing\"\n",
    "            target_details = f\"Pincode: {target_pincode}, State: {target_state}, City: {target_city}\"\n",
    "        else:\n",
    "            target_details = f\"Key '{key_column}' with value '{key_value}' not present in target table '{target_table}'.\"\n",
    "\n",
    "        # Check if pincode exists in reference\n",
    "        ref_matches = reference_df[reference_df['pincode'] == pincode]\n",
    "        if ref_matches.empty:\n",
    "            issue = f\"Invalid pincode ({pincode}).\"\n",
    "            pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "                key_column: key_value,\n",
    "                'pincode': pincode,\n",
    "                'state': state,\n",
    "                'city': city,\n",
    "                'issue': issue,\n",
    "                f'{target_table}_details': target_details\n",
    "            }])], ignore_index=True)\n",
    "            error_detail = {\n",
    "                'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'pincode_mapping',\n",
    "                'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "                'source_table': master_table,\n",
    "                'target_table': target_table,\n",
    "                'issue_column': 'pincode',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "            continue\n",
    "\n",
    "        # Check if any of the reference entries match both the city and state\n",
    "        exact_match = ref_matches[\n",
    "            (ref_matches['city'] == city) & (ref_matches['state'] == state)\n",
    "        ]\n",
    "        if not exact_match.empty:\n",
    "            continue  # No issue, mapping is correct\n",
    "\n",
    "        # Check for state mismatch\n",
    "        state_matches = ref_matches[ref_matches['state'] == state]\n",
    "\n",
    "        # Check for city mismatch\n",
    "        city_matches = ref_matches[ref_matches['city'] == city]\n",
    "\n",
    "        if state_matches.empty and city_matches.empty:\n",
    "            # Both state and city do not match\n",
    "            expected_entries = ref_matches[['state', 'city']].drop_duplicates()\n",
    "            expected_states = expected_entries['state'].tolist()\n",
    "            expected_cities = expected_entries['city'].tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}' and city '{city}'. Expected states: {expected_states_str}; Expected cities: {expected_cities_str}.\"\n",
    "        elif state_matches.empty:\n",
    "            # State does not match\n",
    "            expected_states = ref_matches['state'].unique().tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}'. Expected states: {expected_states_str}.\"\n",
    "        elif city_matches.empty:\n",
    "            # City does not match\n",
    "            expected_cities = state_matches['city'].unique().tolist()\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match city '{city}'. Expected cities: {expected_cities_str}.\"\n",
    "        else:\n",
    "            # Other cases\n",
    "            issue = f\"Pincode {pincode} has a mapping inconsistency.\"\n",
    "\n",
    "        pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "            key_column: key_value,\n",
    "            'pincode': pincode,\n",
    "            'state': state,\n",
    "            'city': city,\n",
    "            'issue': issue,\n",
    "            f'{target_table}_details': target_details\n",
    "        }])], ignore_index=True)\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'pincode_mapping',\n",
    "            'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table,\n",
    "            'issue_column': 'pincode',\n",
    "            'unique_identifier': f'{key_column}: {key_value}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(pincode_mapping_issues)} pincode mapping issues in master table '{master_table}'.\")\n",
    "    return pincode_mapping_issues, error_logs_m\n",
    "\n",
    "def create_table(doc, data, column_names):\n",
    "    \"\"\"\n",
    "    Helper function to create a table in a docx document from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        data (list or list of dict): Data to populate the table.\n",
    "        column_names (list): List of column names for the table headers.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    table = doc.add_table(rows=1, cols=len(column_names))\n",
    "    table.style = 'Light List Accent 1'\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for i, col_name in enumerate(column_names):\n",
    "        hdr_cells[i].text = col_name\n",
    "\n",
    "    for row_data in data:\n",
    "        row_cells = table.add_row().cells\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            cell_value = str(row_data.get(col_name, '')).strip()\n",
    "            row_cells[i].text = cell_value\n",
    "    logging.info(\"Added table to the Word document.\")\n",
    "\n",
    "def add_non_matching_keys_section(doc, df1_only_keys, table1_name, df2_only_keys, table2_name, key_column_master, key_column_target):\n",
    "    \"\"\"\n",
    "    Add a section in the Word document for non-matching keys between two tables.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        df1_only_keys (list): Keys present only in table1.\n",
    "        table1_name (str): Name of the first table.\n",
    "        df2_only_keys (list): Keys present only in table2.\n",
    "        table2_name (str): Name of the second table.\n",
    "        key_column_master (str): The key column in the master table.\n",
    "        key_column_target (str): The key column in the target table.\n",
    "    \"\"\"\n",
    "    if df1_only_keys or df2_only_keys:\n",
    "        if df1_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_master}' present only in '{table1_name}' and not in '{table2_name}' ({len(df1_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_master: key[key_column_master]} for key in df1_only_keys], [key_column_master])\n",
    "        if df2_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_target}' present only in '{table2_name}' and not in '{table1_name}' ({len(df2_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_target: key[key_column_target]} for key in df2_only_keys], [key_column_target])\n",
    "    else:\n",
    "        doc.add_paragraph(\"No non-matching keys found.\")\n",
    "\n",
    "def add_table_of_contents(doc):\n",
    "    \"\"\"\n",
    "    Adds a Table of Contents to the Word document.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "    \"\"\"\n",
    "    paragraph = doc.add_paragraph()\n",
    "    run = paragraph.add_run()\n",
    "    fldChar_begin = OxmlElement('w:fldChar')  # creates a new element\n",
    "    fldChar_begin.set(qn('w:fldCharType'), 'begin')  # sets attribute on element\n",
    "    instrText = OxmlElement('w:instrText')\n",
    "    instrText.set(qn('xml:space'), 'preserve')  # sets attribute on element\n",
    "    instrText.text = 'TOC \\\\o \"1-2\" \\\\h \\\\z \\\\u'  # change to what you need\n",
    "    fldChar_separate = OxmlElement('w:fldChar')\n",
    "    fldChar_separate.set(qn('w:fldCharType'), 'separate')\n",
    "    fldChar_end = OxmlElement('w:fldChar')\n",
    "    fldChar_end.set(qn('w:fldCharType'), 'end')\n",
    "    run._r.append(fldChar_begin)\n",
    "    run._r.append(instrText)\n",
    "    run._r.append(fldChar_separate)\n",
    "    run._r.append(fldChar_end)\n",
    "    logging.info(\"Added Table of Contents to the Word document.\")\n",
    "\n",
    "def create_aggregated_document(all_results, base_name):\n",
    "    \"\"\"\n",
    "    Creates a single Word document that presents all comparison results for a base table.\n",
    "\n",
    "    Args:\n",
    "        all_results (list): List of comparison result dictionaries.\n",
    "        base_name (str): The base name of the table.\n",
    "\n",
    "    Returns:\n",
    "        str: The filepath of the saved report.\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "    doc.add_heading(f'{base_name.capitalize()} Tables Comparison Report', level=0)\n",
    "    doc.add_paragraph(f'Report generated on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "    # Add Instruction for TOC Update\n",
    "    doc.add_paragraph(\n",
    "        \"📌 **Note:** To update the Table of Contents and make the links clickable, go to the ‘References’ tab and click ‘Update Table’ or press F9 in Windows and Fn+F9 in Mac after opening this document in Microsoft Word.\",\n",
    "        style='Intense Quote'\n",
    "    )\n",
    "\n",
    "    # Add Table of Contents\n",
    "    doc.add_heading('Table of Contents', level=1)\n",
    "    add_table_of_contents(doc)\n",
    "    doc.add_page_break()\n",
    "\n",
    "    for result in all_results:\n",
    "        table1_name = result['table1_name']\n",
    "        table2_name = result['table2_name']\n",
    "        key_column_master = result['key_column_master']\n",
    "        key_column_target = result['key_column_target']\n",
    "        doc.add_heading(f'Comparison: {table1_name} vs {table2_name}', level=1)\n",
    "\n",
    "        # Mismatches\n",
    "        if result['mismatches']:\n",
    "            doc.add_heading(f'Mismatches ({len(result[\"mismatches\"])})', level=2)\n",
    "            column_names = [key_column_master, 'column', f'{table1_name}_value', f'{table2_name}_value']\n",
    "            create_table(doc, result['mismatches'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No mismatches found.\", level=2)\n",
    "\n",
    "        # Null values in master table\n",
    "        if not result['null_values_master'].empty:\n",
    "            count_null_master = len(result['null_values_master'])\n",
    "            doc.add_heading(f'Null values in {table1_name} ({count_null_master})', level=2)\n",
    "            column_names = [key_column_master, 'column', table2_name]\n",
    "            create_table(doc, result['null_values_master'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table1_name}.\", level=2)\n",
    "\n",
    "        # Null values in target table\n",
    "        if not result['null_values_target'].empty:\n",
    "            count_null_target = len(result['null_values_target'])\n",
    "            doc.add_heading(f'Null values in {table2_name} ({count_null_target})', level=2)\n",
    "            column_names = [key_column_target, 'column', table1_name]\n",
    "            create_table(doc, result['null_values_target'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Duplicate keys in master table\n",
    "        if not result['duplicates_master'].empty:\n",
    "            count_dup_master = len(result['duplicates_master'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table1_name} ({count_dup_master})', level=2)\n",
    "            create_table(doc, result['duplicates_master'].to_dict('records'), [key_column_master, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(\"No duplicate keys found in master table.\", level=2)\n",
    "\n",
    "        # Duplicate keys in target table\n",
    "        if not result['duplicates_target'].empty:\n",
    "            count_dup_target = len(result['duplicates_target'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table2_name} ({count_dup_target})', level=2)\n",
    "            create_table(doc, result['duplicates_target'].to_dict('records'), [key_column_target, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(f\"No duplicate keys found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Data type issues\n",
    "        if not result['data_type_issues'].empty:\n",
    "            count_data_type_issues = len(result['data_type_issues'])\n",
    "            doc.add_heading(f'Data Type Issues ({count_data_type_issues})', level=2)\n",
    "            column_names = ['column_name', f'{table1_name}_data_type', f'{table2_name}_data_type']\n",
    "            create_table(doc, result['data_type_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No data type issues found.\", level=2)\n",
    "\n",
    "        # Format issues in master table with target values\n",
    "        if not result['format_issues_master'].empty:\n",
    "            count_format_issues_master = len(result['format_issues_master'])\n",
    "            doc.add_heading(f'Format Issues in {table1_name} ({count_format_issues_master})', level=2)\n",
    "            column_names_master = [key_column_master, 'column', 'value', 'issue', f'{table2_name}_value']\n",
    "            create_table(doc, result['format_issues_master'].to_dict('records'), column_names_master)\n",
    "        else:\n",
    "            doc.add_heading(f\"No format issues found in {table1_name}.\", level=2)\n",
    "\n",
    "        # Pincode Mapping Issues with target details\n",
    "        if not result['pincode_mapping_issues'].empty:\n",
    "            count_pincode_issues = len(result['pincode_mapping_issues'])\n",
    "            doc.add_heading(f'Pincode Mapping Issues in {table1_name} ({count_pincode_issues})', level=2)\n",
    "            column_names = [\n",
    "                key_column_master, 'pincode', 'state', 'city', 'issue',\n",
    "                f'{table2_name}_details'\n",
    "            ]\n",
    "            create_table(doc, result['pincode_mapping_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No pincode mapping issues found.\", level=2)\n",
    "\n",
    "        # Non-matching keys in master DataFrame\n",
    "        if result['df_master_only_keys']:\n",
    "            count_master_only = len(result['df_master_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table1_name} ({count_master_only})', level=2)\n",
    "            column_names = [key_column_master]\n",
    "            create_table(doc, result['df_master_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table1_name}.\", level=2)\n",
    "\n",
    "        # Non-matching keys in target DataFrame\n",
    "        if result['df_target_only_keys']:\n",
    "            count_target_only = len(result['df_target_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table2_name} ({count_target_only})', level=2)\n",
    "            column_names = [key_column_target]\n",
    "            create_table(doc, result['df_target_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table2_name}.\", level=2)\n",
    "\n",
    "        doc.add_page_break()  # Optional: Add a page break between comparisons\n",
    "\n",
    "    # Save the aggregated document to the current directory\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_filename = f\"{base_name}_comparison_report_aggregated_{timestamp}.docx\"\n",
    "    doc.save(report_filename)\n",
    "    logging.info(f\"Saved aggregated comparison report as '{report_filename}'.\")\n",
    "\n",
    "    return report_filename  # Return the filename for further processing\n",
    "\n",
    "def send_slack_alert(message):\n",
    "    \"\"\"\n",
    "    Send a message to a specified Slack channel.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to send.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping Slack notification.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = slack_client.chat_postMessage(\n",
    "            channel=SLACK_CHANNEL,\n",
    "            text=message\n",
    "        )\n",
    "        logging.info(f\"Message sent to {SLACK_CHANNEL}: {response['ts']}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Error sending message to Slack: {e.response['error']}\")\n",
    "\n",
    "def upload_file_to_slack(filepath, title=None):\n",
    "    \"\"\"\n",
    "    Upload a file to the specified Slack channel using files_upload_v2.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the file to upload.\n",
    "        title (str, optional): The title for the uploaded file. Defaults to the file's basename.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping file upload.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            response = slack_client.files_upload_v2(\n",
    "                channel=SLACK_CHANNEL,\n",
    "                file=f,\n",
    "                filename=os.path.basename(filepath),  # Explicitly set the filename with extension\n",
    "                title=title if title else os.path.basename(filepath),  # Set the title\n",
    "                initial_comment=title if title else \"File uploaded.\"  # Optional: Add an initial comment\n",
    "            )\n",
    "\n",
    "        # Verify if the upload was successful\n",
    "        if response.get('ok'):\n",
    "            file_permalink = response['file']['permalink']\n",
    "            logging.info(f\"File uploaded to Slack channel '{SLACK_CHANNEL}': {file_permalink}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to upload file to Slack: {response}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Slack API Error during file upload: {e.response['error']}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during file upload: {e}\")\n",
    "\n",
    "def find_non_matching_keys(df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table):\n",
    "    \"\"\"\n",
    "    Identify keys present in df_master but not in df_target and vice versa, including duplicates.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame.\n",
    "        df_target (pd.DataFrame): Target DataFrame.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "        duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "        master_table (str): Name of the master table.\n",
    "        target_table (str): Name of the target table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (master_only_keys, target_only_keys, error_logs_m)\n",
    "    \"\"\"\n",
    "    error_logs_m = []\n",
    "    # Include all keys, including duplicates\n",
    "    keys_master = set(df_master[master_key].astype(str).str.strip())\n",
    "    keys_target = set(df_target[target_key].astype(str).str.strip())\n",
    "\n",
    "    # Keys present only in master\n",
    "    master_only = keys_master - keys_target\n",
    "    # Keys present only in target\n",
    "    target_only = keys_target - keys_master\n",
    "\n",
    "    logging.info(f\"Found {len(master_only)} keys in source not in target and {len(target_only)} keys in target not in source.\")\n",
    "\n",
    "    # Convert to list of dictionaries for consistency\n",
    "    master_only_keys = [{master_key: key} for key in master_only]\n",
    "    target_only_keys = [{target_key: key} for key in target_only]\n",
    "\n",
    "    # Log errors for keys only in master\n",
    "    for key in master_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{master_key}' with value '{key}' is present only in '{master_table}' and missing in '{target_table}'.\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table, \n",
    "            'issue_column': master_key,\n",
    "            'unique_identifier': f\"{master_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    # Log errors for keys only in target\n",
    "    for key in target_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{target_key}' with value '{key}' is present only in '{target_table}' and missing in '{master_table}'.\",\n",
    "            'source_table': target_table,\n",
    "            'target_table': master_table,\n",
    "            'issue_column': target_key,\n",
    "            'unique_identifier': f\"{target_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    return master_only_keys, target_only_keys, error_logs_m\n",
    "\n",
    "def find_detailed_nulls(df_master, df_target, master_key, target_key, master_table, target_table, columns_to_check):\n",
    "    \"\"\"\n",
    "    Identify null values in both master and target tables for specified columns and fetch corresponding values or indicate missing keys.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "        df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        master_table (str): Name of the master table.\n",
    "        target_table (str): Name of the target table.\n",
    "        columns_to_check (list): List of columns to check for null values.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (null_values_master, null_values_target, error_logs_m)\n",
    "    \"\"\"\n",
    "    null_values_master = []\n",
    "    null_values_target = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    # Find nulls in master\n",
    "    null_master = df_master[df_master[columns_to_check].isnull().any(axis=1)]\n",
    "    for idx, row in null_master.iterrows():\n",
    "        key_value = str(row[master_key]).strip()\n",
    "        for column in columns_to_check:\n",
    "            if column == master_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and non-important columns\n",
    "            if column not in row:\n",
    "                continue  # Skip if column is not in the row\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row[column] if column in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                null_record = {\n",
    "                    master_key: key_value,\n",
    "                    'column': column,\n",
    "                    target_table: target_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'null',\n",
    "                    'error_message': 'Null in columns',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': column,\n",
    "                    'unique_identifier': f'{master_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_master.append(null_record)\n",
    "\n",
    "    # Find nulls in target\n",
    "    null_target = df_target[df_target[columns_to_check].isnull().any(axis=1)]\n",
    "    for idx, row in null_target.iterrows():\n",
    "        key_value = str(row[target_key]).strip()\n",
    "        for column in columns_to_check:\n",
    "            if column == target_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and non-important columns\n",
    "            if column not in row:\n",
    "                continue  # Skip if column is not in the row\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_master[master_key].astype(str).str.strip().values:\n",
    "                    master_row = df_master[df_master[master_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    master_value = master_row[column] if column in master_row else \"Column not present\"\n",
    "                else:\n",
    "                    master_value = f\"'{master_key}' not present\"\n",
    "                null_record = {\n",
    "                    target_key: key_value,\n",
    "                    'column': column,\n",
    "                    master_table: master_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'null',\n",
    "                    'error_message': 'Null in columns',\n",
    "                    'source_table': target_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': column,\n",
    "                    'unique_identifier': f'{target_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_target.append(null_record)\n",
    "\n",
    "    logging.info(f\"Found {len(null_values_master)} null values in master table '{master_table}'.\")\n",
    "    logging.info(f\"Found {len(null_values_target)} null values in target table '{target_table}'.\")\n",
    "    return null_values_master, null_values_target, error_logs_m\n",
    "\n",
    "def validate_pincode_mapping(df_master, df_target, key_column, target_key, target_table, client, master_table):\n",
    "    \"\"\"\n",
    "    Validate pincode mapping by comparing with the all_india_PO_list reference table.\n",
    "    If a pincode issue is found in the master table, then check the corresponding pincode in the target table.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        master_table (str): Name of the master table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pincode_mapping_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "\n",
    "    error_logs_m = []\n",
    "    # Read the reference table from Analytics dataset\n",
    "    try:\n",
    "        reference_table = \"all_india_po_list\"\n",
    "        reference_dataset = \"analytics_data\"\n",
    "        query = f\"SELECT pincode, city, state FROM `{PROJECT_ID}.{reference_dataset}.{reference_table}`\"\n",
    "        reference_df = client.query(query).to_dataframe()\n",
    "        reference_df['pincode'] = reference_df['pincode'].astype(str).str.strip()\n",
    "        reference_df['city'] = reference_df['city'].astype(str).str.strip().str.lower()\n",
    "        reference_df['state'] = reference_df['state'].astype(str).str.strip().str.lower()\n",
    "        logging.info(f\"Loaded reference pincode mapping from '{reference_table}' in '{reference_dataset}' dataset.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load reference pincode mapping: {e}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Check if df_master has 'pincode', 'city', 'state' columns\n",
    "    required_columns = {'pincode', 'city', 'state'}\n",
    "    if not required_columns.issubset(df_master.columns):\n",
    "        logging.info(f\"DataFrame does not have required columns for pincode mapping validation: {required_columns}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Initialize the issues DataFrame with a single target table details column\n",
    "    pincode_mapping_issues = pd.DataFrame(columns=[\n",
    "        key_column, 'pincode', 'state', 'city', 'issue',\n",
    "        f'{target_table}_details'\n",
    "    ])\n",
    "\n",
    "    # Iterate over each row in df_master to validate pincode mapping\n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "        pincode = str(row['pincode']).strip()\n",
    "        city = str(row['city']).strip().lower()\n",
    "        state = str(row['state']).strip().lower()\n",
    "\n",
    "        # Fetch corresponding target row if exists\n",
    "        target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value]\n",
    "        if not target_row.empty:\n",
    "            target_row = target_row.iloc[0]\n",
    "            target_pincode = target_row['pincode'] if 'pincode' in target_row and pd.notnull(target_row['pincode']) else \"Pincode missing\"\n",
    "            target_state = target_row['state'] if 'state' in target_row and pd.notnull(target_row['state']) else \"State missing\"\n",
    "            target_city = target_row['city'] if 'city' in target_row and pd.notnull(target_row['city']) else \"City missing\"\n",
    "            target_details = f\"Pincode: {target_pincode}, State: {target_state}, City: {target_city}\"\n",
    "        else:\n",
    "            target_details = f\"Key '{key_column}' with value '{key_value}' not present in target table '{target_table}'.\"\n",
    "\n",
    "        # Check if pincode exists in reference\n",
    "        ref_matches = reference_df[reference_df['pincode'] == pincode]\n",
    "        if ref_matches.empty:\n",
    "            issue = f\"Invalid pincode ({pincode}).\"\n",
    "            pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "                key_column: key_value,\n",
    "                'pincode': pincode,\n",
    "                'state': state,\n",
    "                'city': city,\n",
    "                'issue': issue,\n",
    "                f'{target_table}_details': target_details\n",
    "            }])], ignore_index=True)\n",
    "            error_detail = {\n",
    "                'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'pincode_mapping',\n",
    "                'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "                'source_table': master_table,\n",
    "                'target_table': target_table,\n",
    "                'issue_column': 'pincode',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "            continue\n",
    "\n",
    "        # Check if any of the reference entries match both the city and state\n",
    "        exact_match = ref_matches[\n",
    "            (ref_matches['city'] == city) & (ref_matches['state'] == state)\n",
    "        ]\n",
    "        if not exact_match.empty:\n",
    "            continue  # No issue, mapping is correct\n",
    "\n",
    "        # Check for state mismatch\n",
    "        state_matches = ref_matches[ref_matches['state'] == state]\n",
    "\n",
    "        # Check for city mismatch\n",
    "        city_matches = ref_matches[ref_matches['city'] == city]\n",
    "\n",
    "        if state_matches.empty and city_matches.empty:\n",
    "            # Both state and city do not match\n",
    "            expected_entries = ref_matches[['state', 'city']].drop_duplicates()\n",
    "            expected_states = expected_entries['state'].tolist()\n",
    "            expected_cities = expected_entries['city'].tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}' and city '{city}'. Expected states: {expected_states_str}; Expected cities: {expected_cities_str}.\"\n",
    "        elif state_matches.empty:\n",
    "            # State does not match\n",
    "            expected_states = ref_matches['state'].unique().tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}'. Expected states: {expected_states_str}.\"\n",
    "        elif city_matches.empty:\n",
    "            # City does not match\n",
    "            expected_cities = state_matches['city'].unique().tolist()\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match city '{city}'. Expected cities: {expected_cities_str}.\"\n",
    "        else:\n",
    "            # Other cases\n",
    "            issue = f\"Pincode {pincode} has a mapping inconsistency.\"\n",
    "\n",
    "        pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "            key_column: key_value,\n",
    "            'pincode': pincode,\n",
    "            'state': state,\n",
    "            'city': city,\n",
    "            'issue': issue,\n",
    "            f'{target_table}_details': target_details\n",
    "        }])], ignore_index=True)\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'pincode_mapping',\n",
    "            'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table,\n",
    "            'issue_column': 'pincode',\n",
    "            'unique_identifier': f'{key_column}: {key_value}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(pincode_mapping_issues)} pincode mapping issues in master table '{master_table}'.\")\n",
    "    return pincode_mapping_issues, error_logs_m\n",
    "\n",
    "def compare_tables(client, dataset_name, base_name, master_table, target_table, master_key, target_key, column_mapping=None):\n",
    "    \"\"\"\n",
    "    Compare two tables and generate a report.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        base_name (str): The base name of the table.\n",
    "        master_table (str): Name of the master_hub_ table.\n",
    "        target_table (str): Name of the target prefixed table.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing all comparison results.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting comparison for base table '{base_name}': '{master_table}' vs '{target_table}'.\")\n",
    "\n",
    "    # Initialize comparison results\n",
    "    mismatches = []\n",
    "    null_values_master = []\n",
    "    null_values_target = []\n",
    "    data_type_issues = pd.DataFrame()\n",
    "    format_issues_master = pd.DataFrame()\n",
    "    pincode_mapping_issues = pd.DataFrame()\n",
    "    duplicates_master = pd.DataFrame()\n",
    "    duplicates_target = pd.DataFrame()\n",
    "    master_only_keys = []\n",
    "    target_only_keys = []\n",
    "\n",
    "    # Load data\n",
    "    df_master = load_table_from_bigquery(client, dataset_name, master_table)\n",
    "    df_target = load_table_from_bigquery(client, dataset_name, target_table)\n",
    "\n",
    "    # Apply standardization\n",
    "    df_master = standardize_dataframe(df_master, exclude_columns=[master_key])\n",
    "    df_target = standardize_dataframe(df_target, exclude_columns=[target_key])\n",
    "\n",
    "    # Apply active filter if defined\n",
    "    base_table_info = BASE_TABLES.get(base_name, {})\n",
    "    active_filter = base_table_info.get('active_filter')\n",
    "    perform_checks = base_table_info.get('perform_checks', True)\n",
    "\n",
    "    if active_filter:\n",
    "        column = active_filter.get('column')\n",
    "        value = active_filter.get('value')\n",
    "        if column and column in df_master.columns:\n",
    "            initial_count = len(df_master)\n",
    "            df_master = df_master[df_master[column] == value]\n",
    "            filtered_count = len(df_master)\n",
    "            logging.info(f\"Filtered '{base_name}' master table: {initial_count - filtered_count} records excluded based on {column} = {value}.\")\n",
    "        else:\n",
    "            logging.warning(f\"Active filter specified but column '{column}' not found in master table '{master_table}'.\")\n",
    "\n",
    "    if df_master.empty or df_target.empty:\n",
    "        logging.warning(f\"One of the tables '{master_table}' or '{target_table}' is empty. Skipping comparison.\")\n",
    "        return None\n",
    "\n",
    "    # Identify BigNumeric columns in master and target tables\n",
    "    schema_master = get_table_schema(client, dataset_name, master_table)\n",
    "    schema_target = get_table_schema(client, dataset_name, target_table)\n",
    "    bignumeric_columns_master = [col for col, dtype in schema_master.items() if dtype == 'BIGNUMERIC']\n",
    "    bignumeric_columns_target = [col for col, dtype in schema_target.items() if dtype == 'BIGNUMERIC']\n",
    "\n",
    "    # Format BigNumeric columns in master table\n",
    "    for col in bignumeric_columns_master:\n",
    "        if col in df_master.columns:\n",
    "            df_master[col] = df_master[col].apply(lambda x: format(x, '.0f') if pd.notnull(x) else x)\n",
    "\n",
    "    # Format BigNumeric columns in target table\n",
    "    for col in bignumeric_columns_target:\n",
    "        if col in df_target.columns:\n",
    "            df_target[col] = df_target[col].apply(lambda x: format(x, '.0f') if pd.notnull(x) else x)\n",
    "\n",
    "    # Get imp_columns and non_imp_columns\n",
    "    imp_columns = Imp_columns.get(base_name, None)\n",
    "    non_imp_columns = Non_imp_columns.get(base_name, [])\n",
    "\n",
    "    # Identify common columns\n",
    "    common_columns, master_unique_cols, target_unique_cols = find_common_and_non_common_columns(df_master, df_target)\n",
    "\n",
    "    if not common_columns:\n",
    "        logging.warning(f\"No common columns found between '{master_table}' and '{target_table}'. Skipping comparison.\")\n",
    "        return None\n",
    "\n",
    "    # Apply column mapping if provided\n",
    "    if column_mapping:\n",
    "        # Rename master columns to target columns for comparison\n",
    "        for master_col, target_col in column_mapping.items():\n",
    "            if master_col in df_master.columns and target_col in df_target.columns:\n",
    "                # Ensure both columns are standardized\n",
    "                df_master[master_col] = df_master[master_col].astype(str).str.strip().str.lower()\n",
    "                df_target[target_col] = df_target[target_col].astype(str).str.strip().str.lower()\n",
    "        # Adjust common_columns by replacing master_col with target_col\n",
    "        adjusted_common_columns = set(common_columns)\n",
    "        for master_col, target_col in column_mapping.items():\n",
    "            if master_col in adjusted_common_columns and target_col in adjusted_common_columns:\n",
    "                adjusted_common_columns.remove(master_col)\n",
    "                adjusted_common_columns.remove(target_col)\n",
    "                adjusted_common_columns.add(master_col)  # Use master_col as the unified name\n",
    "        common_columns = list(adjusted_common_columns)\n",
    "\n",
    "    # Determine columns to check based on imp_columns\n",
    "    if imp_columns:\n",
    "        columns_to_check = [col for col in imp_columns if col in common_columns]\n",
    "        logging.info(f\"Important columns defined for '{base_name}': {columns_to_check}\")\n",
    "    else:\n",
    "        columns_to_check = [col for col in common_columns if col not in non_imp_columns]\n",
    "        logging.info(f\"No important columns defined for '{base_name}'. Applying checks to all columns except non_imp_columns: {columns_to_check}\")\n",
    "\n",
    "    if perform_checks:\n",
    "        # Find duplicates in both tables\n",
    "        duplicates_master, error_logs_m = find_duplicates(df_master, master_key, master_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "        duplicates_target, error_logs_m = find_duplicates(df_target, target_key, target_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    if not duplicates_master.empty:\n",
    "        logging.warning(f\"Duplicate keys found in source table '{master_table}'. These will be reported but not used in mismatch comparison.\")\n",
    "    if not duplicates_target.empty:\n",
    "        logging.warning(f\"Duplicate keys found in target table '{target_table}'. These will be reported but not used in mismatch comparison.\")\n",
    "\n",
    "    # Perform mismatch comparison\n",
    "    if perform_checks:\n",
    "        mismatches, error_logs_m = find_mismatches(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            columns_to_check,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            duplicates_master,\n",
    "            duplicates_target,\n",
    "            non_imp_columns,\n",
    "            column_mapping  # Pass column_mapping to handle differently named columns\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Find detailed null values in both tables\n",
    "    if perform_checks:\n",
    "        null_values_master, null_values_target, error_logs_m = find_detailed_nulls(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            columns_to_check  # Pass columns_to_check\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Validate data types between master and target schemas\n",
    "    if perform_checks:\n",
    "        data_type_issues, error_logs_m = validate_data_types(\n",
    "            schema_master,\n",
    "            schema_target,\n",
    "            master_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            columns_to_check  # Pass columns_to_check\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Validate formats in master table only\n",
    "    if perform_checks:\n",
    "        format_issues_master, error_logs_m = validate_formats(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            target_table,\n",
    "            master_table,\n",
    "            columns_to_check  # Pass columns_to_check\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "        # Validate pincode mapping if applicable\n",
    "        pincode_mapping_issues = pd.DataFrame()\n",
    "        if {'pincode', 'city', 'state'}.issubset(df_master.columns):\n",
    "            pincode_mapping_issues, error_logs_m = validate_pincode_mapping(\n",
    "                df_master,\n",
    "                df_target,\n",
    "                master_key,\n",
    "                target_key,\n",
    "                target_table,\n",
    "                client,\n",
    "                master_table\n",
    "            )\n",
    "            ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Find non-matching keys\n",
    "    master_only_keys, target_only_keys, error_logs_m = find_non_matching_keys(\n",
    "        df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table\n",
    "    )\n",
    "    ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        'mismatches': mismatches,\n",
    "        'null_values_master': pd.DataFrame(null_values_master),\n",
    "        'null_values_target': pd.DataFrame(null_values_target),\n",
    "        'duplicates_master': duplicates_master,\n",
    "        'duplicates_target': duplicates_target,\n",
    "        'data_type_issues': data_type_issues,\n",
    "        'format_issues_master': format_issues_master,\n",
    "        'pincode_mapping_issues': pincode_mapping_issues,\n",
    "        'key_column_master': master_key,\n",
    "        'key_column_target': target_key,\n",
    "        'df_master_only_keys': master_only_keys,\n",
    "        'df_target_only_keys': target_only_keys,\n",
    "        'table1_name': master_table,\n",
    "        'table2_name': target_table,\n",
    "        'df_master': df_master,\n",
    "        'df_target': df_target\n",
    "    }\n",
    "\n",
    "    logging.info(f\"Completed comparison for '{master_table}' vs '{target_table}'.\")\n",
    "    return results\n",
    "\n",
    "def generate_string_schema(df):\n",
    "    \"\"\"\n",
    "    Generates a BigQuery schema with all fields as STRING.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame for which to generate the schema.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of SchemaField objects with type STRING.\n",
    "    \"\"\"\n",
    "    schema = [SchemaField(column, \"STRING\", mode=\"NULLABLE\") for column in df.columns]\n",
    "    return schema\n",
    "\n",
    "def _upload_dataframe_to_bigquery(client, analytics_dataset, table_name, df):\n",
    "    \"\"\"\n",
    "    Helper function to upload a DataFrame to BigQuery.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        analytics_dataset (str): The name of the Analytics dataset.\n",
    "        table_name (str): The name of the table to upload.\n",
    "        df (pd.DataFrame): The DataFrame to upload.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logging.info(f\"No data to upload for '{table_name}'. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Convert all columns to string type\n",
    "    df = df.astype(str)\n",
    "\n",
    "    # Generate BigQuery schema with all fields as STRING\n",
    "    schema = generate_string_schema(df)\n",
    "\n",
    "    # Ensure table name doesn't exceed BigQuery's maximum length (1,024 characters)\n",
    "    if len(table_name) > 1024:\n",
    "        original_table_name = table_name\n",
    "        table_name = table_name[:1021] + '...'\n",
    "        logging.warning(f\"Table name truncated from '{original_table_name}' to '{table_name}' due to length constraints.\")\n",
    "\n",
    "    # Define the full table ID\n",
    "    table_id = f\"{client.project}.{analytics_dataset}.{table_name}\"\n",
    "\n",
    "    # Upload the DataFrame to BigQuery\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(\n",
    "            df,\n",
    "            table_id,\n",
    "            job_config=bigquery.LoadJobConfig(\n",
    "                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "                schema=schema  # Using the provided schema with all fields as STRING\n",
    "            )\n",
    "        )\n",
    "        job.result()  # Wait for the job to complete\n",
    "        logging.info(f\"Successfully uploaded '{table_id}' with {len(df)} records.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to upload '{table_id}' to BigQuery: {e}\")\n",
    "\n",
    "def upload_comparison_results_to_bigquery(client, analytics_dataset, ERROR_LOG_M):\n",
    "    \"\"\"\n",
    "    Uploads the ERROR_LOG_M to BigQuery as a separate table in the Analytics dataset.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        analytics_dataset (str): The name of the Analytics dataset.\n",
    "        ERROR_LOG_M (list): The error log data as a list of dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Handle ERROR_LOG separately\n",
    "    if ERROR_LOG_M is not None:\n",
    "        # Determine the DataFrame to upload\n",
    "        if isinstance(ERROR_LOG_M, pd.DataFrame):\n",
    "            error_df = ERROR_LOG_M\n",
    "        elif isinstance(ERROR_LOG_M, list):\n",
    "            error_df = pd.DataFrame(ERROR_LOG_M)\n",
    "        else:\n",
    "            logging.warning(\"Unsupported data type for ERROR_LOG. Skipping upload.\")\n",
    "            error_df = None\n",
    "\n",
    "        if error_df is not None and not error_df.empty:\n",
    "            _upload_dataframe_to_bigquery(client, analytics_dataset, \"error_logs_master_hub\", error_df)\n",
    "        else:\n",
    "            logging.info(\"No error logs to upload.\")\n",
    "    else:\n",
    "        logging.info(\"No error logs present.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the comparison of multiple base tables against their master_hub_ counterparts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize BigQuery client\n",
    "        try:\n",
    "            client = get_bigquery_client(PROJECT_ID)\n",
    "        except Exception:\n",
    "            logging.error(\"Exiting due to BigQuery client initialization failure.\")\n",
    "            return\n",
    "\n",
    "        # Find common tables with 'master_hub_' and other prefixes, passing BASE_TABLES\n",
    "        common_tables = find_common_tables_with_master_hub(client, DATASET_ID, PREFIXES, BASE_TABLES)\n",
    "\n",
    "        if not common_tables:\n",
    "            logging.info(\"No common tables found with 'master_hub_' and the specified prefixes.\")\n",
    "            return\n",
    "\n",
    "        # Iterate over each base table and perform comparisons\n",
    "        for base_name, tables in common_tables.items():\n",
    "            base_table_info = BASE_TABLES.get(base_name)\n",
    "            if not base_table_info:\n",
    "                logging.warning(f\"No configuration found for base table '{base_name}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            master_key = base_table_info.get('master_key')\n",
    "            target_tables = base_table_info.get('targets', {})\n",
    "            column_mapping = base_table_info.get('column_mapping', {})\n",
    "            \n",
    "\n",
    "            master_table = tables.get('master_hub_')\n",
    "            if not master_table:\n",
    "                logging.warning(f\"Master table 'master_hub_{base_name}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            all_results = []\n",
    "\n",
    "            # Iterate through each prefix and its corresponding target_key\n",
    "            for prefix, target_key in target_tables.items():\n",
    "                target_table = tables.get(prefix)\n",
    "                if not target_table:\n",
    "                    logging.warning(f\"Target table with prefix '{prefix}' for base table '{base_name}' not found. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                comparison_result = compare_tables(\n",
    "                    client, \n",
    "                    DATASET_ID, \n",
    "                    base_name, \n",
    "                    master_table, \n",
    "                    target_table, \n",
    "                    master_key, \n",
    "                    target_key,  # Pass the correct target_key per prefix\n",
    "                    column_mapping  # Pass the column_mapping\n",
    "                )\n",
    "                if comparison_result:\n",
    "                    all_results.append(comparison_result)\n",
    "\n",
    "                    # Prepare and send a separate Slack message for each comparison\n",
    "                    total_mismatches = len(comparison_result['mismatches'])\n",
    "                    total_nulls_master = len(comparison_result['null_values_master'])\n",
    "                    total_nulls_target = len(comparison_result['null_values_target'])\n",
    "                    total_dup_master = len(comparison_result['duplicates_master'])\n",
    "                    total_dup_target = len(comparison_result['duplicates_target'])\n",
    "                    total_data_type_issues = len(comparison_result['data_type_issues'])\n",
    "                    total_format_issues_master = len(comparison_result['format_issues_master'])\n",
    "                    total_pincode_issues = len(comparison_result['pincode_mapping_issues'])\n",
    "                    total_non_matching_source = len(comparison_result.get('df_master_only_keys', []))\n",
    "                    total_non_matching_target = len(comparison_result.get('df_target_only_keys', []))\n",
    "\n",
    "                    message = (\n",
    "                        f\"✅ *Comparison Report Generated for `{base_name}`*\\n\"\n",
    "                        f\"*Tables Compared: `{comparison_result['table1_name']}` vs `{comparison_result['table2_name']}`*\\n\"\n",
    "                        f\"- *Total Mismatches between values of same column name of both tables : `{total_mismatches}`*\\n\"\n",
    "                        f\"- *Total Null Values in `{comparison_result['table1_name']}`: `{total_nulls_master}`*\\n\"\n",
    "                        f\"- *Total Null Values in `{comparison_result['table2_name']}`: `{total_nulls_target}`*\\n\"\n",
    "                        f\"- *Duplicate `{master_key}` in `{comparison_result['table1_name']}`: `{total_dup_master}`*\\n\"\n",
    "                        f\"- *Duplicate `{target_key}` in `{comparison_result['table2_name']}`: `{total_dup_target}`*\\n\"\n",
    "                        f\"- *Total Data Type Issues(mismatch between datatype in columns with same name of both tables): `{total_data_type_issues}`*\\n\"\n",
    "                        f\"- *Total Format/Value Issues(gstin, email, pincode) in `{comparison_result['table1_name']}`: `{total_format_issues_master}`*\\n\"\n",
    "                        f\"- *Total Pincode Mapping Issues in `{comparison_result['table1_name']}`: `{total_pincode_issues}`*\\n\"\n",
    "                         \"- *Non-Matching Keys*:\\n\"\n",
    "                        f\"--*`{master_key}` only in `{comparison_result['table1_name']}` and not in `{comparison_result['table2_name']}`:`{total_non_matching_source}`,*\\n\"\n",
    "                        f\"--*`{target_key}` only in `{comparison_result['table2_name']}` and not in `{comparison_result['table1_name']}`:`{total_non_matching_target}`*\"\n",
    "                    )\n",
    "\n",
    "                    send_slack_alert(message)\n",
    "            \n",
    "            if all_results:\n",
    "                # Generate aggregated report for the base name and get the filepath\n",
    "                report_filepath = create_aggregated_document(all_results, base_name)\n",
    "                \n",
    "                # Upload the report to Slack using the updated function\n",
    "                upload_file_to_slack(report_filepath, title=f\"{base_name.capitalize()} Comparison Report\")\n",
    "                \n",
    "                # Remove the local report file after successful upload\n",
    "                try:\n",
    "                    os.remove(report_filepath)\n",
    "                    logging.info(f\"Removed local report file '{report_filepath}'.\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to remove local report file '{report_filepath}': {e}\")\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                logging.info(f\"No comparison results to report for base name '{base_name}'.\")\n",
    "               \n",
    "\n",
    "        # Upload error logs to BigQuery after all comparisons\n",
    "        upload_comparison_results_to_bigquery(\n",
    "            client, \n",
    "            'analytics_data',\n",
    "            ERROR_LOG_M\n",
    "            )\n",
    "\n",
    "        logging.info(\"All comparisons completed.\")\n",
    "    except Exception as e:\n",
    "        # Capture the full traceback\n",
    "        tb = traceback.format_exc()\n",
    "        logging.error(\"An unexpected error occurred in the main process.\", exc_info=True)\n",
    "\n",
    "        # Prepare a detailed error message for Slack\n",
    "        error_message = (\n",
    "            f\"❌ *Comparison Process Failed*\\n\"\n",
    "            f\"*Error:* {str(e)}\\n\"\n",
    "            f\"*Traceback:*\\n```{tb}```\"\n",
    "        )\n",
    "        send_slack_alert(error_message)\n",
    "\n",
    "        # Optionally, exit the script with a non-zero status\n",
    "        sys.exit(1)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47fff70f-c472-4aa1-8f06-4abace1ce6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 09:24:13,345 - INFO - Slack client initialized successfully.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "2024-11-27 09:24:13,819 - INFO - BigQuery client initialized successfully.\n",
      "2024-11-27 09:24:14,925 - INFO - Found 211 tables in dataset 'Impetus_dev_sit'.\n",
      "2024-11-27 09:24:14,926 - INFO - Identified 8 common base names with 'master_hub_' and other specified prefixes.\n",
      "2024-11-27 09:24:14,927 - INFO - Starting comparison for base table 'brand': 'master_hub_brand' vs 'procuro_brand'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:15,788 - INFO - Loaded data from table 'master_hub_brand' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:16,538 - INFO - Loaded data from table 'procuro_brand' into DataFrame.\n",
      "2024-11-27 09:24:16,542 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:16,545 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:16,546 - INFO - Filtered 'brand' master table: 22 records excluded based on is_active = True.\n",
      "2024-11-27 09:24:16,647 - INFO - Retrieved schema for table 'master_hub_brand'.\n",
      "2024-11-27 09:24:16,731 - INFO - Retrieved schema for table 'procuro_brand'.\n",
      "2024-11-27 09:24:16,734 - INFO - Found 10 common columns, 10 unique to first table, 1 unique to second table.\n",
      "2024-11-27 09:24:16,735 - INFO - Important columns defined for 'brand': ['id', 'code']\n",
      "2024-11-27 09:24:16,740 - INFO - Found 0 duplicate entries based on 'code'.\n",
      "2024-11-27 09:24:16,928 - INFO - Found 416 duplicate entries based on 'code'.\n",
      "2024-11-27 09:24:16,929 - WARNING - Duplicate keys found in target table 'procuro_brand'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:24:16,932 - INFO - Merged DataFrame has 1 records for mismatch comparison.\n",
      "2024-11-27 09:24:16,933 - INFO - Found 0 mismatches between 'master_hub_brand' and 'procuro_brand'.\n",
      "2024-11-27 09:24:16,934 - INFO - Found 0 null values in master table 'master_hub_brand'.\n",
      "2024-11-27 09:24:16,935 - INFO - Found 0 null values in target table 'procuro_brand'.\n",
      "2024-11-27 09:24:16,935 - INFO - Found 1 data type issues.\n",
      "2024-11-27 09:24:16,937 - INFO - Found 0 format issues.\n",
      "2024-11-27 09:24:16,938 - INFO - Found 29 keys in source not in target and 430 keys in target not in source.\n",
      "2024-11-27 09:24:16,941 - INFO - Completed comparison for 'master_hub_brand' vs 'procuro_brand'.\n",
      "2024-11-27 09:24:17,306 - INFO - Message sent to C07UN19ETK5: 1732679657.180339\n",
      "2024-11-27 09:24:17,308 - INFO - Starting comparison for base table 'brand': 'master_hub_brand' vs 'costing_engine_brand'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:18,192 - INFO - Loaded data from table 'master_hub_brand' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:19,035 - INFO - Loaded data from table 'costing_engine_brand' into DataFrame.\n",
      "2024-11-27 09:24:19,044 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:19,051 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:19,052 - INFO - Filtered 'brand' master table: 22 records excluded based on is_active = True.\n",
      "2024-11-27 09:24:19,243 - INFO - Retrieved schema for table 'master_hub_brand'.\n",
      "2024-11-27 09:24:19,438 - INFO - Retrieved schema for table 'costing_engine_brand'.\n",
      "2024-11-27 09:24:19,440 - INFO - Found 14 common columns, 6 unique to first table, 2 unique to second table.\n",
      "2024-11-27 09:24:19,441 - INFO - Important columns defined for 'brand': ['name', 'id', 'slug', 'code']\n",
      "2024-11-27 09:24:19,445 - INFO - Found 0 duplicate entries based on 'code'.\n",
      "2024-11-27 09:24:19,461 - INFO - Found 3 duplicate entries based on 'code'.\n",
      "2024-11-27 09:24:19,462 - WARNING - Duplicate keys found in target table 'costing_engine_brand'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:24:19,467 - INFO - Merged DataFrame has 3 records for mismatch comparison.\n",
      "2024-11-27 09:24:19,468 - INFO - Found 0 mismatches between 'master_hub_brand' and 'costing_engine_brand'.\n",
      "2024-11-27 09:24:19,470 - INFO - Found 0 null values in master table 'master_hub_brand'.\n",
      "2024-11-27 09:24:19,471 - INFO - Found 0 null values in target table 'costing_engine_brand'.\n",
      "2024-11-27 09:24:19,471 - INFO - Found 0 data type issues.\n",
      "2024-11-27 09:24:19,473 - INFO - Found 0 format issues.\n",
      "2024-11-27 09:24:19,474 - INFO - Found 27 keys in source not in target and 51 keys in target not in source.\n",
      "2024-11-27 09:24:19,475 - INFO - Completed comparison for 'master_hub_brand' vs 'costing_engine_brand'.\n",
      "2024-11-27 09:24:19,826 - INFO - Message sent to C07UN19ETK5: 1732679659.692539\n",
      "2024-11-27 09:24:19,828 - INFO - Starting comparison for base table 'brand_pm_mapping': 'master_hub_brand_pm_mapping' vs 'costing_engine_brand_pm_mapping'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:20,783 - INFO - Loaded data from table 'master_hub_brand_pm_mapping' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:21,446 - INFO - Loaded data from table 'costing_engine_brand_pm_mapping' into DataFrame.\n",
      "2024-11-27 09:24:21,453 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:21,457 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:21,592 - INFO - Retrieved schema for table 'master_hub_brand_pm_mapping'.\n",
      "2024-11-27 09:24:21,778 - INFO - Retrieved schema for table 'costing_engine_brand_pm_mapping'.\n",
      "2024-11-27 09:24:21,781 - INFO - Found 13 common columns, 5 unique to first table, 1 unique to second table.\n",
      "2024-11-27 09:24:21,782 - INFO - No important columns defined for 'brand_pm_mapping'. Applying checks to all columns except non_imp_columns: ['is_active', '_boltic_merged', 'id', '_boltic_meta_id', '_boltic_pipe_id', '_boltic_updated_at', '_id', 'updated_at', '_boltic_id', 'pm_id', '_boltic_mark_deleted', 'created_at', '_boltic_ingested_at']\n",
      "2024-11-27 09:24:21,816 - INFO - Found 15 duplicate entries based on 'pm_id'.\n",
      "2024-11-27 09:24:21,835 - INFO - Found 18 duplicate entries based on 'pm_id'.\n",
      "2024-11-27 09:24:21,836 - WARNING - Duplicate keys found in source table 'master_hub_brand_pm_mapping'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:24:21,836 - WARNING - Duplicate keys found in target table 'costing_engine_brand_pm_mapping'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:24:21,840 - INFO - Merged DataFrame has 16 records for mismatch comparison.\n",
      "2024-11-27 09:24:21,841 - INFO - Found 0 mismatches between 'master_hub_brand_pm_mapping' and 'costing_engine_brand_pm_mapping'.\n",
      "2024-11-27 09:24:21,855 - INFO - Found 0 null values in master table 'master_hub_brand_pm_mapping'.\n",
      "2024-11-27 09:24:21,855 - INFO - Found 10 null values in target table 'costing_engine_brand_pm_mapping'.\n",
      "2024-11-27 09:24:21,856 - INFO - Found 0 data type issues.\n",
      "2024-11-27 09:24:21,859 - INFO - Found 0 format issues.\n",
      "2024-11-27 09:24:21,860 - INFO - Found 31 keys in source not in target and 12 keys in target not in source.\n",
      "2024-11-27 09:24:21,861 - INFO - Completed comparison for 'master_hub_brand_pm_mapping' vs 'costing_engine_brand_pm_mapping'.\n",
      "2024-11-27 09:24:22,401 - INFO - Message sent to C07UN19ETK5: 1732679662.261779\n",
      "2024-11-27 09:24:22,402 - INFO - Starting comparison for base table 'brick': 'master_hub_brick' vs 'costing_engine_brick'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:24,414 - INFO - Loaded data from table 'master_hub_brick' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:25,125 - INFO - Loaded data from table 'costing_engine_brick' into DataFrame.\n",
      "2024-11-27 09:24:25,133 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:25,136 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:25,230 - INFO - Retrieved schema for table 'master_hub_brick'.\n",
      "2024-11-27 09:24:25,315 - INFO - Retrieved schema for table 'costing_engine_brick'.\n",
      "2024-11-27 09:24:25,380 - INFO - Found 11 common columns, 9 unique to first table, 2 unique to second table.\n",
      "2024-11-27 09:24:25,382 - INFO - No important columns defined for 'brick'. Applying checks to all columns except non_imp_columns: ['_boltic_merged', 'id', '_boltic_meta_id', '_boltic_pipe_id', '_boltic_updated_at', '_id', 'updated_at', '_boltic_id', '_boltic_mark_deleted', 'created_at', '_boltic_ingested_at']\n",
      "2024-11-27 09:24:25,723 - INFO - Found 340 duplicate entries based on 'brick_code'.\n",
      "2024-11-27 09:24:25,724 - INFO - Found 0 duplicate entries based on 'code'.\n",
      "2024-11-27 09:24:25,725 - WARNING - Duplicate keys found in source table 'master_hub_brick'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:24:25,731 - INFO - Merged DataFrame has 478 records for mismatch comparison.\n",
      "2024-11-27 09:24:25,742 - INFO - Found 0 mismatches between 'master_hub_brick' and 'costing_engine_brick'.\n",
      "2024-11-27 09:24:27,698 - INFO - Found 192 null values in master table 'master_hub_brick'.\n",
      "2024-11-27 09:24:27,699 - INFO - Found 801 null values in target table 'costing_engine_brick'.\n",
      "2024-11-27 09:24:27,700 - INFO - Found 0 data type issues.\n",
      "2024-11-27 09:24:27,845 - INFO - Found 0 format issues.\n",
      "2024-11-27 09:24:27,850 - INFO - Found 8881 keys in source not in target and 363 keys in target not in source.\n",
      "2024-11-27 09:24:27,873 - INFO - Completed comparison for 'master_hub_brick' vs 'costing_engine_brick'.\n",
      "2024-11-27 09:24:28,203 - INFO - Message sent to C07UN19ETK5: 1732679668.077189\n",
      "2024-11-27 09:24:28,203 - INFO - Starting comparison for base table 'coe_bom_element_type_mapping': 'master_hub_coe_bom_element_type_mapping' vs 'costing_engine_coe_bom_element_type_mapping'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:28,743 - INFO - Loaded data from table 'master_hub_coe_bom_element_type_mapping' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:29,555 - INFO - Loaded data from table 'costing_engine_coe_bom_element_type_mapping' into DataFrame.\n",
      "2024-11-27 09:24:29,562 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:29,567 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:29,762 - INFO - Retrieved schema for table 'master_hub_coe_bom_element_type_mapping'.\n",
      "2024-11-27 09:24:29,853 - INFO - Retrieved schema for table 'costing_engine_coe_bom_element_type_mapping'.\n",
      "2024-11-27 09:24:29,856 - INFO - Found 15 common columns, 3 unique to first table, 0 unique to second table.\n",
      "2024-11-27 09:24:29,859 - INFO - No important columns defined for 'coe_bom_element_type_mapping'. Applying checks to all columns except non_imp_columns: ['coe_name', 'coe_approver_email', 'is_active', '_boltic_merged', 'id', 'element_type', '_boltic_meta_id', '_boltic_pipe_id', '_boltic_updated_at', '_id', 'updated_at', '_boltic_id', '_boltic_mark_deleted', 'created_at', '_boltic_ingested_at']\n",
      "2024-11-27 09:24:29,884 - INFO - Found 9 duplicate entries based on 'coe_name'.\n",
      "2024-11-27 09:24:29,896 - INFO - Found 11 duplicate entries based on 'coe_name'.\n",
      "2024-11-27 09:24:29,897 - WARNING - Duplicate keys found in source table 'master_hub_coe_bom_element_type_mapping'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:24:29,897 - WARNING - Duplicate keys found in target table 'costing_engine_coe_bom_element_type_mapping'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:24:29,901 - INFO - Merged DataFrame has 16 records for mismatch comparison.\n",
      "2024-11-27 09:24:29,902 - INFO - Found 0 mismatches between 'master_hub_coe_bom_element_type_mapping' and 'costing_engine_coe_bom_element_type_mapping'.\n",
      "2024-11-27 09:24:29,909 - INFO - Found 0 null values in master table 'master_hub_coe_bom_element_type_mapping'.\n",
      "2024-11-27 09:24:29,909 - INFO - Found 0 null values in target table 'costing_engine_coe_bom_element_type_mapping'.\n",
      "2024-11-27 09:24:29,910 - INFO - Found 0 data type issues.\n",
      "2024-11-27 09:24:29,912 - INFO - Found 0 format issues.\n",
      "2024-11-27 09:24:29,913 - INFO - Found 0 keys in source not in target and 3 keys in target not in source.\n",
      "2024-11-27 09:24:29,914 - INFO - Completed comparison for 'master_hub_coe_bom_element_type_mapping' vs 'costing_engine_coe_bom_element_type_mapping'.\n",
      "2024-11-27 09:24:30,280 - INFO - Message sent to C07UN19ETK5: 1732679670.146469\n",
      "2024-11-27 09:24:30,281 - INFO - Starting comparison for base table 'supplier': 'master_hub_supplier' vs 'procuro_supplier'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:31,320 - INFO - Loaded data from table 'master_hub_supplier' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:34,257 - INFO - Loaded data from table 'procuro_supplier' into DataFrame.\n",
      "2024-11-27 09:24:34,264 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:34,288 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:34,291 - INFO - Filtered 'supplier' master table: 171 records excluded based on is_active = True.\n",
      "2024-11-27 09:24:34,387 - INFO - Retrieved schema for table 'master_hub_supplier'.\n",
      "2024-11-27 09:24:34,465 - INFO - Retrieved schema for table 'procuro_supplier'.\n",
      "2024-11-27 09:24:34,482 - INFO - Found 20 common columns, 54 unique to first table, 15 unique to second table.\n",
      "2024-11-27 09:24:34,482 - INFO - No important columns defined for 'supplier'. Applying checks to all columns except non_imp_columns: ['state', 'gstin', 'supplier_code', '_boltic_updated_at', 'address', 'is_msme', '_boltic_pipe_id', '_boltic_mark_deleted', '_boltic_merged', '_boltic_meta_id', '_boltic_ingested_at', 'name', 'pan', '_boltic_id', 'city', 'pincode']\n",
      "2024-11-27 09:24:34,528 - INFO - Found 13 duplicate entries based on 'supplier_code'.\n",
      "2024-11-27 09:24:35,484 - INFO - Found 673 duplicate entries based on 'supplier_code'.\n",
      "2024-11-27 09:24:35,485 - WARNING - Duplicate keys found in source table 'master_hub_supplier'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:24:35,485 - WARNING - Duplicate keys found in target table 'procuro_supplier'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:24:35,500 - INFO - Merged DataFrame has 679 records for mismatch comparison.\n",
      "2024-11-27 09:24:35,523 - INFO - Found 0 mismatches between 'master_hub_supplier' and 'procuro_supplier'.\n",
      "2024-11-27 09:24:35,920 - INFO - Found 0 null values in master table 'master_hub_supplier'.\n",
      "2024-11-27 09:24:35,921 - INFO - Found 156 null values in target table 'procuro_supplier'.\n",
      "2024-11-27 09:24:35,923 - INFO - Found 0 data type issues.\n",
      "2024-11-27 09:24:38,524 - INFO - Found 884 format issues.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:46,327 - INFO - Loaded reference pincode mapping from 'all_india_PO_list' in 'Analytics' dataset.\n",
      "2024-11-27 09:24:52,336 - INFO - Found 305 pincode mapping issues in master table 'master_hub_supplier'.\n",
      "2024-11-27 09:24:52,342 - INFO - Found 54 keys in source not in target and 9806 keys in target not in source.\n",
      "2024-11-27 09:24:52,368 - INFO - Completed comparison for 'master_hub_supplier' vs 'procuro_supplier'.\n",
      "2024-11-27 09:24:52,709 - INFO - Message sent to C07UN19ETK5: 1732679692.585159\n",
      "2024-11-27 09:24:52,710 - INFO - Starting comparison for base table 'supplier': 'master_hub_supplier' vs 'costing_engine_supplier'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:53,776 - INFO - Loaded data from table 'master_hub_supplier' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:24:54,560 - INFO - Loaded data from table 'costing_engine_supplier' into DataFrame.\n",
      "2024-11-27 09:24:54,576 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:54,580 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:24:54,582 - INFO - Filtered 'supplier' master table: 171 records excluded based on is_active = True.\n",
      "2024-11-27 09:24:54,674 - INFO - Retrieved schema for table 'master_hub_supplier'.\n",
      "2024-11-27 09:24:54,773 - INFO - Retrieved schema for table 'costing_engine_supplier'.\n",
      "2024-11-27 09:24:54,785 - INFO - Found 25 common columns, 49 unique to first table, 2 unique to second table.\n",
      "2024-11-27 09:24:54,788 - INFO - No important columns defined for 'supplier'. Applying checks to all columns except non_imp_columns: ['state', 'gstin', 'supplier_code', '_boltic_updated_at', 'address', 'vendor_status', 'country_of_origin', 'is_msme', '_boltic_pipe_id', 'is_sample_supplier', '_boltic_mark_deleted', 'email', '_boltic_merged', '_boltic_meta_id', '_boltic_ingested_at', 'is_active', 'name', 'pan', '_boltic_id', 'city', 'pincode']\n",
      "2024-11-27 09:24:54,850 - INFO - Found 13 duplicate entries based on 'supplier_code'.\n",
      "2024-11-27 09:24:54,863 - INFO - Found 8 duplicate entries based on 'supplier_code'.\n",
      "2024-11-27 09:24:54,863 - WARNING - Duplicate keys found in source table 'master_hub_supplier'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:24:54,864 - WARNING - Duplicate keys found in target table 'costing_engine_supplier'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:24:54,868 - INFO - Merged DataFrame has 85 records for mismatch comparison.\n",
      "2024-11-27 09:24:54,873 - INFO - Found 0 mismatches between 'master_hub_supplier' and 'costing_engine_supplier'.\n",
      "2024-11-27 09:24:54,906 - INFO - Found 0 null values in master table 'master_hub_supplier'.\n",
      "2024-11-27 09:24:54,906 - INFO - Found 0 null values in target table 'costing_engine_supplier'.\n",
      "2024-11-27 09:24:54,906 - INFO - Found 0 data type issues.\n",
      "2024-11-27 09:24:55,192 - INFO - Found 884 format issues.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:25:01,345 - INFO - Loaded reference pincode mapping from 'all_india_PO_list' in 'Analytics' dataset.\n",
      "2024-11-27 09:25:06,338 - INFO - Found 305 pincode mapping issues in master table 'master_hub_supplier'.\n",
      "2024-11-27 09:25:06,343 - INFO - Found 648 keys in source not in target and 2 keys in target not in source.\n",
      "2024-11-27 09:25:06,346 - INFO - Completed comparison for 'master_hub_supplier' vs 'costing_engine_supplier'.\n",
      "2024-11-27 09:25:06,684 - INFO - Message sent to C07UN19ETK5: 1732679706.557029\n",
      "2024-11-27 09:25:06,709 - INFO - Starting comparison for base table 'vendor_details': 'master_hub_supplier' vs 'scan_pack_vendor_details'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:25:07,694 - INFO - Loaded data from table 'master_hub_supplier' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:25:08,235 - INFO - Loaded data from table 'scan_pack_vendor_details' into DataFrame.\n",
      "2024-11-27 09:25:08,255 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:25:08,258 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:25:08,262 - INFO - Filtered 'vendor_details' master table: 171 records excluded based on is_active = True.\n",
      "2024-11-27 09:25:08,363 - INFO - Retrieved schema for table 'master_hub_supplier'.\n",
      "2024-11-27 09:25:08,481 - INFO - Retrieved schema for table 'scan_pack_vendor_details'.\n",
      "2024-11-27 09:25:08,490 - INFO - Found 18 common columns, 56 unique to first table, 8 unique to second table.\n",
      "2024-11-27 09:25:08,492 - INFO - No important columns defined for 'vendor_details'. Applying checks to all columns except non_imp_columns: ['state', '_boltic_merged', '_boltic_loaded_at', 'name', 'city', '_boltic_meta_id', '_boltic_pipe_id', '_boltic_updated_at', 'pan', '_boltic_id', '_boltic_mark_deleted', 'address', 'pincode', '_boltic_ingested_at']\n",
      "2024-11-27 09:25:08,549 - INFO - Found 13 duplicate entries based on 'supplier_code'.\n",
      "2024-11-27 09:25:08,551 - INFO - Found 0 duplicate entries based on 'vendor_code'.\n",
      "2024-11-27 09:25:08,551 - WARNING - Duplicate keys found in source table 'master_hub_supplier'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-27 09:25:08,561 - INFO - Merged DataFrame has 38 records for mismatch comparison.\n",
      "2024-11-27 09:25:08,562 - INFO - Found 0 mismatches between 'master_hub_supplier' and 'scan_pack_vendor_details'.\n",
      "2024-11-27 09:25:08,608 - INFO - Found 0 null values in master table 'master_hub_supplier'.\n",
      "2024-11-27 09:25:08,609 - INFO - Found 60 null values in target table 'scan_pack_vendor_details'.\n",
      "2024-11-27 09:25:08,609 - INFO - Found 0 data type issues.\n",
      "2024-11-27 09:25:08,659 - INFO - Found 138 format issues.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:25:14,506 - INFO - Loaded reference pincode mapping from 'all_india_PO_list' in 'Analytics' dataset.\n",
      "2024-11-27 09:25:19,407 - INFO - Found 305 pincode mapping issues in master table 'master_hub_supplier'.\n",
      "2024-11-27 09:25:19,411 - INFO - Found 695 keys in source not in target and 8 keys in target not in source.\n",
      "2024-11-27 09:25:19,413 - INFO - Completed comparison for 'master_hub_supplier' vs 'scan_pack_vendor_details'.\n",
      "2024-11-27 09:25:19,758 - INFO - Message sent to C07UN19ETK5: 1732679719.628909\n",
      "2024-11-27 09:25:19,759 - INFO - Starting comparison for base table 'hsn_tax_mapping': 'master_hub_hsn' vs 'procuro_hsn_tax_mapping'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:25:20,572 - INFO - Loaded data from table 'master_hub_hsn' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:25:26,867 - INFO - Loaded data from table 'procuro_hsn_tax_mapping' into DataFrame.\n",
      "2024-11-27 09:25:26,869 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:25:26,904 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:25:27,071 - INFO - Retrieved schema for table 'master_hub_hsn'.\n",
      "2024-11-27 09:25:27,241 - INFO - Retrieved schema for table 'procuro_hsn_tax_mapping'.\n",
      "2024-11-27 09:25:27,372 - INFO - Found 19 common columns, 8 unique to first table, 7 unique to second table.\n",
      "2024-11-27 09:25:27,373 - INFO - No important columns defined for 'hsn_tax_mapping'. Applying checks to all columns except non_imp_columns: ['cgst_1', 'igst_1', 'hsn_code', '_boltic_updated_at', 'id', 'igst_2', 'cess_1', 'cess_2', 'cgst_2', '_boltic_pipe_id', '_boltic_mark_deleted', 'created_at', '_boltic_merged', '_boltic_meta_id', '_boltic_ingested_at', 'sgst_1', 'sgst_2', '_id', '_boltic_id']\n",
      "2024-11-27 09:25:27,378 - INFO - Found 20 keys in source not in target and 15893 keys in target not in source.\n",
      "2024-11-27 09:25:27,420 - INFO - Completed comparison for 'master_hub_hsn' vs 'procuro_hsn_tax_mapping'.\n",
      "2024-11-27 09:25:27,774 - INFO - Message sent to C07UN19ETK5: 1732679727.652489\n",
      "2024-11-27 09:25:27,775 - INFO - Starting comparison for base table 'config_buyer_brand_mapping': 'master_hub_buyer_brand_mapping' vs 'costing_engine_config_buyer_brand_mapping'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:25:28,486 - INFO - Loaded data from table 'master_hub_buyer_brand_mapping' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-27 09:25:29,388 - INFO - Loaded data from table 'costing_engine_config_buyer_brand_mapping' into DataFrame.\n",
      "2024-11-27 09:25:29,396 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:25:29,401 - INFO - Standardized DataFrame for comparison.\n",
      "2024-11-27 09:25:29,563 - INFO - Retrieved schema for table 'master_hub_buyer_brand_mapping'.\n",
      "2024-11-27 09:25:29,726 - INFO - Retrieved schema for table 'costing_engine_config_buyer_brand_mapping'.\n",
      "2024-11-27 09:25:29,730 - INFO - Found 14 common columns, 4 unique to first table, 1 unique to second table.\n",
      "2024-11-27 09:25:29,731 - INFO - No important columns defined for 'config_buyer_brand_mapping'. Applying checks to all columns except non_imp_columns: ['is_active', '_boltic_merged', 'id', 'buyer_email', '_boltic_meta_id', '_boltic_pipe_id', 'buyer_name', '_boltic_updated_at', 'updated_at', '_id', '_boltic_id', '_boltic_mark_deleted', 'created_at', '_boltic_ingested_at']\n",
      "2024-11-27 09:25:29,733 - INFO - Found 0 duplicate entries based on 'id'.\n",
      "2024-11-27 09:25:29,738 - INFO - Found 0 duplicate entries based on 'id'.\n",
      "2024-11-27 09:25:29,746 - INFO - Merged DataFrame has 66 records for mismatch comparison.\n",
      "2024-11-27 09:25:29,752 - INFO - Found 0 mismatches between 'master_hub_buyer_brand_mapping' and 'costing_engine_config_buyer_brand_mapping'.\n",
      "2024-11-27 09:25:29,761 - INFO - Found 0 null values in master table 'master_hub_buyer_brand_mapping'.\n",
      "2024-11-27 09:25:29,761 - INFO - Found 1 null values in target table 'costing_engine_config_buyer_brand_mapping'.\n",
      "2024-11-27 09:25:29,761 - INFO - Found 0 data type issues.\n",
      "2024-11-27 09:25:29,765 - INFO - Found 0 format issues.\n",
      "2024-11-27 09:25:29,765 - INFO - Found 68 keys in source not in target and 15 keys in target not in source.\n",
      "2024-11-27 09:25:29,766 - INFO - Completed comparison for 'master_hub_buyer_brand_mapping' vs 'costing_engine_config_buyer_brand_mapping'.\n",
      "2024-11-27 09:25:30,130 - INFO - Message sent to C07UN19ETK5: 1732679729.997379\n",
      "2024-11-27 09:25:39,606 - INFO - Successfully uploaded 'fynd-jio-impetus-non-prod.Analytics.error_logs' with 42610 records.\n",
      "2024-11-27 09:25:39,608 - INFO - All comparisons completed.\n"
     ]
    }
   ],
   "source": [
    "#imp and non imp column added\n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "from docx import Document\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = 'fynd-jio-impetus-non-prod'       # Replace with your project ID\n",
    "DATASET_ID = 'Impetus_dev_sit'                 # Replace with your dataset ID\n",
    "PREFIXES = ['procuro_', 'costing_engine_', 'scan_pack_', 'pigeon_']  # Define your prefixes\n",
    "\n",
    "# Error log list\n",
    "ERROR_LOG_M = []\n",
    "\n",
    "# Mapping of base table names to their key columns in master and target tables\n",
    "BASE_TABLES = {\n",
    "    'brand': {\n",
    "        'master_key': 'code',\n",
    "        'targets': {\n",
    "            'procuro_': 'code',\n",
    "            'costing_engine_': 'code'\n",
    "        },\n",
    "        'active_filter': {\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True  # Default behavior\n",
    "    },\n",
    "    'brand_pm_mapping': {\n",
    "        'master_key': 'pm_id',\n",
    "        'targets': {\n",
    "            'costing_engine_': 'pm_id'\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'brick': {\n",
    "        'master_key': 'brick_code',\n",
    "        'targets': {\n",
    "            'costing_engine_': 'code'\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'coe_bom_element_type_mapping': {\n",
    "        'master_key': 'coe_name',\n",
    "        'targets': {\n",
    "            'costing_engine_': 'coe_name'\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    # 'event_log': {\n",
    "    #     'master_key': 'user_id',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'user_id'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    'supplier': {\n",
    "        'master_key': 'supplier_code',\n",
    "        'targets': {\n",
    "            'procuro_': 'supplier_code',\n",
    "            'costing_engine_': 'supplier_code'\n",
    "        },\n",
    "        'active_filter': {  # Apply active filter\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'vendor_details': {  # Newly added entry\n",
    "        'master_key': 'supplier_code',  # Using supplier_code as the key\n",
    "        'master_table': 'master_hub_supplier',  # Specify the master table explicitly\n",
    "        'targets': {\n",
    "            'scan_pack_': 'vendor_code'\n",
    "        },\n",
    "        'active_filter': {  # Apply active filter\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'hsn_tax_mapping': {  # Newly added base table for HSN Codes\n",
    "        'master_key': 'hsn_code',  # Assuming 'hsn_code' is the key column\n",
    "        'master_table': 'master_hub_hsn',\n",
    "        'targets': {\n",
    "            'procuro_': 'hsn_code',\n",
    "        },\n",
    "        'perform_checks': False  # Only perform key comparisons\n",
    "    },\n",
    "    'config_buyer_brand_mapping': {  # Newly added entry\n",
    "        'master_key': 'id', \n",
    "        'master_table': 'master_hub_buyer_brand_mapping',  # Specify the master table explicitly\n",
    "        'targets': {\n",
    "            'costing_engine_': 'id'\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },    \n",
    "}\n",
    "\n",
    "# Define Non-Important Columns\n",
    "Non_imp_columns = {\n",
    "    'supplier': ['id', '_id', 'updated_at', 'created_at'],\n",
    "    'vendor_details': ['id', '_id', 'updated_at', 'created_at']  # Add if applicable\n",
    "}\n",
    "\n",
    "# Define Important Columns\n",
    "Imp_columns = {\n",
    "    'brand': ['name', 'id', 'slug', 'code'],\n",
    "    # Add more base tables and their important columns as needed\n",
    "}\n",
    "\n",
    "# Slack configuration\n",
    "SLACK_TOKEN = \"xoxb-2151238541-7946286860052-5FCcfqBPem0xKigGlIcKdLgX\"  # Replace with your Slack token\n",
    "SLACK_CHANNEL = \"C07UN19ETK5\"  # Replace with your Slack channel ID\n",
    "\n",
    "# Initialize Slack client\n",
    "if SLACK_TOKEN and SLACK_CHANNEL:\n",
    "    slack_client = WebClient(token=SLACK_TOKEN)\n",
    "    logging.info(\"Slack client initialized successfully.\")\n",
    "else:\n",
    "    slack_client = None\n",
    "    logging.warning(\"Slack token or channel not found. Slack notifications will be disabled.\")\n",
    "\n",
    "def get_bigquery_client(project_id):\n",
    "    \"\"\"\n",
    "    Initialize and return a BigQuery client.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID.\n",
    "\n",
    "    Returns:\n",
    "        bigquery.Client: An initialized BigQuery client.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        logging.info(\"BigQuery client initialized successfully.\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize BigQuery client: {e}\")\n",
    "        raise\n",
    "\n",
    "def find_common_tables_with_master_hub(client, dataset_name, prefixes, base_tables):\n",
    "    \"\"\"\n",
    "    Find tables in the specified dataset that share the same base name after removing the 'master_hub_' prefix\n",
    "    and exist with other given prefixes.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset to search within.\n",
    "        prefixes (list): List of prefixes to compare with 'master_hub_'.\n",
    "        base_tables (dict): The BASE_TABLES dictionary containing base table configurations.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are base names and values are dictionaries showing which prefixes have tables.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Reference the dataset\n",
    "        dataset_ref = client.dataset(dataset_name)\n",
    "\n",
    "        # List all tables in the dataset\n",
    "        tables = client.list_tables(dataset_ref)\n",
    "        table_names = [table.table_id for table in tables]\n",
    "        logging.info(f\"Found {len(table_names)} tables in dataset '{dataset_name}'.\")\n",
    "\n",
    "        # Dictionary to hold base names and their corresponding tables\n",
    "        common_tables = {}\n",
    "        for base_name, config in base_tables.items():\n",
    "            # Determine the master table\n",
    "            master_table = config.get('master_table', f'master_hub_{base_name}')\n",
    "            if master_table in table_names:\n",
    "                common_tables[base_name] = {'master_hub_': master_table}\n",
    "                # Check for target tables with specified prefixes\n",
    "                for prefix, target_key in config.get('targets', {}).items():\n",
    "                    target_table = f\"{prefix}{base_name}\"\n",
    "                    if target_table in table_names:\n",
    "                        common_tables[base_name][prefix] = target_table\n",
    "            else:\n",
    "                logging.warning(f\"Master table '{master_table}' for base '{base_name}' not found in dataset.\")\n",
    "\n",
    "        # Filter out base names that only have 'master_hub_' but no other matching prefixes\n",
    "        common_tables_with_prefixes = {base_name: tables for base_name, tables in common_tables.items() if len(tables) > 1}\n",
    "\n",
    "        logging.info(f\"Identified {len(common_tables_with_prefixes)} common base names with 'master_hub_' and other specified prefixes.\")\n",
    "        return common_tables_with_prefixes\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Google API Error: {e.message}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return {}\n",
    "\n",
    "def get_table_schema(client, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Retrieve the schema of a specified BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        table_name (str): The name of the table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping column names to their data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table_ref = client.dataset(dataset_name).table(table_name)\n",
    "        table = client.get_table(table_ref)\n",
    "        schema = {field.name: field.field_type for field in table.schema}\n",
    "        logging.info(f\"Retrieved schema for table '{table_name}'.\")\n",
    "        return schema\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Failed to retrieve schema for table '{table_name}': {e.message}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while retrieving schema for table '{table_name}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def load_table_from_bigquery(client, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Load a table from BigQuery into a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        table_name (str): The name of the table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the table data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f\"SELECT * FROM `{PROJECT_ID}.{dataset_name}.{table_name}`\"\n",
    "        df = client.query(query).to_dataframe()\n",
    "        logging.info(f\"Loaded data from table '{table_name}' into DataFrame.\")\n",
    "        return df\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Failed to load table '{table_name}': {e.message}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while loading table '{table_name}': {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def standardize_dataframe(df, exclude_columns=[]):\n",
    "    \"\"\"\n",
    "    Standardize string columns in the DataFrame by stripping whitespace and converting to lowercase,\n",
    "    excluding specified columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to standardize.\n",
    "        exclude_columns (list): Columns to exclude from standardization.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Standardized DataFrame.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in df_copy.columns:\n",
    "        if col in exclude_columns:\n",
    "            continue  # Skip standardizing this column\n",
    "        if pd.api.types.is_string_dtype(df_copy[col]):\n",
    "            df_copy[col] = df_copy[col].astype(str).str.strip().str.lower()\n",
    "    logging.info(\"Standardized DataFrame for comparison.\")\n",
    "    return df_copy\n",
    "\n",
    "def find_common_and_non_common_columns(df1, df2):\n",
    "    \"\"\"\n",
    "    Identify common and unique columns between two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df1 (pd.DataFrame): First DataFrame.\n",
    "        df2 (pd.DataFrame): Second DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (common_columns, df1_unique_columns, df2_unique_columns)\n",
    "    \"\"\"\n",
    "    common_columns = list(set(df1.columns).intersection(set(df2.columns)))\n",
    "    df1_unique_columns = list(set(df1.columns) - set(df2.columns))\n",
    "    df2_unique_columns = list(set(df2.columns) - set(df1.columns))\n",
    "    logging.info(f\"Found {len(common_columns)} common columns, {len(df1_unique_columns)} unique to first table, {len(df2_unique_columns)} unique to second table.\")\n",
    "    return common_columns, df1_unique_columns, df2_unique_columns\n",
    "\n",
    "def find_mismatches(df_master, df_target, columns_to_check, master_key, target_key, table1, table2, duplicates_master, duplicates_target, non_imp_columns):\n",
    "    \"\"\"\n",
    "    Identify mismatches between two DataFrames based on specified columns and key columns.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "        df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "        columns_to_check (list): List of columns to apply mismatch checks.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        table1 (str): Name of the source table.\n",
    "        table2 (str): Name of the target table.\n",
    "        duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "        duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "        non_imp_columns (list): List of non-important columns to exclude.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mismatches, error_logs_m)\n",
    "    \"\"\"\n",
    "    mismatches = []\n",
    "    error_logs_m = []\n",
    "    # Ensure key columns are present in both DataFrames\n",
    "    if master_key not in df_master.columns or target_key not in df_target.columns:\n",
    "        logging.error(f\"Key columns '{master_key}' or '{target_key}' not found in the respective tables.\")\n",
    "        return mismatches, error_logs_m\n",
    "\n",
    "    # Rename target key to match master key for easier comparison\n",
    "    df_target_renamed = df_target.rename(columns={target_key: master_key})\n",
    "\n",
    "    # Merge DataFrames on the master_key, excluding duplicates\n",
    "    merged_df = pd.merge(\n",
    "        df_master.drop_duplicates(subset=master_key),\n",
    "        df_target_renamed.drop_duplicates(subset=master_key),\n",
    "        on=master_key,\n",
    "        suffixes=(f'_{table1}', f'_{table2}'),\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Merged DataFrame has {len(merged_df)} records for mismatch comparison.\")\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        key = row[master_key]\n",
    "        for column in columns_to_check:\n",
    "            if column.startswith('_boltic_') or column in non_imp_columns:\n",
    "                continue  # Skip columns starting with '_boltic_' or non-important columns\n",
    "            if column not in row:\n",
    "                continue  # Skip if column is not in the merged row\n",
    "            val_master = row.get(f\"{column}_{table1}\")\n",
    "            val_target = row.get(f\"{column}_{table2}\")\n",
    "            # Handle NaN values in comparison\n",
    "            if pd.isna(val_master) and pd.isna(val_target):\n",
    "                continue  # Both are NaN, treat as equal\n",
    "            elif pd.isna(val_master) or pd.isna(val_target) or val_master != val_target:\n",
    "                mismatch_detail = {\n",
    "                    master_key: key,\n",
    "                    'column': column,\n",
    "                    f'{table1}_value': val_master,\n",
    "                    f'{table2}_value': val_target\n",
    "                }\n",
    "                mismatches.append(mismatch_detail)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'mismatch',\n",
    "                    'error_message': '',\n",
    "                    'source_table': table1,\n",
    "                    'target_table': table2,\n",
    "                    'issue_column': column,\n",
    "                    'unique_identifier': f'{master_key}: {key}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(mismatches)} mismatches between '{table1}' and '{table2}'.\")\n",
    "    return mismatches, error_logs_m\n",
    "\n",
    "def find_duplicates(df, key_column, table_name):\n",
    "    \"\"\"\n",
    "    Detect duplicate key_column entries in the DataFrame and identify differences.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to check.\n",
    "        key_column (str): The key column to check for duplicates.\n",
    "        table_name (str): Name of the table being checked.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (duplicate_records_df, error_logs_m)\n",
    "    \"\"\"\n",
    "    if key_column not in df.columns:\n",
    "        logging.error(f\"Key column '{key_column}' not found in DataFrame.\")\n",
    "        return pd.DataFrame(), []\n",
    "\n",
    "    # Get all duplicate entries (keep=False to get all duplicates)\n",
    "    duplicates_df = df[df.duplicated(subset=key_column, keep=False)]\n",
    "\n",
    "    # Group by key_column\n",
    "    grouped = duplicates_df.groupby(key_column)\n",
    "\n",
    "    duplicate_records = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    for key, group in grouped:\n",
    "        if len(group) <= 1:\n",
    "            continue  # Not a duplicate\n",
    "\n",
    "        # Drop key_column and any columns starting with '_boltic_'\n",
    "        group_non_key = group.drop(columns=[key_column] + [col for col in group.columns if col.startswith('_boltic_')])\n",
    "\n",
    "        # Check if all rows are identical\n",
    "        if group_non_key.nunique().sum() == 0:\n",
    "            difference = \"No difference exists\"\n",
    "        else:\n",
    "            # Find which columns have differences\n",
    "            cols_with_diff = group_non_key.columns[group_non_key.nunique() > 1].tolist()\n",
    "            difference = \"Difference in value of columns: \" + ', '.join(cols_with_diff)\n",
    "\n",
    "        duplicate_records.append({\n",
    "            key_column: key,\n",
    "            'Difference in value': difference\n",
    "        })\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'duplicate',\n",
    "            'error_message': f'{difference}',\n",
    "            'source_table': f'{table_name}',\n",
    "            'target_table': '',\n",
    "            'issue_column': '',\n",
    "            'unique_identifier': f'{key_column}: {key}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(duplicate_records)} duplicate entries based on '{key_column}'.\")\n",
    "    return pd.DataFrame(duplicate_records), error_logs_m\n",
    "\n",
    "def validate_data_types(schema_master, schema_target, master_key, table1_name, table2_name, columns_to_check):\n",
    "    \"\"\"\n",
    "    Compare data types of specified columns between master and target schemas.\n",
    "\n",
    "    Args:\n",
    "        schema_master (dict): Schema of the master table.\n",
    "        schema_target (dict): Schema of the target table.\n",
    "        master_key (str): The key column for reference.\n",
    "        table1_name (str): Name of the first table.\n",
    "        table2_name (str): Name of the second table.\n",
    "        columns_to_check (list): List of columns to validate data types.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (data_type_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "    data_type_issues = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    # Identify common columns to check\n",
    "    common_columns = set(columns_to_check).intersection(set(schema_master.keys()), set(schema_target.keys()))\n",
    "\n",
    "    for column in common_columns:\n",
    "        type_master = schema_master[column]\n",
    "        type_target = schema_target[column]\n",
    "        if type_master != type_target:\n",
    "            data_type_issues.append({\n",
    "                'column_name': column,\n",
    "                f'{table1_name}_data_type': type_master,\n",
    "                f'{table2_name}_data_type': type_target\n",
    "            })\n",
    "            error_detail = {\n",
    "                'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'data_type_issues',\n",
    "                'error_message': f'{table1_name}_data_type: {type_master} , {table2_name}_data_type: {type_target}',\n",
    "                'source_table': table1_name,\n",
    "                'target_table': table2_name,\n",
    "                'issue_column': column,\n",
    "                'unique_identifier': ''\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(data_type_issues)} data type issues.\")\n",
    "    return pd.DataFrame(data_type_issues), error_logs_m\n",
    "\n",
    "def validate_formats(df_master, df_target, key_column, target_key, target_table, master_table, columns_to_check):\n",
    "    \"\"\"\n",
    "    Validate specific column formats using regular expressions and include corresponding target table values.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "        master_table (str): The name of the master table.\n",
    "        columns_to_check (list): List of columns to validate formats.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (format_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "    format_issues = pd.DataFrame(columns=[key_column, 'column', 'value', 'issue', f'{target_table}_value'])\n",
    "    error_logs_m = []\n",
    "\n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "\n",
    "        # GSTIN format validation\n",
    "        if 'gstin' in columns_to_check and 'gstin' in df_master.columns:\n",
    "            gstin = str(row['gstin']).strip()\n",
    "            if not re.match(r'^[0-9]{2}[A-Z]{5}[0-9]{4}[A-Z]{1}[A-Z0-9]{3}$', gstin):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['gstin'] if 'gstin' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'gstin',\n",
    "                    'value': row['gstin'],\n",
    "                    'issue': 'Invalid GSTIN format',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Invalid GSTIN format',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'gstin',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "        # Email format validation\n",
    "        if 'email' in columns_to_check and 'email' in df_master.columns:\n",
    "            email = str(row['email']).strip()\n",
    "            if not re.match(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', email):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['email'] if 'email' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'email',\n",
    "                    'value': row['email'],\n",
    "                    'issue': 'Invalid email format',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Invalid email format',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'email',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "        # Pincode format validation\n",
    "        if 'pincode' in columns_to_check and 'pincode' in df_master.columns:\n",
    "            pincode = str(row['pincode']).strip()\n",
    "            if not re.match(r'^\\d{6}$', pincode):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['pincode'] if 'pincode' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'pincode',\n",
    "                    'value': row['pincode'],\n",
    "                    'issue': 'Pincode must be exactly 6 digits',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Pincode must be exactly 6 digits',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'pincode',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(format_issues)} format issues.\")\n",
    "    return format_issues, error_logs_m\n",
    "\n",
    "def validate_pincode_mapping(df_master, df_target, key_column, target_key, target_table, client, master_table):\n",
    "    \"\"\"\n",
    "    Validate pincode mapping by comparing with the all_india_PO_list reference table.\n",
    "    If a pincode issue is found in the master table, then check the corresponding pincode in the target table.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        master_table (str): Name of the master table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pincode_mapping_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "\n",
    "    error_logs_m = []\n",
    "    # Read the reference table from Analytics dataset\n",
    "    try:\n",
    "        reference_table = \"all_india_po_list\"\n",
    "        reference_dataset = \"analytics_data\"\n",
    "        query = f\"SELECT pincode, city, state FROM `{PROJECT_ID}.{reference_dataset}.{reference_table}`\"\n",
    "        reference_df = client.query(query).to_dataframe()\n",
    "        reference_df['pincode'] = reference_df['pincode'].astype(str).str.strip()\n",
    "        reference_df['city'] = reference_df['city'].astype(str).str.strip().str.lower()\n",
    "        reference_df['state'] = reference_df['state'].astype(str).str.strip().str.lower()\n",
    "        logging.info(f\"Loaded reference pincode mapping from '{reference_table}' in '{reference_dataset}' dataset.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load reference pincode mapping: {e}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Check if df_master has 'pincode', 'city', 'state' columns\n",
    "    required_columns = {'pincode', 'city', 'state'}\n",
    "    if not required_columns.issubset(df_master.columns):\n",
    "        logging.info(f\"DataFrame does not have required columns for pincode mapping validation: {required_columns}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Initialize the issues DataFrame with a single target table details column\n",
    "    pincode_mapping_issues = pd.DataFrame(columns=[\n",
    "        key_column, 'pincode', 'state', 'city', 'issue',\n",
    "        f'{target_table}_details'\n",
    "    ])\n",
    "\n",
    "    # Iterate over each row in df_master to validate pincode mapping\n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "        pincode = str(row['pincode']).strip()\n",
    "        city = str(row['city']).strip().lower()\n",
    "        state = str(row['state']).strip().lower()\n",
    "\n",
    "        # Fetch corresponding target row if exists\n",
    "        target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value]\n",
    "        if not target_row.empty:\n",
    "            target_row = target_row.iloc[0]\n",
    "            target_pincode = target_row['pincode'] if 'pincode' in target_row and pd.notnull(target_row['pincode']) else \"Pincode missing\"\n",
    "            target_state = target_row['state'] if 'state' in target_row and pd.notnull(target_row['state']) else \"State missing\"\n",
    "            target_city = target_row['city'] if 'city' in target_row and pd.notnull(target_row['city']) else \"City missing\"\n",
    "            target_details = f\"Pincode: {target_pincode}, State: {target_state}, City: {target_city}\"\n",
    "        else:\n",
    "            target_details = f\"Key '{key_column}' with value '{key_value}' not present in target table '{target_table}'.\"\n",
    "\n",
    "        # Check if pincode exists in reference\n",
    "        ref_matches = reference_df[reference_df['pincode'] == pincode]\n",
    "        if ref_matches.empty:\n",
    "            issue = f\"Invalid pincode ({pincode}).\"\n",
    "            pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "                key_column: key_value,\n",
    "                'pincode': pincode,\n",
    "                'state': state,\n",
    "                'city': city,\n",
    "                'issue': issue,\n",
    "                f'{target_table}_details': target_details\n",
    "            }])], ignore_index=True)\n",
    "            error_detail = {\n",
    "                'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'pincode_mapping',\n",
    "                'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "                'source_table': master_table,\n",
    "                'target_table': target_table,\n",
    "                'issue_column': 'pincode',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "            continue\n",
    "\n",
    "        # Check if any of the reference entries match both the city and state\n",
    "        exact_match = ref_matches[\n",
    "            (ref_matches['city'] == city) & (ref_matches['state'] == state)\n",
    "        ]\n",
    "        if not exact_match.empty:\n",
    "            continue  # No issue, mapping is correct\n",
    "\n",
    "        # Check for state mismatch\n",
    "        state_matches = ref_matches[ref_matches['state'] == state]\n",
    "\n",
    "        # Check for city mismatch\n",
    "        city_matches = ref_matches[ref_matches['city'] == city]\n",
    "\n",
    "        if state_matches.empty and city_matches.empty:\n",
    "            # Both state and city do not match\n",
    "            expected_entries = ref_matches[['state', 'city']].drop_duplicates()\n",
    "            expected_states = expected_entries['state'].tolist()\n",
    "            expected_cities = expected_entries['city'].tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}' and city '{city}'. Expected states: {expected_states_str}; Expected cities: {expected_cities_str}.\"\n",
    "        elif state_matches.empty:\n",
    "            # State does not match\n",
    "            expected_states = ref_matches['state'].unique().tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}'. Expected states: {expected_states_str}.\"\n",
    "        elif city_matches.empty:\n",
    "            # City does not match\n",
    "            expected_cities = state_matches['city'].unique().tolist()\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match city '{city}'. Expected cities: {expected_cities_str}.\"\n",
    "        else:\n",
    "            # Other cases\n",
    "            issue = f\"Pincode {pincode} has a mapping inconsistency.\"\n",
    "\n",
    "        pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "            key_column: key_value,\n",
    "            'pincode': pincode,\n",
    "            'state': state,\n",
    "            'city': city,\n",
    "            'issue': issue,\n",
    "            f'{target_table}_details': target_details\n",
    "        }])], ignore_index=True)\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'pincode_mapping',\n",
    "            'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table,\n",
    "            'issue_column': 'pincode',\n",
    "            'unique_identifier': f'{key_column}: {key_value}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(pincode_mapping_issues)} pincode mapping issues in master table '{master_table}'.\")\n",
    "    return pincode_mapping_issues, error_logs_m\n",
    "\n",
    "def create_table(doc, data, column_names):\n",
    "    \"\"\"\n",
    "    Helper function to create a table in a docx document from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        data (list or list of dict): Data to populate the table.\n",
    "        column_names (list): List of column names for the table headers.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    table = doc.add_table(rows=1, cols=len(column_names))\n",
    "    table.style = 'Light List Accent 1'\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for i, col_name in enumerate(column_names):\n",
    "        hdr_cells[i].text = col_name\n",
    "\n",
    "    for row_data in data:\n",
    "        row_cells = table.add_row().cells\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            cell_value = str(row_data.get(col_name, '')).strip()\n",
    "            row_cells[i].text = cell_value\n",
    "    logging.info(\"Added table to the Word document.\")\n",
    "\n",
    "def add_non_matching_keys_section(doc, df1_only_keys, table1_name, df2_only_keys, table2_name, key_column_master, key_column_target):\n",
    "    \"\"\"\n",
    "    Add a section in the Word document for non-matching keys between two tables.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        df1_only_keys (list): Keys present only in table1.\n",
    "        table1_name (str): Name of the first table.\n",
    "        df2_only_keys (list): Keys present only in table2.\n",
    "        table2_name (str): Name of the second table.\n",
    "        key_column_master (str): The key column in the master table.\n",
    "        key_column_target (str): The key column in the target table.\n",
    "    \"\"\"\n",
    "    if df1_only_keys or df2_only_keys:\n",
    "        if df1_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_master}' present only in '{table1_name}' and not in '{table2_name}' ({len(df1_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_master: key[key_column_master]} for key in df1_only_keys], [key_column_master])\n",
    "        if df2_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_target}' present only in '{table2_name}' and not in '{table1_name}' ({len(df2_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_target: key[key_column_target]} for key in df2_only_keys], [key_column_target])\n",
    "    else:\n",
    "        doc.add_paragraph(\"No non-matching keys found.\")\n",
    "\n",
    "def add_table_of_contents(doc):\n",
    "    \"\"\"\n",
    "    Adds a Table of Contents to the Word document.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "    \"\"\"\n",
    "    paragraph = doc.add_paragraph()\n",
    "    run = paragraph.add_run()\n",
    "    fldChar_begin = OxmlElement('w:fldChar')  # creates a new element\n",
    "    fldChar_begin.set(qn('w:fldCharType'), 'begin')  # sets attribute on element\n",
    "    instrText = OxmlElement('w:instrText')\n",
    "    instrText.set(qn('xml:space'), 'preserve')  # sets attribute on element\n",
    "    instrText.text = 'TOC \\\\o \"1-2\" \\\\h \\\\z \\\\u'  # change to what you need\n",
    "    fldChar_separate = OxmlElement('w:fldChar')\n",
    "    fldChar_separate.set(qn('w:fldCharType'), 'separate')\n",
    "    fldChar_end = OxmlElement('w:fldChar')\n",
    "    fldChar_end.set(qn('w:fldCharType'), 'end')\n",
    "    run._r.append(fldChar_begin)\n",
    "    run._r.append(instrText)\n",
    "    run._r.append(fldChar_separate)\n",
    "    run._r.append(fldChar_end)\n",
    "    logging.info(\"Added Table of Contents to the Word document.\")\n",
    "\n",
    "def create_aggregated_document(all_results, base_name):\n",
    "    \"\"\"\n",
    "    Creates a single Word document that presents all comparison results for a base table.\n",
    "\n",
    "    Args:\n",
    "        all_results (list): List of comparison result dictionaries.\n",
    "        base_name (str): The base name of the table.\n",
    "\n",
    "    Returns:\n",
    "        str: The filepath of the saved report.\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "    doc.add_heading(f'{base_name.capitalize()} Tables Comparison Report', level=0)\n",
    "    doc.add_paragraph(f'Report generated on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "    # Add Instruction for TOC Update\n",
    "    doc.add_paragraph(\n",
    "        \"📌 **Note:** To update the Table of Contents and make the links clickable, go to the ‘References’ tab and click ‘Update Table’ or press F9 in Windows and Fn+F9 in Mac after opening this document in Microsoft Word.\",\n",
    "        style='Intense Quote'\n",
    "    )\n",
    "\n",
    "    # Add Table of Contents\n",
    "    doc.add_heading('Table of Contents', level=1)\n",
    "    add_table_of_contents(doc)\n",
    "    doc.add_page_break()\n",
    "\n",
    "    for result in all_results:\n",
    "        table1_name = result['table1_name']\n",
    "        table2_name = result['table2_name']\n",
    "        key_column_master = result['key_column_master']\n",
    "        key_column_target = result['key_column_target']\n",
    "        doc.add_heading(f'Comparison: {table1_name} vs {table2_name}', level=1)\n",
    "\n",
    "        # Mismatches\n",
    "        if result['mismatches']:\n",
    "            doc.add_heading(f'Mismatches ({len(result[\"mismatches\"])})', level=2)\n",
    "            column_names = [key_column_master, 'column', f'{table1_name}_value', f'{table2_name}_value']\n",
    "            create_table(doc, result['mismatches'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No mismatches found.\", level=2)\n",
    "\n",
    "        # Null values in master table\n",
    "        if not result['null_values_master'].empty:\n",
    "            count_null_master = len(result['null_values_master'])\n",
    "            doc.add_heading(f'Null values in {table1_name} ({count_null_master})', level=2)\n",
    "            column_names = [key_column_master, 'column', table2_name]\n",
    "            create_table(doc, result['null_values_master'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table1_name}.\", level=2)\n",
    "\n",
    "        # Null values in target table\n",
    "        if not result['null_values_target'].empty:\n",
    "            count_null_target = len(result['null_values_target'])\n",
    "            doc.add_heading(f'Null values in {table2_name} ({count_null_target})', level=2)\n",
    "            column_names = [key_column_target, 'column', table1_name]\n",
    "            create_table(doc, result['null_values_target'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Duplicate keys in master table\n",
    "        if not result['duplicates_master'].empty:\n",
    "            count_dup_master = len(result['duplicates_master'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table1_name} ({count_dup_master})', level=2)\n",
    "            create_table(doc, result['duplicates_master'].to_dict('records'), [key_column_master, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(\"No duplicate keys found in master table.\", level=2)\n",
    "\n",
    "        # Duplicate keys in target table\n",
    "        if not result['duplicates_target'].empty:\n",
    "            count_dup_target = len(result['duplicates_target'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table2_name} ({count_dup_target})', level=2)\n",
    "            create_table(doc, result['duplicates_target'].to_dict('records'), [key_column_target, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(f\"No duplicate keys found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Data type issues\n",
    "        if not result['data_type_issues'].empty:\n",
    "            count_data_type_issues = len(result['data_type_issues'])\n",
    "            doc.add_heading(f'Data Type Issues ({count_data_type_issues})', level=2)\n",
    "            column_names = ['column_name', f'{table1_name}_data_type', f'{table2_name}_data_type']\n",
    "            create_table(doc, result['data_type_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No data type issues found.\", level=2)\n",
    "\n",
    "        # Format issues in master table with target values\n",
    "        if not result['format_issues_master'].empty:\n",
    "            count_format_issues_master = len(result['format_issues_master'])\n",
    "            doc.add_heading(f'Format Issues in {table1_name} ({count_format_issues_master})', level=2)\n",
    "            column_names_master = [key_column_master, 'column', 'value', 'issue', f'{table2_name}_value']\n",
    "            create_table(doc, result['format_issues_master'].to_dict('records'), column_names_master)\n",
    "        else:\n",
    "            doc.add_heading(f\"No format issues found in {table1_name}.\", level=2)\n",
    "\n",
    "        # Pincode Mapping Issues with target details\n",
    "        if not result['pincode_mapping_issues'].empty:\n",
    "            count_pincode_issues = len(result['pincode_mapping_issues'])\n",
    "            doc.add_heading(f'Pincode Mapping Issues in {table1_name} ({count_pincode_issues})', level=2)\n",
    "            column_names = [\n",
    "                key_column_master, 'pincode', 'state', 'city', 'issue',\n",
    "                f'{table2_name}_details'\n",
    "            ]\n",
    "            create_table(doc, result['pincode_mapping_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No pincode mapping issues found.\", level=2)\n",
    "\n",
    "        # Non-matching keys in master DataFrame\n",
    "        if result['df_master_only_keys']:\n",
    "            count_master_only = len(result['df_master_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table1_name} ({count_master_only})', level=2)\n",
    "            column_names = [key_column_master]\n",
    "            create_table(doc, result['df_master_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table1_name}.\", level=2)\n",
    "\n",
    "        # Non-matching keys in target DataFrame\n",
    "        if result['df_target_only_keys']:\n",
    "            count_target_only = len(result['df_target_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table2_name} ({count_target_only})', level=2)\n",
    "            column_names = [key_column_target]\n",
    "            create_table(doc, result['df_target_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table2_name}.\", level=2)\n",
    "\n",
    "        doc.add_page_break()  # Optional: Add a page break between comparisons\n",
    "\n",
    "    # Save the aggregated document to the current directory\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_filename = f\"{base_name}_comparison_report_aggregated_{timestamp}.docx\"\n",
    "    doc.save(report_filename)\n",
    "    logging.info(f\"Saved aggregated comparison report as '{report_filename}'.\")\n",
    "\n",
    "    return report_filename  # Return the filename for further processing\n",
    "\n",
    "def send_slack_alert(message):\n",
    "    \"\"\"\n",
    "    Send a message to a specified Slack channel.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to send.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping Slack notification.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = slack_client.chat_postMessage(\n",
    "            channel=SLACK_CHANNEL,\n",
    "            text=message\n",
    "        )\n",
    "        logging.info(f\"Message sent to {SLACK_CHANNEL}: {response['ts']}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Error sending message to Slack: {e.response['error']}\")\n",
    "\n",
    "def upload_file_to_slack(filepath, title=None):\n",
    "    \"\"\"\n",
    "    Upload a file to the specified Slack channel using files_upload_v2.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the file to upload.\n",
    "        title (str, optional): The title for the uploaded file. Defaults to the file's basename.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping file upload.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            response = slack_client.files_upload_v2(\n",
    "                channel=SLACK_CHANNEL,\n",
    "                file=f,\n",
    "                filename=os.path.basename(filepath),  # Explicitly set the filename with extension\n",
    "                title=title if title else os.path.basename(filepath),  # Set the title\n",
    "                initial_comment=title if title else \"File uploaded.\"  # Optional: Add an initial comment\n",
    "            )\n",
    "\n",
    "        # Verify if the upload was successful\n",
    "        if response.get('ok'):\n",
    "            file_permalink = response['file']['permalink']\n",
    "            logging.info(f\"File uploaded to Slack channel '{SLACK_CHANNEL}': {file_permalink}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to upload file to Slack: {response}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Slack API Error during file upload: {e.response['error']}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during file upload: {e}\")\n",
    "\n",
    "def find_non_matching_keys(df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table):\n",
    "    \"\"\"\n",
    "    Identify keys present in df_master but not in df_target and vice versa, including duplicates.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame.\n",
    "        df_target (pd.DataFrame): Target DataFrame.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "        duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "        master_table (str): Name of the master table.\n",
    "        target_table (str): Name of the target table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (master_only_keys, target_only_keys, error_logs_m)\n",
    "    \"\"\"\n",
    "    error_logs_m = []\n",
    "    # Include all keys, including duplicates\n",
    "    keys_master = set(df_master[master_key].astype(str).str.strip())\n",
    "    keys_target = set(df_target[target_key].astype(str).str.strip())\n",
    "\n",
    "    # Keys present only in master\n",
    "    master_only = keys_master - keys_target\n",
    "    # Keys present only in target\n",
    "    target_only = keys_target - keys_master\n",
    "\n",
    "    logging.info(f\"Found {len(master_only)} keys in source not in target and {len(target_only)} keys in target not in source.\")\n",
    "\n",
    "    # Convert to list of dictionaries for consistency\n",
    "    master_only_keys = [{master_key: key} for key in master_only]\n",
    "    target_only_keys = [{target_key: key} for key in target_only]\n",
    "\n",
    "    # Log errors for keys only in master\n",
    "    for key in master_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{master_key}' with value '{key}' is present only in '{master_table}' and missing in '{target_table}'.\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table, \n",
    "            'issue_column': master_key,\n",
    "            'unique_identifier': f\"{master_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    # Log errors for keys only in target\n",
    "    for key in target_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{target_key}' with value '{key}' is present only in '{target_table}' and missing in '{master_table}'.\",\n",
    "            'source_table': target_table,\n",
    "            'target_table': master_table,\n",
    "            'issue_column': target_key,\n",
    "            'unique_identifier': f\"{target_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    return master_only_keys, target_only_keys, error_logs_m\n",
    "\n",
    "def find_detailed_nulls(df_master, df_target, master_key, target_key, master_table, target_table, columns_to_check):\n",
    "    \"\"\"\n",
    "    Identify null values in both master and target tables for specified columns and fetch corresponding values or indicate missing keys.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "        df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        master_table (str): Name of the master table.\n",
    "        target_table (str): Name of the target table.\n",
    "        columns_to_check (list): List of columns to check for null values.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (null_values_master, null_values_target, error_logs_m)\n",
    "    \"\"\"\n",
    "    null_values_master = []\n",
    "    null_values_target = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    # Find nulls in master\n",
    "    null_master = df_master[df_master[columns_to_check].isnull().any(axis=1)]\n",
    "    for idx, row in null_master.iterrows():\n",
    "        key_value = str(row[master_key]).strip()\n",
    "        for column in columns_to_check:\n",
    "            if column == master_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and non-important columns\n",
    "            if column not in row:\n",
    "                continue  # Skip if column is not in the row\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row[column] if column in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                null_record = {\n",
    "                    master_key: key_value,\n",
    "                    'column': column,\n",
    "                    target_table: target_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'null',\n",
    "                    'error_message': 'Null in columns',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': column,\n",
    "                    'unique_identifier': f'{master_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_master.append(null_record)\n",
    "\n",
    "    # Find nulls in target\n",
    "    null_target = df_target[df_target[columns_to_check].isnull().any(axis=1)]\n",
    "    for idx, row in null_target.iterrows():\n",
    "        key_value = str(row[target_key]).strip()\n",
    "        for column in columns_to_check:\n",
    "            if column == target_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and non-important columns\n",
    "            if column not in row:\n",
    "                continue  # Skip if column is not in the row\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_master[master_key].astype(str).str.strip().values:\n",
    "                    master_row = df_master[df_master[master_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    master_value = master_row[column] if column in master_row else \"Column not present\"\n",
    "                else:\n",
    "                    master_value = f\"'{master_key}' not present\"\n",
    "                null_record = {\n",
    "                    target_key: key_value,\n",
    "                    'column': column,\n",
    "                    master_table: master_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'null',\n",
    "                    'error_message': 'Null in columns',\n",
    "                    'source_table': target_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': column,\n",
    "                    'unique_identifier': f'{target_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_target.append(null_record)\n",
    "\n",
    "    logging.info(f\"Found {len(null_values_master)} null values in master table '{master_table}'.\")\n",
    "    logging.info(f\"Found {len(null_values_target)} null values in target table '{target_table}'.\")\n",
    "    return null_values_master, null_values_target, error_logs_m\n",
    "\n",
    "def validate_pincode_mapping(df_master, df_target, key_column, target_key, target_table, client, master_table):\n",
    "    \"\"\"\n",
    "    Validate pincode mapping by comparing with the all_india_PO_list reference table.\n",
    "    If a pincode issue is found in the master table, then check the corresponding pincode in the target table.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        master_table (str): Name of the master table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pincode_mapping_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "\n",
    "    error_logs_m = []\n",
    "    # Read the reference table from Analytics dataset\n",
    "    try:\n",
    "        reference_table = \"all_india_PO_list\"\n",
    "        reference_dataset = \"Analytics\"\n",
    "        query = f\"SELECT pincode, city, state FROM `{PROJECT_ID}.{reference_dataset}.{reference_table}`\"\n",
    "        reference_df = client.query(query).to_dataframe()\n",
    "        reference_df['pincode'] = reference_df['pincode'].astype(str).str.strip()\n",
    "        reference_df['city'] = reference_df['city'].astype(str).str.strip().str.lower()\n",
    "        reference_df['state'] = reference_df['state'].astype(str).str.strip().str.lower()\n",
    "        logging.info(f\"Loaded reference pincode mapping from '{reference_table}' in '{reference_dataset}' dataset.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load reference pincode mapping: {e}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Check if df_master has 'pincode', 'city', 'state' columns\n",
    "    required_columns = {'pincode', 'city', 'state'}\n",
    "    if not required_columns.issubset(df_master.columns):\n",
    "        logging.info(f\"DataFrame does not have required columns for pincode mapping validation: {required_columns}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Initialize the issues DataFrame with a single target table details column\n",
    "    pincode_mapping_issues = pd.DataFrame(columns=[\n",
    "        key_column, 'pincode', 'state', 'city', 'issue',\n",
    "        f'{target_table}_details'\n",
    "    ])\n",
    "\n",
    "    # Iterate over each row in df_master to validate pincode mapping\n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "        pincode = str(row['pincode']).strip()\n",
    "        city = str(row['city']).strip().lower()\n",
    "        state = str(row['state']).strip().lower()\n",
    "\n",
    "        # Fetch corresponding target row if exists\n",
    "        target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value]\n",
    "        if not target_row.empty:\n",
    "            target_row = target_row.iloc[0]\n",
    "            target_pincode = target_row['pincode'] if 'pincode' in target_row and pd.notnull(target_row['pincode']) else \"Pincode missing\"\n",
    "            target_state = target_row['state'] if 'state' in target_row and pd.notnull(target_row['state']) else \"State missing\"\n",
    "            target_city = target_row['city'] if 'city' in target_row and pd.notnull(target_row['city']) else \"City missing\"\n",
    "            target_details = f\"Pincode: {target_pincode}, State: {target_state}, City: {target_city}\"\n",
    "        else:\n",
    "            target_details = f\"Key '{key_column}' with value '{key_value}' not present in target table '{target_table}'.\"\n",
    "\n",
    "        # Check if pincode exists in reference\n",
    "        ref_matches = reference_df[reference_df['pincode'] == pincode]\n",
    "        if ref_matches.empty:\n",
    "            issue = f\"Invalid pincode ({pincode}).\"\n",
    "            pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "                key_column: key_value,\n",
    "                'pincode': pincode,\n",
    "                'state': state,\n",
    "                'city': city,\n",
    "                'issue': issue,\n",
    "                f'{target_table}_details': target_details\n",
    "            }])], ignore_index=True)\n",
    "            error_detail = {\n",
    "                'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'pincode_mapping',\n",
    "                'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "                'source_table': master_table,\n",
    "                'target_table': target_table,\n",
    "                'issue_column': 'pincode',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "            continue\n",
    "\n",
    "        # Check if any of the reference entries match both the city and state\n",
    "        exact_match = ref_matches[\n",
    "            (ref_matches['city'] == city) & (ref_matches['state'] == state)\n",
    "        ]\n",
    "        if not exact_match.empty:\n",
    "            continue  # No issue, mapping is correct\n",
    "\n",
    "        # Check for state mismatch\n",
    "        state_matches = ref_matches[ref_matches['state'] == state]\n",
    "\n",
    "        # Check for city mismatch\n",
    "        city_matches = ref_matches[ref_matches['city'] == city]\n",
    "\n",
    "        if state_matches.empty and city_matches.empty:\n",
    "            # Both state and city do not match\n",
    "            expected_entries = ref_matches[['state', 'city']].drop_duplicates()\n",
    "            expected_states = expected_entries['state'].tolist()\n",
    "            expected_cities = expected_entries['city'].tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}' and city '{city}'. Expected states: {expected_states_str}; Expected cities: {expected_cities_str}.\"\n",
    "        elif state_matches.empty:\n",
    "            # State does not match\n",
    "            expected_states = ref_matches['state'].unique().tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}'. Expected states: {expected_states_str}.\"\n",
    "        elif city_matches.empty:\n",
    "            # City does not match\n",
    "            expected_cities = state_matches['city'].unique().tolist()\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match city '{city}'. Expected cities: {expected_cities_str}.\"\n",
    "        else:\n",
    "            # Other cases\n",
    "            issue = f\"Pincode {pincode} has a mapping inconsistency.\"\n",
    "\n",
    "        pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "            key_column: key_value,\n",
    "            'pincode': pincode,\n",
    "            'state': state,\n",
    "            'city': city,\n",
    "            'issue': issue,\n",
    "            f'{target_table}_details': target_details\n",
    "        }])], ignore_index=True)\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'pincode_mapping',\n",
    "            'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table,\n",
    "            'issue_column': 'pincode',\n",
    "            'unique_identifier': f'{key_column}: {key_value}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(pincode_mapping_issues)} pincode mapping issues in master table '{master_table}'.\")\n",
    "    return pincode_mapping_issues, error_logs_m\n",
    "\n",
    "def create_table(doc, data, column_names):\n",
    "    \"\"\"\n",
    "    Helper function to create a table in a docx document from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        data (list or list of dict): Data to populate the table.\n",
    "        column_names (list): List of column names for the table headers.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    table = doc.add_table(rows=1, cols=len(column_names))\n",
    "    table.style = 'Light List Accent 1'\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for i, col_name in enumerate(column_names):\n",
    "        hdr_cells[i].text = col_name\n",
    "\n",
    "    for row_data in data:\n",
    "        row_cells = table.add_row().cells\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            cell_value = str(row_data.get(col_name, '')).strip()\n",
    "            row_cells[i].text = cell_value\n",
    "    logging.info(\"Added table to the Word document.\")\n",
    "\n",
    "def add_non_matching_keys_section(doc, df1_only_keys, table1_name, df2_only_keys, table2_name, key_column_master, key_column_target):\n",
    "    \"\"\"\n",
    "    Add a section in the Word document for non-matching keys between two tables.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        df1_only_keys (list): Keys present only in table1.\n",
    "        table1_name (str): Name of the first table.\n",
    "        df2_only_keys (list): Keys present only in table2.\n",
    "        table2_name (str): Name of the second table.\n",
    "        key_column_master (str): The key column in the master table.\n",
    "        key_column_target (str): The key column in the target table.\n",
    "    \"\"\"\n",
    "    if df1_only_keys or df2_only_keys:\n",
    "        if df1_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_master}' present only in '{table1_name}' and not in '{table2_name}' ({len(df1_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_master: key[key_column_master]} for key in df1_only_keys], [key_column_master])\n",
    "        if df2_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_target}' present only in '{table2_name}' and not in '{table1_name}' ({len(df2_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_target: key[key_column_target]} for key in df2_only_keys], [key_column_target])\n",
    "    else:\n",
    "        doc.add_paragraph(\"No non-matching keys found.\")\n",
    "\n",
    "def add_table_of_contents(doc):\n",
    "    \"\"\"\n",
    "    Adds a Table of Contents to the Word document.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "    \"\"\"\n",
    "    paragraph = doc.add_paragraph()\n",
    "    run = paragraph.add_run()\n",
    "    fldChar_begin = OxmlElement('w:fldChar')  # creates a new element\n",
    "    fldChar_begin.set(qn('w:fldCharType'), 'begin')  # sets attribute on element\n",
    "    instrText = OxmlElement('w:instrText')\n",
    "    instrText.set(qn('xml:space'), 'preserve')  # sets attribute on element\n",
    "    instrText.text = 'TOC \\\\o \"1-2\" \\\\h \\\\z \\\\u'  # change to what you need\n",
    "    fldChar_separate = OxmlElement('w:fldChar')\n",
    "    fldChar_separate.set(qn('w:fldCharType'), 'separate')\n",
    "    fldChar_end = OxmlElement('w:fldChar')\n",
    "    fldChar_end.set(qn('w:fldCharType'), 'end')\n",
    "    run._r.append(fldChar_begin)\n",
    "    run._r.append(instrText)\n",
    "    run._r.append(fldChar_separate)\n",
    "    run._r.append(fldChar_end)\n",
    "    logging.info(\"Added Table of Contents to the Word document.\")\n",
    "\n",
    "def create_aggregated_document(all_results, base_name):\n",
    "    \"\"\"\n",
    "    Creates a single Word document that presents all comparison results for a base table.\n",
    "\n",
    "    Args:\n",
    "        all_results (list): List of comparison result dictionaries.\n",
    "        base_name (str): The base name of the table.\n",
    "\n",
    "    Returns:\n",
    "        str: The filepath of the saved report.\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "    doc.add_heading(f'{base_name.capitalize()} Tables Comparison Report', level=0)\n",
    "    doc.add_paragraph(f'Report generated on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "    # Add Instruction for TOC Update\n",
    "    doc.add_paragraph(\n",
    "        \"📌 **Note:** To update the Table of Contents and make the links clickable, go to the ‘References’ tab and click ‘Update Table’ or press F9 in Windows and Fn+F9 in Mac after opening this document in Microsoft Word.\",\n",
    "        style='Intense Quote'\n",
    "    )\n",
    "\n",
    "    # Add Table of Contents\n",
    "    doc.add_heading('Table of Contents', level=1)\n",
    "    add_table_of_contents(doc)\n",
    "    doc.add_page_break()\n",
    "\n",
    "    for result in all_results:\n",
    "        table1_name = result['table1_name']\n",
    "        table2_name = result['table2_name']\n",
    "        key_column_master = result['key_column_master']\n",
    "        key_column_target = result['key_column_target']\n",
    "        doc.add_heading(f'Comparison: {table1_name} vs {table2_name}', level=1)\n",
    "\n",
    "        # Mismatches\n",
    "        if result['mismatches']:\n",
    "            doc.add_heading(f'Mismatches ({len(result[\"mismatches\"])})', level=2)\n",
    "            column_names = [key_column_master, 'column', f'{table1_name}_value', f'{table2_name}_value']\n",
    "            create_table(doc, result['mismatches'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No mismatches found.\", level=2)\n",
    "\n",
    "        # Null values in master table\n",
    "        if not result['null_values_master'].empty:\n",
    "            count_null_master = len(result['null_values_master'])\n",
    "            doc.add_heading(f'Null values in {table1_name} ({count_null_master})', level=2)\n",
    "            column_names = [key_column_master, 'column', table2_name]\n",
    "            create_table(doc, result['null_values_master'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table1_name}.\", level=2)\n",
    "\n",
    "        # Null values in target table\n",
    "        if not result['null_values_target'].empty:\n",
    "            count_null_target = len(result['null_values_target'])\n",
    "            doc.add_heading(f'Null values in {table2_name} ({count_null_target})', level=2)\n",
    "            column_names = [key_column_target, 'column', table1_name]\n",
    "            create_table(doc, result['null_values_target'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Duplicate keys in master table\n",
    "        if not result['duplicates_master'].empty:\n",
    "            count_dup_master = len(result['duplicates_master'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table1_name} ({count_dup_master})', level=2)\n",
    "            create_table(doc, result['duplicates_master'].to_dict('records'), [key_column_master, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(\"No duplicate keys found in master table.\", level=2)\n",
    "\n",
    "        # Duplicate keys in target table\n",
    "        if not result['duplicates_target'].empty:\n",
    "            count_dup_target = len(result['duplicates_target'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table2_name} ({count_dup_target})', level=2)\n",
    "            create_table(doc, result['duplicates_target'].to_dict('records'), [key_column_target, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(f\"No duplicate keys found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Data type issues\n",
    "        if not result['data_type_issues'].empty:\n",
    "            count_data_type_issues = len(result['data_type_issues'])\n",
    "            doc.add_heading(f'Data Type Issues ({count_data_type_issues})', level=2)\n",
    "            column_names = ['column_name', f'{table1_name}_data_type', f'{table2_name}_data_type']\n",
    "            create_table(doc, result['data_type_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No data type issues found.\", level=2)\n",
    "\n",
    "        # Format issues in master table with target values\n",
    "        if not result['format_issues_master'].empty:\n",
    "            count_format_issues_master = len(result['format_issues_master'])\n",
    "            doc.add_heading(f'Format Issues in {table1_name} ({count_format_issues_master})', level=2)\n",
    "            column_names_master = [key_column_master, 'column', 'value', 'issue', f'{table2_name}_value']\n",
    "            create_table(doc, result['format_issues_master'].to_dict('records'), column_names_master)\n",
    "        else:\n",
    "            doc.add_heading(f\"No format issues found in {table1_name}.\", level=2)\n",
    "\n",
    "        # Pincode Mapping Issues with target details\n",
    "        if not result['pincode_mapping_issues'].empty:\n",
    "            count_pincode_issues = len(result['pincode_mapping_issues'])\n",
    "            doc.add_heading(f'Pincode Mapping Issues in {table1_name} ({count_pincode_issues})', level=2)\n",
    "            column_names = [\n",
    "                key_column_master, 'pincode', 'state', 'city', 'issue',\n",
    "                f'{table2_name}_details'\n",
    "            ]\n",
    "            create_table(doc, result['pincode_mapping_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No pincode mapping issues found.\", level=2)\n",
    "\n",
    "        # Non-matching keys in master DataFrame\n",
    "        if result['df_master_only_keys']:\n",
    "            count_master_only = len(result['df_master_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table1_name} ({count_master_only})', level=2)\n",
    "            column_names = [key_column_master]\n",
    "            create_table(doc, result['df_master_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table1_name}.\", level=2)\n",
    "\n",
    "        # Non-matching keys in target DataFrame\n",
    "        if result['df_target_only_keys']:\n",
    "            count_target_only = len(result['df_target_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table2_name} ({count_target_only})', level=2)\n",
    "            column_names = [key_column_target]\n",
    "            create_table(doc, result['df_target_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table2_name}.\", level=2)\n",
    "\n",
    "        doc.add_page_break()  # Optional: Add a page break between comparisons\n",
    "\n",
    "    # Save the aggregated document to the current directory\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_filename = f\"{base_name}_comparison_report_aggregated_{timestamp}.docx\"\n",
    "    doc.save(report_filename)\n",
    "    logging.info(f\"Saved aggregated comparison report as '{report_filename}'.\")\n",
    "\n",
    "    return report_filename  # Return the filename for further processing\n",
    "\n",
    "def send_slack_alert(message):\n",
    "    \"\"\"\n",
    "    Send a message to a specified Slack channel.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to send.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping Slack notification.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = slack_client.chat_postMessage(\n",
    "            channel=SLACK_CHANNEL,\n",
    "            text=message\n",
    "        )\n",
    "        logging.info(f\"Message sent to {SLACK_CHANNEL}: {response['ts']}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Error sending message to Slack: {e.response['error']}\")\n",
    "\n",
    "def upload_file_to_slack(filepath, title=None):\n",
    "    \"\"\"\n",
    "    Upload a file to the specified Slack channel using files_upload_v2.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the file to upload.\n",
    "        title (str, optional): The title for the uploaded file. Defaults to the file's basename.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping file upload.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            response = slack_client.files_upload_v2(\n",
    "                channel=SLACK_CHANNEL,\n",
    "                file=f,\n",
    "                filename=os.path.basename(filepath),  # Explicitly set the filename with extension\n",
    "                title=title if title else os.path.basename(filepath),  # Set the title\n",
    "                initial_comment=title if title else \"File uploaded.\"  # Optional: Add an initial comment\n",
    "            )\n",
    "\n",
    "        # Verify if the upload was successful\n",
    "        if response.get('ok'):\n",
    "            file_permalink = response['file']['permalink']\n",
    "            logging.info(f\"File uploaded to Slack channel '{SLACK_CHANNEL}': {file_permalink}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to upload file to Slack: {response}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Slack API Error during file upload: {e.response['error']}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during file upload: {e}\")\n",
    "\n",
    "def find_non_matching_keys(df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table):\n",
    "    \"\"\"\n",
    "    Identify keys present in df_master but not in df_target and vice versa, including duplicates.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame.\n",
    "        df_target (pd.DataFrame): Target DataFrame.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "        duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "        master_table (str): Name of the master table.\n",
    "        target_table (str): Name of the target table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (master_only_keys, target_only_keys, error_logs_m)\n",
    "    \"\"\"\n",
    "    error_logs_m = []\n",
    "    # Include all keys, including duplicates\n",
    "    keys_master = set(df_master[master_key].astype(str).str.strip())\n",
    "    keys_target = set(df_target[target_key].astype(str).str.strip())\n",
    "\n",
    "    # Keys present only in master\n",
    "    master_only = keys_master - keys_target\n",
    "    # Keys present only in target\n",
    "    target_only = keys_target - keys_master\n",
    "\n",
    "    logging.info(f\"Found {len(master_only)} keys in source not in target and {len(target_only)} keys in target not in source.\")\n",
    "\n",
    "    # Convert to list of dictionaries for consistency\n",
    "    master_only_keys = [{master_key: key} for key in master_only]\n",
    "    target_only_keys = [{target_key: key} for key in target_only]\n",
    "\n",
    "    # Log errors for keys only in master\n",
    "    for key in master_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{master_key}' with value '{key}' is present only in '{master_table}' and missing in '{target_table}'.\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table, \n",
    "            'issue_column': master_key,\n",
    "            'unique_identifier': f\"{master_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    # Log errors for keys only in target\n",
    "    for key in target_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{target_key}' with value '{key}' is present only in '{target_table}' and missing in '{master_table}'.\",\n",
    "            'source_table': target_table,\n",
    "            'target_table': master_table,\n",
    "            'issue_column': target_key,\n",
    "            'unique_identifier': f\"{target_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    return master_only_keys, target_only_keys, error_logs_m\n",
    "\n",
    "def find_detailed_nulls(df_master, df_target, master_key, target_key, master_table, target_table, columns_to_check):\n",
    "    \"\"\"\n",
    "    Identify null values in both master and target tables for specified columns and fetch corresponding values or indicate missing keys.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "        df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        master_table (str): Name of the master table.\n",
    "        target_table (str): Name of the target table.\n",
    "        columns_to_check (list): List of columns to check for null values.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (null_values_master, null_values_target, error_logs_m)\n",
    "    \"\"\"\n",
    "    null_values_master = []\n",
    "    null_values_target = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    # Find nulls in master\n",
    "    null_master = df_master[df_master[columns_to_check].isnull().any(axis=1)]\n",
    "    for idx, row in null_master.iterrows():\n",
    "        key_value = str(row[master_key]).strip()\n",
    "        for column in columns_to_check:\n",
    "            if column == master_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and non-important columns\n",
    "            if column not in row:\n",
    "                continue  # Skip if column is not in the row\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row[column] if column in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                null_record = {\n",
    "                    master_key: key_value,\n",
    "                    'column': column,\n",
    "                    target_table: target_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'null',\n",
    "                    'error_message': 'Null in columns',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': column,\n",
    "                    'unique_identifier': f'{master_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_master.append(null_record)\n",
    "\n",
    "    # Find nulls in target\n",
    "    null_target = df_target[df_target[columns_to_check].isnull().any(axis=1)]\n",
    "    for idx, row in null_target.iterrows():\n",
    "        key_value = str(row[target_key]).strip()\n",
    "        for column in columns_to_check:\n",
    "            if column == target_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and non-important columns\n",
    "            if column not in row:\n",
    "                continue  # Skip if column is not in the row\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_master[master_key].astype(str).str.strip().values:\n",
    "                    master_row = df_master[df_master[master_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    master_value = master_row[column] if column in master_row else \"Column not present\"\n",
    "                else:\n",
    "                    master_value = f\"'{master_key}' not present\"\n",
    "                null_record = {\n",
    "                    target_key: key_value,\n",
    "                    'column': column,\n",
    "                    master_table: master_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'null',\n",
    "                    'error_message': 'Null in columns',\n",
    "                    'source_table': target_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': column,\n",
    "                    'unique_identifier': f'{target_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_target.append(null_record)\n",
    "\n",
    "    logging.info(f\"Found {len(null_values_master)} null values in master table '{master_table}'.\")\n",
    "    logging.info(f\"Found {len(null_values_target)} null values in target table '{target_table}'.\")\n",
    "    return null_values_master, null_values_target, error_logs_m\n",
    "\n",
    "def validate_pincode_mapping(df_master, df_target, key_column, target_key, target_table, client, master_table):\n",
    "    \"\"\"\n",
    "    Validate pincode mapping by comparing with the all_india_PO_list reference table.\n",
    "    If a pincode issue is found in the master table, then check the corresponding pincode in the target table.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        master_table (str): Name of the master table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pincode_mapping_issues_df, error_logs_m)\n",
    "    \"\"\"\n",
    "\n",
    "    error_logs_m = []\n",
    "    # Read the reference table from Analytics dataset\n",
    "    try:\n",
    "        reference_table = \"all_india_PO_list\"\n",
    "        reference_dataset = \"Analytics\"\n",
    "        query = f\"SELECT pincode, city, state FROM `{PROJECT_ID}.{reference_dataset}.{reference_table}`\"\n",
    "        reference_df = client.query(query).to_dataframe()\n",
    "        reference_df['pincode'] = reference_df['pincode'].astype(str).str.strip()\n",
    "        reference_df['city'] = reference_df['city'].astype(str).str.strip().str.lower()\n",
    "        reference_df['state'] = reference_df['state'].astype(str).str.strip().str.lower()\n",
    "        logging.info(f\"Loaded reference pincode mapping from '{reference_table}' in '{reference_dataset}' dataset.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load reference pincode mapping: {e}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Check if df_master has 'pincode', 'city', 'state' columns\n",
    "    required_columns = {'pincode', 'city', 'state'}\n",
    "    if not required_columns.issubset(df_master.columns):\n",
    "        logging.info(f\"DataFrame does not have required columns for pincode mapping validation: {required_columns}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "\n",
    "    # Initialize the issues DataFrame with a single target table details column\n",
    "    pincode_mapping_issues = pd.DataFrame(columns=[\n",
    "        key_column, 'pincode', 'state', 'city', 'issue',\n",
    "        f'{target_table}_details'\n",
    "    ])\n",
    "\n",
    "    # Iterate over each row in df_master to validate pincode mapping\n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "        pincode = str(row['pincode']).strip()\n",
    "        city = str(row['city']).strip().lower()\n",
    "        state = str(row['state']).strip().lower()\n",
    "\n",
    "        # Fetch corresponding target row if exists\n",
    "        target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value]\n",
    "        if not target_row.empty:\n",
    "            target_row = target_row.iloc[0]\n",
    "            target_pincode = target_row['pincode'] if 'pincode' in target_row and pd.notnull(target_row['pincode']) else \"Pincode missing\"\n",
    "            target_state = target_row['state'] if 'state' in target_row and pd.notnull(target_row['state']) else \"State missing\"\n",
    "            target_city = target_row['city'] if 'city' in target_row and pd.notnull(target_row['city']) else \"City missing\"\n",
    "            target_details = f\"Pincode: {target_pincode}, State: {target_state}, City: {target_city}\"\n",
    "        else:\n",
    "            target_details = f\"Key '{key_column}' with value '{key_value}' not present in target table '{target_table}'.\"\n",
    "\n",
    "        # Check if pincode exists in reference\n",
    "        ref_matches = reference_df[reference_df['pincode'] == pincode]\n",
    "        if ref_matches.empty:\n",
    "            issue = f\"Invalid pincode ({pincode}).\"\n",
    "            pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "                key_column: key_value,\n",
    "                'pincode': pincode,\n",
    "                'state': state,\n",
    "                'city': city,\n",
    "                'issue': issue,\n",
    "                f'{target_table}_details': target_details\n",
    "            }])], ignore_index=True)\n",
    "            error_detail = {\n",
    "                'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'pincode_mapping',\n",
    "                'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "                'source_table': master_table,\n",
    "                'target_table': target_table,\n",
    "                'issue_column': 'pincode',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "            continue\n",
    "\n",
    "        # Check if any of the reference entries match both the city and state\n",
    "        exact_match = ref_matches[\n",
    "            (ref_matches['city'] == city) & (ref_matches['state'] == state)\n",
    "        ]\n",
    "        if not exact_match.empty:\n",
    "            continue  # No issue, mapping is correct\n",
    "\n",
    "        # Check for state mismatch\n",
    "        state_matches = ref_matches[ref_matches['state'] == state]\n",
    "\n",
    "        # Check for city mismatch\n",
    "        city_matches = ref_matches[ref_matches['city'] == city]\n",
    "\n",
    "        if state_matches.empty and city_matches.empty:\n",
    "            # Both state and city do not match\n",
    "            expected_entries = ref_matches[['state', 'city']].drop_duplicates()\n",
    "            expected_states = expected_entries['state'].tolist()\n",
    "            expected_cities = expected_entries['city'].tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}' and city '{city}'. Expected states: {expected_states_str}; Expected cities: {expected_cities_str}.\"\n",
    "        elif state_matches.empty:\n",
    "            # State does not match\n",
    "            expected_states = ref_matches['state'].unique().tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}'. Expected states: {expected_states_str}.\"\n",
    "        elif city_matches.empty:\n",
    "            # City does not match\n",
    "            expected_cities = state_matches['city'].unique().tolist()\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match city '{city}'. Expected cities: {expected_cities_str}.\"\n",
    "        else:\n",
    "            # Other cases\n",
    "            issue = f\"Pincode {pincode} has a mapping inconsistency.\"\n",
    "\n",
    "        pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "            key_column: key_value,\n",
    "            'pincode': pincode,\n",
    "            'state': state,\n",
    "            'city': city,\n",
    "            'issue': issue,\n",
    "            f'{target_table}_details': target_details\n",
    "        }])], ignore_index=True)\n",
    "        error_detail = {\n",
    "            'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'pincode_mapping',\n",
    "            'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table,\n",
    "            'issue_column': 'pincode',\n",
    "            'unique_identifier': f'{key_column}: {key_value}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(pincode_mapping_issues)} pincode mapping issues in master table '{master_table}'.\")\n",
    "    return pincode_mapping_issues, error_logs_m\n",
    "\n",
    "def compare_tables(client, dataset_name, base_name, master_table, target_table, master_key, target_key):\n",
    "    \"\"\"\n",
    "    Compare two tables and generate a report.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        base_name (str): The base name of the table.\n",
    "        master_table (str): Name of the master_hub_ table.\n",
    "        target_table (str): Name of the target prefixed table.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing all comparison results.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting comparison for base table '{base_name}': '{master_table}' vs '{target_table}'.\")\n",
    "\n",
    "    # Initialize comparison results\n",
    "    mismatches = []\n",
    "    null_values_master = []\n",
    "    null_values_target = []\n",
    "    data_type_issues = pd.DataFrame()\n",
    "    format_issues_master = pd.DataFrame()\n",
    "    pincode_mapping_issues = pd.DataFrame()\n",
    "    duplicates_master = pd.DataFrame()\n",
    "    duplicates_target = pd.DataFrame()\n",
    "    master_only_keys = []\n",
    "    target_only_keys = []\n",
    "\n",
    "    # Load data\n",
    "    df_master = load_table_from_bigquery(client, dataset_name, master_table)\n",
    "    df_target = load_table_from_bigquery(client, dataset_name, target_table)\n",
    "\n",
    "    # Apply standardization\n",
    "    df_master = standardize_dataframe(df_master, exclude_columns=[master_key])\n",
    "    df_target = standardize_dataframe(df_target, exclude_columns=[target_key])\n",
    "\n",
    "    # Apply active filter if defined\n",
    "    base_table_info = BASE_TABLES.get(base_name, {})\n",
    "    active_filter = base_table_info.get('active_filter')\n",
    "    perform_checks = base_table_info.get('perform_checks', True)\n",
    "\n",
    "    if active_filter:\n",
    "        column = active_filter.get('column')\n",
    "        value = active_filter.get('value')\n",
    "        if column and column in df_master.columns:\n",
    "            initial_count = len(df_master)\n",
    "            df_master = df_master[df_master[column] == value]\n",
    "            filtered_count = len(df_master)\n",
    "            logging.info(f\"Filtered '{base_name}' master table: {initial_count - filtered_count} records excluded based on {column} = {value}.\")\n",
    "        else:\n",
    "            logging.warning(f\"Active filter specified but column '{column}' not found in master table '{master_table}'.\")\n",
    "\n",
    "    if df_master.empty or df_target.empty:\n",
    "        logging.warning(f\"One of the tables '{master_table}' or '{target_table}' is empty. Skipping comparison.\")\n",
    "        return None\n",
    "\n",
    "    # Identify BigNumeric columns in master and target tables\n",
    "    schema_master = get_table_schema(client, dataset_name, master_table)\n",
    "    schema_target = get_table_schema(client, dataset_name, target_table)\n",
    "    bignumeric_columns_master = [col for col, dtype in schema_master.items() if dtype == 'BIGNUMERIC']\n",
    "    bignumeric_columns_target = [col for col, dtype in schema_target.items() if dtype == 'BIGNUMERIC']\n",
    "\n",
    "    # Format BigNumeric columns in master table\n",
    "    for col in bignumeric_columns_master:\n",
    "        if col in df_master.columns:\n",
    "            df_master[col] = df_master[col].apply(lambda x: format(x, '.0f') if pd.notnull(x) else x)\n",
    "\n",
    "    # Format BigNumeric columns in target table\n",
    "    for col in bignumeric_columns_target:\n",
    "        if col in df_target.columns:\n",
    "            df_target[col] = df_target[col].apply(lambda x: format(x, '.0f') if pd.notnull(x) else x)\n",
    "\n",
    "    # Get imp_columns and non_imp_columns\n",
    "    imp_columns = Imp_columns.get(base_name, None)\n",
    "    non_imp_columns = Non_imp_columns.get(base_name, [])\n",
    "\n",
    "    # Identify common columns\n",
    "    common_columns, master_unique_cols, target_unique_cols = find_common_and_non_common_columns(df_master, df_target)\n",
    "\n",
    "    if not common_columns:\n",
    "        logging.warning(f\"No common columns found between '{master_table}' and '{target_table}'. Skipping comparison.\")\n",
    "        return None\n",
    "\n",
    "    # Determine columns to check based on imp_columns\n",
    "    if imp_columns:\n",
    "        columns_to_check = [col for col in imp_columns if col in common_columns]\n",
    "        logging.info(f\"Important columns defined for '{base_name}': {columns_to_check}\")\n",
    "    else:\n",
    "        columns_to_check = [col for col in common_columns if col not in non_imp_columns]\n",
    "        logging.info(f\"No important columns defined for '{base_name}'. Applying checks to all columns except non_imp_columns: {columns_to_check}\")\n",
    "\n",
    "    if perform_checks:\n",
    "        # Find duplicates in both tables\n",
    "        duplicates_master, error_logs_m = find_duplicates(df_master, master_key, master_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "        duplicates_target, error_logs_m = find_duplicates(df_target, target_key, target_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    if not duplicates_master.empty:\n",
    "        logging.warning(f\"Duplicate keys found in source table '{master_table}'. These will be reported but not used in mismatch comparison.\")\n",
    "    if not duplicates_target.empty:\n",
    "        logging.warning(f\"Duplicate keys found in target table '{target_table}'. These will be reported but not used in mismatch comparison.\")\n",
    "\n",
    "    # Perform mismatch comparison\n",
    "    if perform_checks:\n",
    "        mismatches, error_logs_m = find_mismatches(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            columns_to_check,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            duplicates_master,\n",
    "            duplicates_target,\n",
    "            non_imp_columns\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Find detailed null values in both tables\n",
    "    if perform_checks:\n",
    "        null_values_master, null_values_target, error_logs_m = find_detailed_nulls(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            columns_to_check  # Pass columns_to_check\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Validate data types between master and target schemas\n",
    "    if perform_checks:\n",
    "        data_type_issues, error_logs_m = validate_data_types(\n",
    "            schema_master,\n",
    "            schema_target,\n",
    "            master_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            columns_to_check  # Pass columns_to_check\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Validate formats in master table only\n",
    "    if perform_checks:\n",
    "        format_issues_master, error_logs_m = validate_formats(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            target_table,\n",
    "            master_table,\n",
    "            columns_to_check  # Pass columns_to_check\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "        # Validate pincode mapping if applicable\n",
    "        pincode_mapping_issues = pd.DataFrame()\n",
    "        if {'pincode', 'city', 'state'}.issubset(df_master.columns):\n",
    "            pincode_mapping_issues, error_logs_m = validate_pincode_mapping(\n",
    "                df_master,\n",
    "                df_target,\n",
    "                master_key,\n",
    "                target_key,\n",
    "                target_table,\n",
    "                client,\n",
    "                master_table\n",
    "            )\n",
    "            ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Find non-matching keys\n",
    "    master_only_keys, target_only_keys, error_logs_m = find_non_matching_keys(\n",
    "        df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table\n",
    "    )\n",
    "    ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        'mismatches': mismatches,\n",
    "        'null_values_master': pd.DataFrame(null_values_master),\n",
    "        'null_values_target': pd.DataFrame(null_values_target),\n",
    "        'duplicates_master': duplicates_master,\n",
    "        'duplicates_target': duplicates_target,\n",
    "        'data_type_issues': data_type_issues,\n",
    "        'format_issues_master': format_issues_master,\n",
    "        'pincode_mapping_issues': pincode_mapping_issues,\n",
    "        'key_column_master': master_key,\n",
    "        'key_column_target': target_key,\n",
    "        'df_master_only_keys': master_only_keys,\n",
    "        'df_target_only_keys': target_only_keys,\n",
    "        'table1_name': master_table,\n",
    "        'table2_name': target_table,\n",
    "        'df_master': df_master,\n",
    "        'df_target': df_target\n",
    "    }\n",
    "\n",
    "    logging.info(f\"Completed comparison for '{master_table}' vs '{target_table}'.\")\n",
    "    return results\n",
    "\n",
    "def generate_string_schema(df):\n",
    "    \"\"\"\n",
    "    Generates a BigQuery schema with all fields as STRING.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame for which to generate the schema.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of SchemaField objects with type STRING.\n",
    "    \"\"\"\n",
    "    schema = [SchemaField(column, \"STRING\", mode=\"NULLABLE\") for column in df.columns]\n",
    "    return schema\n",
    "\n",
    "def _upload_dataframe_to_bigquery(client, analytics_dataset, table_name, df):\n",
    "    \"\"\"\n",
    "    Helper function to upload a DataFrame to BigQuery.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        analytics_dataset (str): The name of the Analytics dataset.\n",
    "        table_name (str): The name of the table to upload.\n",
    "        df (pd.DataFrame): The DataFrame to upload.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logging.info(f\"No data to upload for '{table_name}'. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Convert all columns to string type\n",
    "    df = df.astype(str)\n",
    "\n",
    "    # Generate BigQuery schema with all fields as STRING\n",
    "    schema = generate_string_schema(df)\n",
    "\n",
    "    # Ensure table name doesn't exceed BigQuery's maximum length (1,024 characters)\n",
    "    if len(table_name) > 1024:\n",
    "        original_table_name = table_name\n",
    "        table_name = table_name[:1021] + '...'\n",
    "        logging.warning(f\"Table name truncated from '{original_table_name}' to '{table_name}' due to length constraints.\")\n",
    "\n",
    "    # Define the full table ID\n",
    "    table_id = f\"{client.project}.{analytics_dataset}.{table_name}\"\n",
    "\n",
    "    # Upload the DataFrame to BigQuery\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(\n",
    "            df,\n",
    "            table_id,\n",
    "            job_config=bigquery.LoadJobConfig(\n",
    "                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "                schema=schema  # Using the provided schema with all fields as STRING\n",
    "            )\n",
    "        )\n",
    "        job.result()  # Wait for the job to complete\n",
    "        logging.info(f\"Successfully uploaded '{table_id}' with {len(df)} records.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to upload '{table_id}' to BigQuery: {e}\")\n",
    "\n",
    "def upload_comparison_results_to_bigquery(client, analytics_dataset, ERROR_LOG_M):\n",
    "    \"\"\"\n",
    "    Uploads the ERROR_LOG_M to BigQuery as a separate table in the Analytics dataset.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        analytics_dataset (str): The name of the Analytics dataset.\n",
    "        ERROR_LOG_M (list): The error log data as a list of dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Handle ERROR_LOG separately\n",
    "    if ERROR_LOG_M is not None:\n",
    "        # Determine the DataFrame to upload\n",
    "        if isinstance(ERROR_LOG_M, pd.DataFrame):\n",
    "            error_df = ERROR_LOG_M\n",
    "        elif isinstance(ERROR_LOG_M, list):\n",
    "            error_df = pd.DataFrame(ERROR_LOG_M)\n",
    "        else:\n",
    "            logging.warning(\"Unsupported data type for ERROR_LOG. Skipping upload.\")\n",
    "            error_df = None\n",
    "\n",
    "        if error_df is not None and not error_df.empty:\n",
    "            _upload_dataframe_to_bigquery(client, analytics_dataset, \"error_logs\", error_df)\n",
    "        else:\n",
    "            logging.info(\"No error logs to upload.\")\n",
    "    else:\n",
    "        logging.info(\"No error logs present.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the comparison of multiple base tables against their master_hub_ counterparts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize BigQuery client\n",
    "        try:\n",
    "            client = get_bigquery_client(PROJECT_ID)\n",
    "        except Exception:\n",
    "            logging.error(\"Exiting due to BigQuery client initialization failure.\")\n",
    "            return\n",
    "\n",
    "        # Find common tables with 'master_hub_' and other prefixes, passing BASE_TABLES\n",
    "        common_tables = find_common_tables_with_master_hub(client, DATASET_ID, PREFIXES, BASE_TABLES)\n",
    "\n",
    "        if not common_tables:\n",
    "            logging.info(\"No common tables found with 'master_hub_' and the specified prefixes.\")\n",
    "            return\n",
    "\n",
    "        # Iterate over each base table and perform comparisons\n",
    "        for base_name, tables in common_tables.items():\n",
    "            base_table_info = BASE_TABLES.get(base_name)\n",
    "            if not base_table_info:\n",
    "                logging.warning(f\"No configuration found for base table '{base_name}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            master_key = base_table_info.get('master_key')\n",
    "            target_tables = base_table_info.get('targets', {})\n",
    "            \n",
    "\n",
    "            master_table = tables.get('master_hub_')\n",
    "            if not master_table:\n",
    "                logging.warning(f\"Master table 'master_hub_{base_name}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            all_results = []\n",
    "\n",
    "            # Iterate through each prefix and its corresponding target_key\n",
    "            for prefix, target_key in target_tables.items():\n",
    "                target_table = tables.get(prefix)\n",
    "                if not target_table:\n",
    "                    logging.warning(f\"Target table with prefix '{prefix}' for base table '{base_name}' not found. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                comparison_result = compare_tables(\n",
    "                    client, \n",
    "                    DATASET_ID, \n",
    "                    base_name, \n",
    "                    master_table, \n",
    "                    target_table, \n",
    "                    master_key, \n",
    "                    target_key  # Pass the correct target_key per prefix\n",
    "                )\n",
    "                if comparison_result:\n",
    "                    all_results.append(comparison_result)\n",
    "\n",
    "                    # Prepare and send a separate Slack message for each comparison\n",
    "                    total_mismatches = len(comparison_result['mismatches'])\n",
    "                    total_nulls_master = len(comparison_result['null_values_master'])\n",
    "                    total_nulls_target = len(comparison_result['null_values_target'])\n",
    "                    total_dup_master = len(comparison_result['duplicates_master'])\n",
    "                    total_dup_target = len(comparison_result['duplicates_target'])\n",
    "                    total_data_type_issues = len(comparison_result['data_type_issues'])\n",
    "                    total_format_issues_master = len(comparison_result['format_issues_master'])\n",
    "                    total_pincode_issues = len(comparison_result['pincode_mapping_issues'])\n",
    "                    total_non_matching_source = len(comparison_result.get('df_master_only_keys', []))\n",
    "                    total_non_matching_target = len(comparison_result.get('df_target_only_keys', []))\n",
    "\n",
    "                    message = (\n",
    "                        f\"✅ *Comparison Report Generated for `{base_name}`*\\n\"\n",
    "                        f\"*Tables Compared: `{comparison_result['table1_name']}` vs `{comparison_result['table2_name']}`*\\n\"\n",
    "                        f\"- *Total Mismatches between values of same column name of both tables : `{total_mismatches}`*\\n\"\n",
    "                        f\"- *Total Null Values in `{comparison_result['table1_name']}`: `{total_nulls_master}`*\\n\"\n",
    "                        f\"- *Total Null Values in `{comparison_result['table2_name']}`: `{total_nulls_target}`*\\n\"\n",
    "                        f\"- *Duplicate `{master_key}` in `{comparison_result['table1_name']}`: `{total_dup_master}`*\\n\"\n",
    "                        f\"- *Duplicate `{target_key}` in `{comparison_result['table2_name']}`: `{total_dup_target}`*\\n\"\n",
    "                        f\"- *Total Data Type Issues(mismatch between datatype in columns with same name of both tables): `{total_data_type_issues}`*\\n\"\n",
    "                        f\"- *Total Format/Value Issues(gstin, email, pincode) in `{comparison_result['table1_name']}`: `{total_format_issues_master}`*\\n\"\n",
    "                        f\"- *Total Pincode Mapping Issues in `{comparison_result['table1_name']}`: `{total_pincode_issues}`*\\n\"\n",
    "                         \"- *Non-Matching Keys*:\\n\"\n",
    "                        f\"--*`{master_key}` only in `{comparison_result['table1_name']}` and not in `{comparison_result['table2_name']}`:`{total_non_matching_source}`,*\\n\"\n",
    "                        f\"--*`{target_key}` only in `{comparison_result['table2_name']}` and not in `{comparison_result['table1_name']}`:`{total_non_matching_target}`*\"\n",
    "                    )\n",
    "\n",
    "                    send_slack_alert(message)\n",
    "\n",
    "        # Upload error logs to BigQuery after all comparisons\n",
    "        upload_comparison_results_to_bigquery(\n",
    "            client, \n",
    "            'Analytics',\n",
    "            ERROR_LOG_M\n",
    "            )\n",
    "\n",
    "        logging.info(\"All comparisons completed.\")\n",
    "    except Exception as e:\n",
    "        # Capture the full traceback\n",
    "        tb = traceback.format_exc()\n",
    "        logging.error(\"An unexpected error occurred in the main process.\", exc_info=True)\n",
    "\n",
    "        # Prepare a detailed error message for Slack\n",
    "        error_message = (\n",
    "            f\"❌ *Comparison Process Failed*\\n\"\n",
    "            f\"*Error:* {str(e)}\\n\"\n",
    "            f\"*Traceback:*\\n```{tb}```\"\n",
    "        )\n",
    "        send_slack_alert(error_message)\n",
    "\n",
    "        # Optionally, exit the script with a non-zero status\n",
    "        sys.exit(1)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15335c25-3488-43cf-ac7b-73b2066808f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log fixed\n",
    "#mismatch fixed\n",
    "#main.py\n",
    "#count in each heading\n",
    "#Duplicate_table_fixed\n",
    "#Big_numeric_readibiliy\n",
    "#paragraph to heading\n",
    "#ToC added\n",
    "#format issue enchaned\n",
    "#pincode issue added and enchanced\n",
    "#enchanced scalabiltiy incase target_key changes acorrding to table\n",
    "# working error log\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "from docx import Document\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import traceback\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = 'fynd-jio-impetus-non-prod'       # Replace with your project ID\n",
    "DATASET_ID = 'Impetus_dev_sit'                 # Replace with your dataset ID\n",
    "PREFIXES = ['procuro_', 'costing_engine_', 'scan_pack_', 'pigeon_']  # Define your prefixes\n",
    "# time_stamp,issue, error_message, tables_compared, issue_table, issue_column, unique_identifer\n",
    "ERROR_LOG_M = []\n",
    "# Mapping of base table names to their key columns in master and target tables\n",
    "# BASE_TABLES = {\n",
    "#     'brand': {'master_key': 'code', 'target_key': 'code'},\n",
    "#     'brand_pm_mapping': {'master_key': 'pm_id', 'target_key': 'pm_id'},\n",
    "#     'brick': {'master_key': 'brick_code', 'target_key': 'code'},  # Different key columns\n",
    "#     'coe_bom_element_type_mapping': {'master_key': 'coe_name', 'target_key': 'coe_name'},\n",
    "#     'event_log': {'master_key': 'user_id', 'target_key': 'user_id'},\n",
    "#     'supplier': {'master_key': 'supplier_code', 'target_key': 'supplier_code'}\n",
    "# }\n",
    "# Mapping of base table names to their master key and target keys per prefix\n",
    "\n",
    "# Get the current datetime\n",
    "now = datetime.now()\n",
    "\n",
    "Non_imp_columns = {\n",
    "    'supplier': ['id', '_id', 'updated_at', 'created_at'],\n",
    "    'vendor_details': ['id', '_id', 'updated_at', 'created_at']  # Add if applicable\n",
    "}\n",
    "\n",
    "\n",
    "# Define Important Columns\n",
    "Imp_columns = {\n",
    "    'brand': ['name', 'id', 'slug', 'code'],  \n",
    "}\n",
    "\n",
    "\n",
    "BASE_TABLES = {\n",
    "    'brand': {\n",
    "        'master_key': 'code',\n",
    "        'targets': {\n",
    "            'procuro_': 'code',\n",
    "            'costing_engine_': 'code'\n",
    "        },\n",
    "        'active_filter': {\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True  # Default behavior\n",
    "    },\n",
    "    'brand_pm_mapping': {\n",
    "        'master_key': 'pm_id',\n",
    "        'targets': {\n",
    "            'costing_engine_': 'pm_id'\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'brick': {\n",
    "        'master_key': 'brick_code',\n",
    "        'targets': {\n",
    "            'costing_engine_': 'code'\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'coe_bom_element_type_mapping': {\n",
    "        'master_key': 'coe_name',\n",
    "        'targets': {\n",
    "            'costing_engine_': 'coe_name'\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'event_log': {\n",
    "        'master_key': 'user_id',\n",
    "        'targets': {\n",
    "            'costing_engine_': 'user_id'\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'supplier': {\n",
    "        'master_key': 'supplier_code',\n",
    "        'targets': {\n",
    "            'procuro_': 'supplier_code',\n",
    "            'costing_engine_': 'supplier_code'\n",
    "        },\n",
    "        'active_filter': {  # Apply active filter\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'vendor_details': {  # Newly added entry\n",
    "        'master_key': 'supplier_code',  # Using supplier_code as the key\n",
    "        'master_table': 'master_hub_supplier',  # Specify the master table explicitly\n",
    "        'targets': {\n",
    "            'scan_pack_': 'vendor_code'\n",
    "        },\n",
    "        'active_filter': {  # Apply active filter\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'hsn_tax_mapping': {  # Newly added base table for HSN Codes\n",
    "        'master_key': 'hsn_code',  # Assuming 'hsn_code' is the key column\n",
    "        'master_table': 'master_hub_hsn',\n",
    "        'targets': {\n",
    "            'procuro_': 'hsn_code',\n",
    "        },\n",
    "        'perform_checks': False  # Only perform key comparisons\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Slack configuration\n",
    "SLACK_TOKEN = \"xoxb-2151238541-7946286860052-5FCcfqBPem0xKigGlIcKdLgX\"\n",
    "SLACK_CHANNEL = \"C07UN19ETK5\"\n",
    "\n",
    "# Initialize Slack client\n",
    "if SLACK_TOKEN and SLACK_CHANNEL:\n",
    "    slack_client = WebClient(token=SLACK_TOKEN)\n",
    "    logging.info(\"Slack client initialized successfully.\")\n",
    "else:\n",
    "    slack_client = None\n",
    "    logging.warning(\"Slack token or channel not found. Slack notifications will be disabled.\")\n",
    "\n",
    "\n",
    "def get_bigquery_client(project_id):\n",
    "    \"\"\"\n",
    "    Initialize and return a BigQuery client.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID.\n",
    "\n",
    "    Returns:\n",
    "        bigquery.Client: An initialized BigQuery client.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        logging.info(\"BigQuery client initialized successfully.\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize BigQuery client: {e}\")\n",
    "        raise\n",
    "\n",
    "def find_common_tables_with_master_hub(client, dataset_name, prefixes, base_tables):\n",
    "    \"\"\"\n",
    "    Find tables in the specified dataset that share the same base name after removing the 'master_hub_' prefix\n",
    "    and exist with other given prefixes.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset to search within.\n",
    "        prefixes (list): List of prefixes to compare with 'master_hub_'.\n",
    "        base_tables (dict): The BASE_TABLES dictionary containing base table configurations.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are base names and values are dictionaries showing which prefixes have tables.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Reference the dataset\n",
    "        dataset_ref = client.dataset(dataset_name)\n",
    "        \n",
    "        # List all tables in the dataset\n",
    "        tables = client.list_tables(dataset_ref)\n",
    "        table_names = [table.table_id for table in tables]\n",
    "        logging.info(f\"Found {len(table_names)} tables in dataset '{dataset_name}'.\")\n",
    "        \n",
    "        # Dictionary to hold base names and their corresponding tables\n",
    "        common_tables = {}\n",
    "        for base_name, config in base_tables.items():\n",
    "            # Determine the master table\n",
    "            master_table = config.get('master_table', f'master_hub_{base_name}')\n",
    "            if master_table in table_names:\n",
    "                common_tables[base_name] = {'master_hub_': master_table}\n",
    "                # Check for target tables with specified prefixes\n",
    "                for prefix, target_key in config.get('targets', {}).items():\n",
    "                    target_table = f\"{prefix}{base_name}\"\n",
    "                    if target_table in table_names:\n",
    "                        common_tables[base_name][prefix] = target_table\n",
    "            else:\n",
    "                logging.warning(f\"Master table '{master_table}' for base '{base_name}' not found in dataset.\")\n",
    "        \n",
    "        # Filter out base names that only have 'master_hub_' but no other matching prefixes\n",
    "        common_tables_with_prefixes = {base_name: tables for base_name, tables in common_tables.items() if len(tables) > 1}\n",
    "        \n",
    "        logging.info(f\"Identified {len(common_tables_with_prefixes)} common base names with 'master_hub_' and other specified prefixes.\")\n",
    "        return common_tables_with_prefixes\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Google API Error: {e.message}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return {}\n",
    "\n",
    "def get_table_schema(client, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Retrieve the schema of a specified BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        table_name (str): The name of the table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping column names to their data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table_ref = client.dataset(dataset_name).table(table_name)\n",
    "        table = client.get_table(table_ref)\n",
    "        schema = {field.name: field.field_type for field in table.schema}\n",
    "        logging.info(f\"Retrieved schema for table '{table_name}'.\")\n",
    "        return schema\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Failed to retrieve schema for table '{table_name}': {e.message}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while retrieving schema for table '{table_name}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def load_table_from_bigquery(client, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Load a table from BigQuery into a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        table_name (str): The name of the table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the table data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f\"SELECT * FROM `{PROJECT_ID}.{dataset_name}.{table_name}`\"\n",
    "        df = client.query(query).to_dataframe()\n",
    "        logging.info(f\"Loaded data from table '{table_name}' into DataFrame.\")\n",
    "        return df\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Failed to load table '{table_name}': {e.message}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while loading table '{table_name}': {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# def standardize_dataframe(df):\n",
    "#     \"\"\"\n",
    "#     Standardize string columns in the DataFrame by stripping whitespace and converting to lowercase.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The DataFrame to standardize.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Standardized DataFrame.\n",
    "#     \"\"\"\n",
    "#     df_copy = df.copy()\n",
    "#     for col in df_copy.columns:\n",
    "#         if pd.api.types.is_string_dtype(df_copy[col]):\n",
    "#             df_copy[col] = df_copy[col].astype(str).str.strip().str.lower()\n",
    "#     logging.info(\"Standardized DataFrame for comparison.\")\n",
    "#     return df_copy\n",
    "\n",
    "def standardize_dataframe(df, exclude_columns=[]):\n",
    "    \"\"\"\n",
    "    Standardize string columns in the DataFrame by stripping whitespace and converting to lowercase,\n",
    "    excluding specified columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to standardize.\n",
    "        exclude_columns (list): Columns to exclude from standardization.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Standardized DataFrame.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in df_copy.columns:\n",
    "        if col in exclude_columns:\n",
    "            continue  # Skip standardizing this column\n",
    "        if pd.api.types.is_string_dtype(df_copy[col]):\n",
    "            df_copy[col] = df_copy[col].astype(str).str.strip().str.lower()\n",
    "    logging.info(\"Standardized DataFrame for comparison.\")\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def find_common_and_non_common_columns(df1, df2):\n",
    "    \"\"\"\n",
    "    Identify common and unique columns between two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df1 (pd.DataFrame): First DataFrame.\n",
    "        df2 (pd.DataFrame): Second DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (common_columns, df1_unique_columns, df2_unique_columns)\n",
    "    \"\"\"\n",
    "    common_columns = list(set(df1.columns).intersection(set(df2.columns)))\n",
    "    df1_unique_columns = list(set(df1.columns) - set(df2.columns))\n",
    "    df2_unique_columns = list(set(df2.columns) - set(df1.columns))\n",
    "    logging.info(f\"Found {len(common_columns)} common columns, {len(df1_unique_columns)} unique to first table, {len(df2_unique_columns)} unique to second table.\")\n",
    "    return common_columns, df1_unique_columns, df2_unique_columns\n",
    "\n",
    "def find_mismatches(df_master, df_target, common_columns, master_key, target_key, table1, table2, duplicates_master, duplicates_target, non_imp_columns):\n",
    "    \"\"\"\n",
    "    Identify mismatches between two DataFrames based on common columns and key columns.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "        df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "        common_columns (list): List of common columns to compare.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        table1 (str): Name of the source table.\n",
    "        table2 (str): Name of the target table.\n",
    "        duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "        duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing mismatch details.\n",
    "    \"\"\"\n",
    "    mismatches = []\n",
    "    error_logs_m = []\n",
    "    # Ensure key columns are present in both DataFrames\n",
    "    if master_key not in df_master.columns or target_key not in df_target.columns:\n",
    "        logging.error(f\"Key columns '{master_key}' or '{target_key}' not found in the respective tables.\")\n",
    "        return mismatches\n",
    "\n",
    "    # Rename target key to match master key for easier comparison\n",
    "    df_target_renamed = df_target.rename(columns={target_key: master_key})\n",
    "\n",
    "    # Merge DataFrames on the master_key\n",
    "    merged_df = pd.merge(\n",
    "        df_master.drop_duplicates(subset=master_key),\n",
    "        df_target_renamed.drop_duplicates(subset=master_key),\n",
    "        on=master_key,\n",
    "        suffixes=(f'_{table1}', f'_{table2}'),\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Merged DataFrame has {len(merged_df)} records for mismatch comparison.\")\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        key = row[master_key]\n",
    "        for column in common_columns:\n",
    "            if column.startswith('_boltic_') or column in non_imp_columns:\n",
    "                continue  # Skip columns starting with '_boltic_'\n",
    "            val_master = row.get(f\"{column}_{table1}\")\n",
    "            val_target = row.get(f\"{column}_{table2}\")\n",
    "            # Handle NaN values in comparison\n",
    "            if pd.isna(val_master) and pd.isna(val_target):\n",
    "                continue  # Both are NaN, treat as equal\n",
    "            elif pd.isna(val_master) or pd.isna(val_target) or val_master != val_target:\n",
    "                mismatch_detail = {\n",
    "                    master_key: key,\n",
    "                    'column': column,\n",
    "                    f'{table1}_value': val_master,\n",
    "                    f'{table2}_value': val_target\n",
    "                }\n",
    "                mismatches.append(mismatch_detail)\n",
    "                error_detail = {\n",
    "                    'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'mismatch',\n",
    "                    'error_message': '',\n",
    "                    'source_table': f'{table1}',\n",
    "                    'target_table': f'{table2}',\n",
    "                    'issue_column': column,\n",
    "                    'unique_identifier': f'{master_key}: {key}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "    \n",
    "\n",
    "    logging.info(f\"Found {len(mismatches)} mismatches between '{table1}' and '{table2}'.\")\n",
    "    return mismatches, error_logs_m\n",
    "\n",
    "def find_duplicates(df, key_column, table_name):\n",
    "    \"\"\"\n",
    "    Detect duplicate key_column entries in the DataFrame and identify differences.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to check.\n",
    "        key_column (str): The key column to check for duplicates.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing duplicated key_column values with differences.\n",
    "    \"\"\"\n",
    "    if key_column not in df.columns:\n",
    "        logging.error(f\"Key column '{key_column}' not found in DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get all duplicate entries (keep=False to get all duplicates)\n",
    "    duplicates_df = df[df.duplicated(subset=key_column, keep=False)]\n",
    "    \n",
    "    # Group by key_column\n",
    "    grouped = duplicates_df.groupby(key_column)\n",
    "    \n",
    "    duplicate_records = []\n",
    "\n",
    "    error_logs_m = []\n",
    "\n",
    "    for key, group in grouped:\n",
    "        if len(group) <= 1:\n",
    "            continue  # Not a duplicate\n",
    "        \n",
    "        # Drop key_column and any columns starting with '_boltic_'\n",
    "        group_non_key = group.drop(columns=[key_column] + [col for col in group.columns if col.startswith('_boltic_')])\n",
    "        \n",
    "        # Check if all rows are identical\n",
    "        if group_non_key.nunique().sum() == 0:\n",
    "            difference = \"No difference exists\"\n",
    "        else:\n",
    "            # Find which columns have differences\n",
    "            cols_with_diff = group_non_key.columns[group_non_key.nunique() > 1].tolist()\n",
    "            # difference = ', '.join(cols_with_diff)\n",
    "            difference = \"Difference in value of columns: \" + ', '.join(cols_with_diff)\n",
    "        \n",
    "        duplicate_records.append({\n",
    "            key_column: key,\n",
    "            'Difference in value': difference\n",
    "        })\n",
    "        error_detail = {\n",
    "            'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'duplicate',\n",
    "            'error_message': f'{difference}',\n",
    "            'source_table': f'{table_name}',\n",
    "            'target_table': '',\n",
    "            'issue_column': '',\n",
    "            'unique_identifier': f'{key_column}: {key}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "    \n",
    "    logging.info(f\"Found {len(duplicate_records)} duplicate entries based on '{key_column}'.\")\n",
    "    return pd.DataFrame(duplicate_records), error_logs_m\n",
    "\n",
    "def validate_data_types(schema_master, schema_target, master_key, table1_name, table2_name):\n",
    "    \"\"\"\n",
    "    Compare data types of common columns between master and target schemas.\n",
    "\n",
    "    Args:\n",
    "        schema_master (dict): Schema of the master table.\n",
    "        schema_target (dict): Schema of the target table.\n",
    "        master_key (str): The key column for reference.\n",
    "        table1_name (str): Name of the first table.\n",
    "        table2_name (str): Name of the second table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing data type discrepancies with table names in headers.\n",
    "    \"\"\"\n",
    "    data_type_issues = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    # Identify common columns\n",
    "    common_columns = set(schema_master.keys()).intersection(set(schema_target.keys()))\n",
    "\n",
    "    for column in common_columns:\n",
    "        type_master = schema_master[column]\n",
    "        type_target = schema_target[column]\n",
    "        if type_master != type_target:\n",
    "            data_type_issues.append({\n",
    "                'column_name': column,\n",
    "                f'{table1_name}_data_type': type_master,\n",
    "                f'{table2_name}_data_type': type_target\n",
    "            })\n",
    "            error_detail = {\n",
    "            'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'data_type_issues',\n",
    "            'error_message': f'{table1_name}_data_type: {type_master} , {table2_name}_data_type: {type_target}',\n",
    "            'source_table': f'{table1_name}',\n",
    "            'target_table': f'{table2_name}',\n",
    "            'issue_column': column,\n",
    "            'unique_identifier': ''\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(data_type_issues)} data type issues.\")\n",
    "    return pd.DataFrame(data_type_issues), error_logs_m\n",
    "\n",
    "def validate_formats(df_master, df_target, key_column, target_key, target_table, master_table):\n",
    "    \"\"\"\n",
    "    Validate specific column formats using regular expressions and include corresponding target table values.\n",
    "    \n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing format issues with corresponding target table values.\n",
    "    \"\"\"\n",
    "    format_issues = pd.DataFrame(columns=[key_column, 'column', 'value', 'issue', f'{target_table}_value'])\n",
    "    error_logs_m = []\n",
    "    \n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "        \n",
    "        # GSTIN format validation\n",
    "        if 'gstin' in df_master.columns:\n",
    "            gstin = str(row['gstin']).strip()\n",
    "            if not re.match(r'^[0-9]{2}[A-Z]{5}[0-9]{4}[A-Z]{1}[A-Z0-9]{3}$', gstin):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['gstin'] if 'gstin' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                \n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'gstin',\n",
    "                    'value': row['gstin'],\n",
    "                    'issue': 'Invalid GSTIN format',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'format_issue',\n",
    "                'error_message': 'Invalid GSTIN format',\n",
    "                'source_table': f'{master_table}',\n",
    "                'target_table': '',\n",
    "                'issue_column': 'gstin',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "        \n",
    "        # Email format validation\n",
    "        if 'email' in df_master.columns:\n",
    "            email = str(row['email']).strip()\n",
    "            if not re.match(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', email):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['email'] if 'email' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                \n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'email',\n",
    "                    'value': row['email'],\n",
    "                    'issue': 'Invalid email format',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'format_issue',\n",
    "                'error_message': 'Invalid email format',\n",
    "                'source_table': f'{master_table}',\n",
    "                'target_table': '',\n",
    "                'issue_column': 'email',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "        \n",
    "        # Pincode format validation\n",
    "        if 'pincode' in df_master.columns:\n",
    "            pincode = str(row['pincode']).strip()\n",
    "            if not re.match(r'^\\d{6}$', pincode):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['pincode'] if 'pincode' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                \n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'pincode',\n",
    "                    'value': row['pincode'],\n",
    "                    'issue': 'Pincode must be exactly 6 digits',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'format_issue',\n",
    "                'error_message': 'Pincode must be exactly 6 digits',\n",
    "                'source_table': f'{master_table}',\n",
    "                'target_table': '',\n",
    "                'issue_column': 'pincode',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "    \n",
    "    logging.info(f\"Found {len(format_issues)} format issues.\")\n",
    "    return format_issues, error_logs_m\n",
    "\n",
    "\n",
    "def create_table(doc, data, column_names):\n",
    "    \"\"\"\n",
    "    Helper function to create a table in a docx document from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        data (list or list of dict): Data to populate the table.\n",
    "        column_names (list): List of column names for the table headers.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    table = doc.add_table(rows=1, cols=len(column_names))\n",
    "    table.style = 'Light List Accent 1'\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for i, col_name in enumerate(column_names):\n",
    "        hdr_cells[i].text = col_name\n",
    "\n",
    "    for row_data in data:\n",
    "        row_cells = table.add_row().cells\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            cell_value = str(row_data.get(col_name, '')).strip()\n",
    "            row_cells[i].text = cell_value\n",
    "    logging.info(\"Added table to the Word document.\")\n",
    "\n",
    "def add_non_matching_keys_section(doc, df1_only_keys, table1_name, df2_only_keys, table2_name, key_column_master, key_column_target):\n",
    "    \"\"\"\n",
    "    Add a section in the Word document for non-matching keys between two tables.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        df1_only_keys (list): Keys present only in table1.\n",
    "        table1_name (str): Name of the first table.\n",
    "        df2_only_keys (list): Keys present only in table2.\n",
    "        table2_name (str): Name of the second table.\n",
    "        key_column_master (str): The key column in the master table.\n",
    "        key_column_target (str): The key column in the target table.\n",
    "    \"\"\"\n",
    "    if df1_only_keys or df2_only_keys:\n",
    "        if df1_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_master}' present only in '{table1_name}' and not in '{table2_name}' ({len(df1_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_master: key.strip()} for key in df1_only_keys], [key_column_master])\n",
    "        if df2_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_target}' present only in '{table2_name}' and not in '{table1_name}' ({len(df2_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_target: key.strip()} for key in df2_only_keys], [key_column_target])\n",
    "    else:\n",
    "        doc.add_paragraph(\"No non-matching keys found.\")\n",
    "\n",
    "def add_table_of_contents(doc):\n",
    "    \"\"\"\n",
    "    Adds a Table of Contents to the Word document.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "    \"\"\"\n",
    "    paragraph = doc.add_paragraph()\n",
    "    run = paragraph.add_run()\n",
    "    fldChar_begin = OxmlElement('w:fldChar')  # creates a new element\n",
    "    fldChar_begin.set(qn('w:fldCharType'), 'begin')  # sets attribute on element\n",
    "    instrText = OxmlElement('w:instrText')\n",
    "    instrText.set(qn('xml:space'), 'preserve')  # sets attribute on element\n",
    "    instrText.text = 'TOC \\\\o \"1-2\" \\\\h \\\\z \\\\u'  # change to what you need\n",
    "    fldChar_separate = OxmlElement('w:fldChar')\n",
    "    fldChar_separate.set(qn('w:fldCharType'), 'separate')\n",
    "    fldChar_end = OxmlElement('w:fldChar')\n",
    "    fldChar_end.set(qn('w:fldCharType'), 'end')\n",
    "    run._r.append(fldChar_begin)\n",
    "    run._r.append(instrText)\n",
    "    run._r.append(fldChar_separate)\n",
    "    run._r.append(fldChar_end)\n",
    "    logging.info(\"Added Table of Contents to the Word document.\")\n",
    "\n",
    "def create_aggregated_document(all_results, base_name):\n",
    "    \"\"\"\n",
    "    Creates a single Word document that presents all comparison results for a base table.\n",
    "\n",
    "    Args:\n",
    "        all_results (list): List of comparison result dictionaries.\n",
    "        base_name (str): The base name of the table.\n",
    "\n",
    "    Returns:\n",
    "        str: The filepath of the saved report.\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "    doc.add_heading(f'{base_name.capitalize()} Tables Comparison Report', level=0)\n",
    "    doc.add_paragraph(f'Report generated on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "    \n",
    "    # Add Instruction for TOC Update\n",
    "    doc.add_paragraph(\n",
    "        \"📌 **Note:** To update the Table of Contents and make the links clickable, go to ‘Reference’ tab and click ‘Update_Table’ or press F9 in Windows and Fn+F9 in mac, after opening after opening this document in Microsoft Word.\",\n",
    "        style='Intense Quote'\n",
    "    )\n",
    "        \n",
    "    # Add Table of Contents\n",
    "    doc.add_heading('Table of Contents', level=1)\n",
    "    add_table_of_contents(doc)\n",
    "    doc.add_page_break()\n",
    "\n",
    "    for result in all_results:\n",
    "        table1_name = result['table1_name']\n",
    "        table2_name = result['table2_name']\n",
    "        key_column_master = result['key_column_master']\n",
    "        key_column_target = result['key_column_target']\n",
    "        doc.add_heading(f'Comparison: {table1_name} vs {table2_name}', level=1)\n",
    "\n",
    "        # Mismatches\n",
    "        if result['mismatches']:\n",
    "            doc.add_heading(f'Mismatches ({len(result[\"mismatches\"])})', level=2)\n",
    "            column_names = [key_column_master, 'column', f'{table1_name}_value', f'{table2_name}_value']\n",
    "            create_table(doc, result['mismatches'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No mismatches found.\", level=2)\n",
    "\n",
    "        # Null values in master table\n",
    "        if not result['null_values_master'].empty:\n",
    "            count_null_master = len(result['null_values_master'])\n",
    "            doc.add_heading(f'Null values in {table1_name} ({count_null_master})', level=2)\n",
    "            column_names = [key_column_master, 'column', table2_name]\n",
    "            create_table(doc, result['null_values_master'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table1_name}.\", level=2)\n",
    "\n",
    "        # Null values in target table\n",
    "        if not result['null_values_target'].empty:\n",
    "            count_null_target = len(result['null_values_target'])\n",
    "            doc.add_heading(f'Null values in {table2_name} ({count_null_target})', level=2)\n",
    "            column_names = [key_column_target, 'column', table1_name]\n",
    "            create_table(doc, result['null_values_target'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Duplicate keys in master table\n",
    "        if not result['duplicates_master'].empty:\n",
    "            count_dup_master = len(result['duplicates_master'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table1_name} ({count_dup_master})', level=2)\n",
    "            # Modified to include 'Difference in value' column\n",
    "            create_table(doc, result['duplicates_master'].to_dict('records'), [key_column_master, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(\"No duplicate keys found in master table.\", level=2)\n",
    "\n",
    "        # Duplicate keys in target table with actual table name\n",
    "        if not result['duplicates_target'].empty:\n",
    "            count_dup_target = len(result['duplicates_target'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table2_name} ({count_dup_target})', level=2)\n",
    "            # Modified to include 'Difference in value' column\n",
    "            create_table(doc, result['duplicates_target'].to_dict('records'), [key_column_target, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(f\"No duplicate keys found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Data type issues\n",
    "        if not result['data_type_issues'].empty:\n",
    "            count_data_type_issues = len(result['data_type_issues'])\n",
    "            doc.add_heading(f'Data Type Issues ({count_data_type_issues})', level=2)\n",
    "            column_names = ['column_name', f'{table1_name}_data_type', f'{table2_name}_data_type']\n",
    "            create_table(doc, result['data_type_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No data type issues found.\", level=2)\n",
    "\n",
    "        # Format issues in master table with target values\n",
    "        if not result['format_issues_master'].empty:\n",
    "            count_format_issues_master = len(result['format_issues_master'])\n",
    "            doc.add_heading(f'Format Issues in {table1_name} ({count_format_issues_master})', level=2)\n",
    "            column_names_master = [key_column_master, 'column', 'value', 'issue', f'{table2_name}_value']\n",
    "            create_table(doc, result['format_issues_master'].to_dict('records'), column_names_master)\n",
    "        else:\n",
    "            doc.add_heading(f\"No format issues found in {table1_name}.\", level=2)\n",
    "\n",
    "        # # Pincode Mapping Issues\n",
    "        # if not result['pincode_mapping_issues'].empty:\n",
    "        #     count_pincode_issues = len(result['pincode_mapping_issues'])\n",
    "        #     doc.add_heading(f'Pincode Mapping Issues in {table1_name} ({count_pincode_issues})', level=2)\n",
    "        #     column_names = [key_column_master, 'pincode', 'state', 'city', 'issue']\n",
    "        #     create_table(doc, result['pincode_mapping_issues'].to_dict('records'), column_names)\n",
    "        # else:\n",
    "        #     doc.add_heading(\"No pincode mapping issues found.\", level=2)\n",
    "\n",
    "        # Pincode Mapping Issues with target details\n",
    "        if not result['pincode_mapping_issues'].empty:\n",
    "            count_pincode_issues = len(result['pincode_mapping_issues'])\n",
    "            doc.add_heading(f'Pincode Mapping Issues in {table1_name} ({count_pincode_issues})', level=2)\n",
    "            column_names = [\n",
    "                key_column_master, 'pincode', 'state', 'city', 'issue',\n",
    "                f'{table2_name}_details'\n",
    "            ]\n",
    "            create_table(doc, result['pincode_mapping_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No pincode mapping issues found.\", level=2)\n",
    "\n",
    "        \n",
    "\n",
    "        # Non-matching keys in master DataFrame\n",
    "        if result['df_master_only_keys']:\n",
    "            count_master_only = len(result['df_master_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table1_name} ({count_master_only})', level=2)\n",
    "            column_names = [key_column_master]\n",
    "            create_table(doc, result['df_master_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table1_name}.\", level=2)\n",
    "\n",
    "        # Non-matching keys in target DataFrame\n",
    "        if result['df_target_only_keys']:\n",
    "            count_target_only = len(result['df_target_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table2_name} ({count_target_only})', level=2)\n",
    "            column_names = [key_column_target]\n",
    "            create_table(doc, result['df_target_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table2_name}.\", level=2)\n",
    "\n",
    "\n",
    "        doc.add_page_break()  # Optional: Add a page break between comparisons\n",
    "\n",
    "    # Save the aggregated document to the current directory\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_filename = f\"{base_name}_comparison_report_aggregated_{timestamp}.docx\"\n",
    "    doc.save(report_filename)\n",
    "    logging.info(f\"Saved aggregated comparison report as '{report_filename}'.\")\n",
    "\n",
    "    return report_filename  # Return the filename for further processing\n",
    "\n",
    "\n",
    "\n",
    "def send_slack_alert(message):\n",
    "    \"\"\"\n",
    "    Send a message to a specified Slack channel.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to send.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping Slack notification.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = slack_client.chat_postMessage(\n",
    "            channel=SLACK_CHANNEL,\n",
    "            text=message\n",
    "        )\n",
    "        logging.info(f\"Message sent to {SLACK_CHANNEL}: {response['ts']}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Error sending message to Slack: {e.response['error']}\")\n",
    "\n",
    "def upload_file_to_slack(filepath, title=None):\n",
    "    \"\"\"\n",
    "    Upload a file to the specified Slack channel using files_upload_v2.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the file to upload.\n",
    "        title (str, optional): The title for the uploaded file. Defaults to the file's basename.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping file upload.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            response = slack_client.files_upload_v2(\n",
    "                channel=SLACK_CHANNEL,\n",
    "                file=f,\n",
    "                filename=os.path.basename(filepath),  # Explicitly set the filename with extension\n",
    "                title=title if title else os.path.basename(filepath),  # Set the title\n",
    "                initial_comment=title if title else \"File uploaded.\"  # Optional: Add an initial comment\n",
    "            )\n",
    "\n",
    "        # Verify if the upload was successful\n",
    "        if response.get('ok'):\n",
    "            file_permalink = response['file']['permalink']\n",
    "            logging.info(f\"File uploaded to Slack channel '{SLACK_CHANNEL}': {file_permalink}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to upload file to Slack: {response}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Slack API Error during file upload: {e.response['error']}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during file upload: {e}\")\n",
    "\n",
    "\n",
    "def find_non_matching_keys(df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table):\n",
    "    \"\"\"\n",
    "    Identify keys present in df_master but not in df_target and vice versa, including duplicates.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame.\n",
    "        df_target (pd.DataFrame): Target DataFrame.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "        duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (master_only_keys, target_only_keys)\n",
    "            - master_only_keys (list of dict): Keys present only in master DataFrame.\n",
    "            - target_only_keys (list of dict): Keys present only in target DataFrame.\n",
    "    \"\"\"\n",
    "    error_logs_m = []\n",
    "    # Include all keys, including duplicates\n",
    "    keys_master = set(df_master[master_key].astype(str).str.strip())\n",
    "    keys_target = set(df_target[target_key].astype(str).str.strip())\n",
    "\n",
    "    # Keys present only in master\n",
    "    master_only = keys_master - keys_target\n",
    "    # Keys present only in target\n",
    "    target_only = keys_target - keys_master\n",
    "\n",
    "    logging.info(f\"Found {len(master_only)} keys in source not in target and {len(target_only)} keys in target not in source.\")\n",
    "\n",
    "    # Convert to list of dictionaries for consistency\n",
    "    master_only_keys = [{master_key: key} for key in master_only]\n",
    "    target_only_keys = [{target_key: key} for key in target_only]\n",
    "        # Log errors for keys only in master\n",
    "    for key in master_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{master_key}' with value '{key}' is present only in '{master_table}' and missing in '{target_table}'.\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table, \n",
    "            'issue_column': master_key,\n",
    "            'unique_identifier': f\"{master_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    # Log errors for keys only in target\n",
    "    for key in target_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{target_key}' with value '{key}' is present only in '{target_table}' and missing in '{master_table}'.\",\n",
    "            'source_table': target_table,\n",
    "            'target_table': master_table,\n",
    "            'issue_column': target_key,\n",
    "            'unique_identifier': f\"{target_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "\n",
    "    return master_only_keys, target_only_keys, error_logs_m\n",
    "\n",
    "\n",
    "\n",
    "def find_detailed_nulls(df_master, df_target, master_key, target_key, master_table, target_table):\n",
    "    \"\"\"\n",
    "    Identify null values in both master and target tables and fetch corresponding values or indicate missing keys.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "        df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        master_table (str): Name of the master table.\n",
    "        target_table (str): Name of the target table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (null_values_master, null_values_target)\n",
    "    \"\"\"\n",
    "    null_values_master = []\n",
    "    null_values_target = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    # Find nulls in master\n",
    "    null_master = df_master[df_master.isnull().any(axis=1)]\n",
    "    for idx, row in null_master.iterrows():\n",
    "        key_value = str(row[master_key]).strip()\n",
    "        for column in df_master.columns:\n",
    "            if column == master_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and columns starting with '_boltic_'\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row[column] if column in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                null_record = {\n",
    "                    master_key: key_value,\n",
    "                    'column': column,\n",
    "                    target_table: target_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'null',\n",
    "                'error_message': 'Null in columns',\n",
    "                'source_table': f'{master_table}',\n",
    "                'target_table': '',\n",
    "                'issue_column': column,\n",
    "                'unique_identifier': f'{master_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_master.append(null_record)\n",
    "                \n",
    "\n",
    "    # Find nulls in target\n",
    "    null_target = df_target[df_target.isnull().any(axis=1)]\n",
    "    for idx, row in null_target.iterrows():\n",
    "        key_value = str(row[target_key]).strip()\n",
    "        for column in df_target.columns:\n",
    "            if column == target_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and columns starting with '_boltic_'\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_master[master_key].astype(str).str.strip().values:\n",
    "                    master_row = df_master[df_master[master_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    master_value = master_row[column] if column in master_row else \"Column not present\"\n",
    "                else:\n",
    "                    master_value = f\"'{master_key}' not present\"\n",
    "                null_record = {\n",
    "                    target_key: key_value,\n",
    "                    'column': column,\n",
    "                    master_table: master_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'null',\n",
    "                'error_message': 'Null in columns',\n",
    "                'source_table': f'{target_table}',\n",
    "                'target_table': '',\n",
    "                'issue_column': column,\n",
    "                'unique_identifier': f'{target_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_target.append(null_record)\n",
    "                \n",
    "\n",
    "    logging.info(f\"Found {len(null_values_master)} null values in master table '{master_table}'.\")\n",
    "    logging.info(f\"Found {len(null_values_target)} null values in target table '{target_table}'.\")\n",
    "    return null_values_master, null_values_target, error_logs_m\n",
    "\n",
    "\n",
    "def validate_pincode_mapping(df_master, df_target, key_column, target_key, target_table, client, master_table):\n",
    "    \"\"\"\n",
    "    Validate pincode mapping by comparing with the all_india_PO_list reference table.\n",
    "    If a pincode issue is found in the master table, then check the corresponding pincode in the target table.\n",
    "    \n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        master_table (str): Name of the master table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing pincode mapping issues with corresponding target table details.\n",
    "        list: List of error log dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    error_logs_m = []\n",
    "    # Read the reference table from Analytics dataset\n",
    "    try:\n",
    "        reference_table = \"all_india_PO_list\"\n",
    "        reference_dataset = \"analytics_data\"\n",
    "        query = f\"SELECT pincode, city, state FROM `{PROJECT_ID}.{reference_dataset}.{reference_table}`\"\n",
    "        reference_df = client.query(query).to_dataframe()\n",
    "        reference_df['pincode'] = reference_df['pincode'].astype(str).str.strip()\n",
    "        reference_df['city'] = reference_df['city'].astype(str).str.strip().str.lower()\n",
    "        reference_df['state'] = reference_df['state'].astype(str).str.strip().str.lower()\n",
    "        logging.info(f\"Loaded reference pincode mapping from '{reference_table}' in '{reference_dataset}' dataset.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load reference pincode mapping: {e}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "    \n",
    "    # Check if df_master has 'pincode', 'city', 'state' columns\n",
    "    required_columns = {'pincode', 'city', 'state'}\n",
    "    if not required_columns.issubset(df_master.columns):\n",
    "        logging.info(f\"DataFrame does not have required columns for pincode mapping validation: {required_columns}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "    \n",
    "    # Initialize the issues DataFrame with a single target table details column\n",
    "    pincode_mapping_issues = pd.DataFrame(columns=[\n",
    "        key_column, 'pincode', 'state', 'city', 'issue',\n",
    "        f'{target_table}_details'\n",
    "    ])\n",
    "    \n",
    "    # Iterate over each row in df_master to validate pincode mapping\n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "        pincode = str(row['pincode']).strip()\n",
    "        city = str(row['city']).strip().lower()\n",
    "        state = str(row['state']).strip().lower()\n",
    "        \n",
    "        # Fetch corresponding target row if exists\n",
    "        target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value]\n",
    "        if not target_row.empty:\n",
    "            target_row = target_row.iloc[0]\n",
    "            target_pincode = target_row['pincode'] if 'pincode' in target_row and pd.notnull(target_row['pincode']) else \"Pincode missing\"\n",
    "            target_state = target_row['state'] if 'state' in target_row and pd.notnull(target_row['state']) else \"State missing\"\n",
    "            target_city = target_row['city'] if 'city' in target_row and pd.notnull(target_row['city']) else \"City missing\"\n",
    "            target_details = f\"Pincode: {target_pincode}, State: {target_state}, City: {target_city}\"\n",
    "        else:\n",
    "            target_details = f\"Key '{key_column}' with value '{key_value}' not present in target table '{target_table}'.\"\n",
    "        \n",
    "        # Check if pincode exists in reference\n",
    "        ref_matches = reference_df[reference_df['pincode'] == pincode]\n",
    "        if ref_matches.empty:\n",
    "            issue = f\"Invalid pincode ({pincode}).\"\n",
    "            pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "                key_column: key_value,\n",
    "                'pincode': pincode,\n",
    "                'state': state,\n",
    "                'city': city,\n",
    "                'issue': issue,\n",
    "                f'{target_table}_details': target_details\n",
    "            }])], ignore_index=True)\n",
    "            error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'pincode_mapping',\n",
    "                'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "                'source_table': master_table,\n",
    "                'target_table': target_table,\n",
    "                'issue_column': 'pincode',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "            continue\n",
    "        \n",
    "        # Check if any of the reference entries match both the city and state\n",
    "        exact_match = ref_matches[\n",
    "            (ref_matches['city'] == city) & (ref_matches['state'] == state)\n",
    "        ]\n",
    "        if not exact_match.empty:\n",
    "            continue  # No issue, mapping is correct\n",
    "        \n",
    "        # Check for state mismatch\n",
    "        state_matches = ref_matches[ref_matches['state'] == state]\n",
    "        \n",
    "        # Check for city mismatch\n",
    "        city_matches = ref_matches[ref_matches['city'] == city]\n",
    "        \n",
    "        if state_matches.empty and city_matches.empty:\n",
    "            # Both state and city do not match\n",
    "            expected_entries = ref_matches[['state', 'city']].drop_duplicates()\n",
    "            expected_states = expected_entries['state'].tolist()\n",
    "            expected_cities = expected_entries['city'].tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}' and city '{city}'. Expected states: {expected_states_str}; Expected cities: {expected_cities_str}.\"\n",
    "        elif state_matches.empty:\n",
    "            # State does not match\n",
    "            expected_states = ref_matches['state'].unique().tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}'. Expected states: {expected_states_str}.\"\n",
    "        elif city_matches.empty:\n",
    "            # City does not match\n",
    "            expected_cities = state_matches['city'].unique().tolist()\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match city '{city}'. Expected cities: {expected_cities_str}.\"\n",
    "        else:\n",
    "            # Other cases\n",
    "            issue = f\"Pincode {pincode} has a mapping inconsistency.\"\n",
    "        \n",
    "        pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "            key_column: key_value,\n",
    "            'pincode': pincode,\n",
    "            'state': state,\n",
    "            'city': city,\n",
    "            'issue': issue,\n",
    "            f'{target_table}_details': target_details\n",
    "        }])], ignore_index=True)\n",
    "        error_detail = {\n",
    "            'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'pincode_mapping',\n",
    "            'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table,\n",
    "            'issue_column': 'pincode',\n",
    "            'unique_identifier': f'{key_column}: {key_value}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "    \n",
    "    logging.info(f\"Found {len(pincode_mapping_issues)} pincode mapping issues in master table '{master_table}'.\")\n",
    "    return pincode_mapping_issues, error_logs_m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compare_tables(client, dataset_name, base_name, master_table, target_table, master_key, target_key):\n",
    "    \"\"\"\n",
    "    Compare two tables and generate a report.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        base_name (str): The base name of the table.\n",
    "        master_table (str): Name of the master_hub_ table.\n",
    "        target_table (str): Name of the target prefixed table.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing all comparison results.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting comparison for base table '{base_name}': '{master_table}' vs '{target_table}'.\")\n",
    "\n",
    "    # Initialize comparison results\n",
    "    mismatches = []\n",
    "    null_values_master = []\n",
    "    null_values_target = []\n",
    "    data_type_issues = pd.DataFrame()\n",
    "    format_issues_master = pd.DataFrame()\n",
    "    pincode_mapping_issues = pd.DataFrame()\n",
    "    duplicates_master = pd.DataFrame()\n",
    "    duplicates_target = pd.DataFrame()\n",
    "    master_only_keys = []\n",
    "    target_only_keys = []\n",
    "\n",
    "    # Load data\n",
    "    df_master = load_table_from_bigquery(client, dataset_name, master_table)\n",
    "    df_target = load_table_from_bigquery(client, dataset_name, target_table)\n",
    "\n",
    "    # **Apply standardization to handle case insensitivity**\n",
    "    df_master = standardize_dataframe(df_master, exclude_columns=[master_key])\n",
    "    df_target = standardize_dataframe(df_target, exclude_columns=[target_key])\n",
    "\n",
    "\n",
    "    # **Apply configurable active filter if defined**\n",
    "    base_table_info = BASE_TABLES.get(base_name, {})\n",
    "    active_filter = base_table_info.get('active_filter')\n",
    "    # Determine whether to perform additional checks\n",
    "    perform_checks = base_table_info.get('perform_checks', True)\n",
    "\n",
    "    if active_filter:\n",
    "        column = active_filter.get('column')\n",
    "        value = active_filter.get('value')\n",
    "        if column and column in df_master.columns:\n",
    "            initial_count = len(df_master)\n",
    "            df_master = df_master[df_master[column] == value]\n",
    "            filtered_count = len(df_master)\n",
    "            logging.info(f\"Filtered '{base_name}' master table: {initial_count - filtered_count} records excluded based on {column} = {value}.\")\n",
    "        else:\n",
    "            logging.warning(f\"Active filter specified but column '{column}' not found in master table '{master_table}'.\")\n",
    "\n",
    "\n",
    "    if df_master.empty or df_target.empty:\n",
    "        logging.warning(f\"One of the tables '{master_table}' or '{target_table}' is empty. Skipping comparison.\")\n",
    "        return None\n",
    "\n",
    "    # Identify BigNumeric columns in master and target tables\n",
    "    schema_master = get_table_schema(client, dataset_name, master_table)\n",
    "    schema_target = get_table_schema(client, dataset_name, target_table)\n",
    "    bignumeric_columns_master = [col for col, dtype in schema_master.items() if dtype == 'BIGNUMERIC']\n",
    "    bignumeric_columns_target = [col for col, dtype in schema_target.items() if dtype == 'BIGNUMERIC']\n",
    "\n",
    "    # Format BigNumeric columns in master table\n",
    "    for col in bignumeric_columns_master:\n",
    "        if col in df_master.columns:\n",
    "            df_master[col] = df_master[col].apply(lambda x: format(x, '.0f') if pd.notnull(x) else x)\n",
    "\n",
    "    # Format BigNumeric columns in target table\n",
    "    for col in bignumeric_columns_target:\n",
    "        if col in df_target.columns:\n",
    "            df_target[col] = df_target[col].apply(lambda x: format(x, '.0f') if pd.notnull(x) else x)\n",
    "    \n",
    "    if perform_checks:\n",
    "        # Find duplicates in both tables\n",
    "        duplicates_master, error_logs_m = find_duplicates(df_master, master_key, master_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "        duplicates_target,  error_logs_m = find_duplicates(df_target, target_key, target_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    if not duplicates_master.empty:\n",
    "        logging.warning(f\"Duplicate keys found in source table '{master_table}'. These will be reported but not used in mismatch comparison.\")\n",
    "    if not duplicates_target.empty:\n",
    "        logging.warning(f\"Duplicate keys found in target table '{target_table}'. These will be reported but not used in mismatch comparison.\")\n",
    "\n",
    "    # Identify common columns\n",
    "    common_columns, master_unique_cols, target_unique_cols = find_common_and_non_common_columns(df_master, df_target)\n",
    "\n",
    "    if not common_columns:\n",
    "        logging.warning(f\"No common columns found between '{master_table}' and '{target_table}'. Skipping comparison.\")\n",
    "        return None\n",
    "    \n",
    "    # Retrieve non-important columns for the current base table\n",
    "    non_imp_columns = Non_imp_columns.get(base_name, [])\n",
    "\n",
    "    # print(non_imp_columns)\n",
    "    \n",
    "    # Perform mismatch comparison if allowed\n",
    "    if perform_checks:\n",
    "    # Find mismatches excluding duplicate keys\n",
    "        mismatches, error_logs_m = find_mismatches(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            common_columns,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            duplicates_master,\n",
    "            duplicates_target,\n",
    "            non_imp_columns\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "    # Find detailed null values in both tables if allowed\n",
    "    if perform_checks:    \n",
    "        # Find detailed null values in both tables\n",
    "        null_values_master, null_values_target, error_logs_m = find_detailed_nulls(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            master_table,\n",
    "            target_table\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    if perform_checks:    \n",
    "        # Validate data types between master and target schemas\n",
    "        data_type_issues, error_logs_m = validate_data_types(schema_master, schema_target, master_key, master_table, target_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    if perform_checks:        \n",
    "        # Validate formats in master table only and include target values\n",
    "        format_issues_master, error_logs_m = validate_formats(df_master, df_target, master_key, target_key, target_table, master_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "        \n",
    "        # # Validate pincode mapping if applicable\n",
    "        # pincode_mapping_issues = pd.DataFrame()\n",
    "        # if {'pincode', 'city', 'state'}.issubset(df_master.columns):\n",
    "        #     pincode_mapping_issues = validate_pincode_mapping(df_master, master_key, client)\n",
    "    \n",
    "        # Validate pincode mapping if applicable and include target values\n",
    "\n",
    "    if perform_checks:        \n",
    "        pincode_mapping_issues = pd.DataFrame()\n",
    "        if {'pincode', 'city', 'state'}.issubset(df_master.columns):\n",
    "            pincode_mapping_issues, error_logs_m = validate_pincode_mapping(\n",
    "                df_master, \n",
    "                df_target, \n",
    "                master_key, \n",
    "                target_key, \n",
    "                target_table, \n",
    "                client, master_table\n",
    "            )\n",
    "            ERROR_LOG_M.extend(error_logs_m)\n",
    "    \n",
    "        # if {'pincode', 'city', 'state'}.issubset(df_target.columns):\n",
    "        # # **Corrected Call: Swap DataFrames and Keys**\n",
    "        #     pincode_mapping_issues_target, error_logs_m = validate_pincode_mapping(\n",
    "        #         df_target,       # df_master becomes target DataFrame\n",
    "        #         df_master,       # df_target becomes master DataFrame\n",
    "        #         target_key,      # key_column is target_key\n",
    "        #         master_key,      # target_key is master_key\n",
    "        #         master_table,    # target_table is master_table\n",
    "        #         client, \n",
    "        #         target_table\n",
    "        #     )\n",
    "        #     pincode_mapping_issues = pd.concat([pincode_mapping_issues, pincode_mapping_issues_target], ignore_index=True)\n",
    "        #     ERROR_LOG_M.extend(error_logs_m)\n",
    "    \n",
    "           \n",
    "    # Find non-matching keys\n",
    "    master_only_keys, target_only_keys, error_logs_m = find_non_matching_keys(\n",
    "        df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table\n",
    "    )\n",
    "    ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        'mismatches': mismatches,\n",
    "        'null_values_master': pd.DataFrame(null_values_master),\n",
    "        'null_values_target': pd.DataFrame(null_values_target),\n",
    "        'duplicates_master': duplicates_master,\n",
    "        'duplicates_target': duplicates_target,\n",
    "        'data_type_issues': data_type_issues,\n",
    "        'format_issues_master': format_issues_master,\n",
    "        'pincode_mapping_issues': pincode_mapping_issues,\n",
    "        'key_column_master': master_key,\n",
    "        'key_column_target': target_key,\n",
    "        'df_master_only_keys': master_only_keys,\n",
    "        'df_target_only_keys': target_only_keys,\n",
    "        'table1_name': master_table,\n",
    "        'table2_name': target_table,\n",
    "        'df_master': df_master,\n",
    "        'df_target': df_target\n",
    "    }\n",
    "\n",
    "    logging.info(f\"Completed comparison for '{master_table}' vs '{target_table}'.\")\n",
    "    return results\n",
    "\n",
    "def generate_string_schema(df):\n",
    "    \"\"\"\n",
    "    Generates a BigQuery schema with all fields as STRING.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame for which to generate the schema.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of SchemaField objects with type STRING.\n",
    "    \"\"\"\n",
    "    schema = [SchemaField(column, \"STRING\", mode=\"NULLABLE\") for column in df.columns]\n",
    "    return schema\n",
    "\n",
    "\n",
    "def _upload_dataframe_to_bigquery(client, analytics_dataset, table_name, df):\n",
    "    \"\"\"\n",
    "    Helper function to upload a DataFrame to BigQuery.\n",
    "    \n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        analytics_dataset (str): The name of the Analytics dataset.\n",
    "        table_name (str): The name of the table to upload.\n",
    "        df (pd.DataFrame): The DataFrame to upload.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logging.info(f\"No data to upload for '{table_name}'. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    # Convert all columns to string type\n",
    "    df = df.astype(str)\n",
    "    \n",
    "    # Generate BigQuery schema with all fields as STRING\n",
    "    schema = generate_string_schema(df)\n",
    "    \n",
    "    # Ensure table name doesn't exceed BigQuery's maximum length (1,024 characters)\n",
    "    if len(table_name) > 1024:\n",
    "        original_table_name = table_name\n",
    "        table_name = table_name[:1021] + '...'\n",
    "        logging.warning(f\"Table name truncated from '{original_table_name}' to '{table_name}' due to length constraints.\")\n",
    "    \n",
    "    # Define the full table ID\n",
    "    table_id = f\"{client.project}.{analytics_dataset}.{table_name}\"\n",
    "    \n",
    "    # Upload the DataFrame to BigQuery\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(\n",
    "            df,\n",
    "            table_id,\n",
    "            job_config=bigquery.LoadJobConfig(\n",
    "                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "                schema=schema  # Using the provided schema with all fields as STRING\n",
    "            )\n",
    "        )\n",
    "        job.result()  # Wait for the job to complete\n",
    "        logging.info(f\"Successfully uploaded '{table_id}' with {len(df)} records.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to upload '{table_id}' to BigQuery: {e}\")\n",
    "\n",
    "def upload_comparison_results_to_bigquery(client, analytics_dataset, ERROR_LOG_M):\n",
    "    \"\"\"\n",
    "    Uploads each part of the comparison_result to BigQuery as separate tables in the Analytics dataset.\n",
    "    The table names follow the format: 'table1_name_vs_table2_name_heading'.\n",
    "    \n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        analytics_dataset (str): The name of the Analytics dataset.\n",
    "        table1_name (str): Name of the first table (e.g., 'master_hub_brand').\n",
    "        table2_name (str): Name of the second table (e.g., 'procuro_brand').\n",
    "        comparison_result (dict): The dictionary containing comparison results.\n",
    "        ERROR_LOG: The error log data, either a DataFrame or a list.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle ERROR_LOG separately\n",
    "    if ERROR_LOG_M is not None:\n",
    "        # Determine the DataFrame to upload\n",
    "        if isinstance(ERROR_LOG_M, pd.DataFrame):\n",
    "            error_df = ERROR_LOG_M\n",
    "        elif isinstance(ERROR_LOG_M, list):\n",
    "            error_df = pd.DataFrame(ERROR_LOG_M)\n",
    "        else:\n",
    "            logging.warning(\"Unsupported data type for ERROR_LOG. Skipping upload.\")\n",
    "            error_df = None\n",
    "        \n",
    "        if error_df is not None:\n",
    "            _upload_dataframe_to_bigquery(client, analytics_dataset, \"error_logs\", error_df)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the comparison of multiple base tables against their master_hub_ counterparts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize BigQuery client\n",
    "        try:\n",
    "            client = get_bigquery_client(PROJECT_ID)\n",
    "        except Exception:\n",
    "            logging.error(\"Exiting due to BigQuery client initialization failure.\")\n",
    "            return\n",
    "\n",
    "        # Find common tables with 'master_hub_' and other prefixes, passing BASE_TABLES\n",
    "        common_tables = find_common_tables_with_master_hub(client, DATASET_ID, PREFIXES, BASE_TABLES)\n",
    "        \n",
    "        if not common_tables:\n",
    "            logging.info(\"No common tables found with 'master_hub_' and the specified prefixes.\")\n",
    "            return\n",
    "\n",
    "        # Iterate over each base table and perform comparisons\n",
    "        for base_name, tables in common_tables.items():\n",
    "            base_table_info = BASE_TABLES.get(base_name)\n",
    "            if not base_table_info:\n",
    "                logging.warning(f\"No configuration found for base table '{base_name}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            master_key = base_table_info.get('master_key')\n",
    "            target_tables = base_table_info.get('targets', {})\n",
    "            \n",
    "            master_table = tables.get('master_hub_')\n",
    "            if not master_table:\n",
    "                logging.warning(f\"Master table 'master_hub_{base_name}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            all_results = []\n",
    "            \n",
    "            # Iterate through each prefix and its corresponding target_key\n",
    "            for prefix, target_key in target_tables.items():\n",
    "                target_table = tables.get(prefix)\n",
    "                if not target_table:\n",
    "                    logging.warning(f\"Target table with prefix '{prefix}' for base table '{base_name}' not found. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                comparison_result = compare_tables(\n",
    "                    client, \n",
    "                    DATASET_ID, \n",
    "                    base_name, \n",
    "                    master_table, \n",
    "                    target_table, \n",
    "                    master_key, \n",
    "                    target_key  # Pass the correct target_key per prefix\n",
    "                )\n",
    "                if comparison_result:\n",
    "                    all_results.append(comparison_result)\n",
    "                    \n",
    "                    # Prepare and send a separate Slack message for each comparison\n",
    "                    total_mismatches = len(comparison_result['mismatches'])\n",
    "                    total_nulls_master = len(comparison_result['null_values_master'])\n",
    "                    total_nulls_target = len(comparison_result['null_values_target'])\n",
    "                    total_dup_master = len(comparison_result['duplicates_master'])\n",
    "                    total_dup_target = len(comparison_result['duplicates_target'])\n",
    "                    total_data_type_issues = len(comparison_result['data_type_issues'])\n",
    "                    total_format_issues_master = len(comparison_result['format_issues_master'])\n",
    "                    total_pincode_issues = len(comparison_result['pincode_mapping_issues'])\n",
    "                    total_non_matching_source = len(comparison_result.get('df_master_only_keys', []))\n",
    "                    total_non_matching_target = len(comparison_result.get('df_target_only_keys', []))\n",
    "                    \n",
    "                    message = (\n",
    "                        f\"✅ *Comparison Report Generated for `{base_name}`*\\n\"\n",
    "                        f\"*Tables Compared: `{comparison_result['table1_name']}` vs `{comparison_result['table2_name']}`*\\n\"\n",
    "                        f\"- *Total Mismatches between values of same column name of both tables : `{total_mismatches}`*\\n\"\n",
    "                        f\"- *Total Null Values in `{comparison_result['table1_name']}`: `{total_nulls_master}`*\\n\"\n",
    "                        f\"- *Total Null Values in `{comparison_result['table2_name']}`: `{total_nulls_target}`*\\n\"\n",
    "                        f\"- *Duplicate `{master_key}` in `{comparison_result['table1_name']}`: `{total_dup_master}`*\\n\"\n",
    "                        f\"- *Duplicate `{target_key}` in `{comparison_result['table2_name']}`: `{total_dup_target}`*\\n\"\n",
    "                        f\"- *Total Data Type Issues(mismatch between datatype in columns with same name of both tables): `{total_data_type_issues}`*\\n\"\n",
    "                        f\"- *Total Format/Value Issues(gstin, email, pincode) in `{comparison_result['table1_name']}`: `{total_format_issues_master}`*\\n\"\n",
    "                        f\"- *Total Pincode Mapping Issues in `{comparison_result['table1_name']}`: `{total_pincode_issues}`*\\n\"\n",
    "                         \"- *Non-Matching Keys*:\\n\"\n",
    "                        f\"--*`{master_key}` only in `{comparison_result['table1_name']}` and not in `{comparison_result['table2_name']}`:`{total_non_matching_source}`,*\\n\"\n",
    "                        f\"--*`{target_key}` only in `{comparison_result['table2_name']}` and not in `{comparison_result['table1_name']}`:`{total_non_matching_target}`*\"\n",
    "                    )\n",
    "                \n",
    "                    send_slack_alert(message)\n",
    "\n",
    "        # Upload error logs to BigQuery after all comparisons\n",
    "        upload_comparison_results_to_bigquery(\n",
    "            client, \n",
    "            'analytics_data',\n",
    "            ERROR_LOG_M\n",
    "            )\n",
    "\n",
    "        logging.info(\"All comparisons completed.\")\n",
    "    except Exception as e:\n",
    "        # Capture the full traceback\n",
    "        tb = traceback.format_exc()\n",
    "        logging.error(\"An unexpected error occurred in the main process.\", exc_info=True)\n",
    "        \n",
    "        # Prepare a detailed error message for Slack\n",
    "        error_message = (\n",
    "            f\"❌ *Comparison Process Failed*\\n\"\n",
    "            f\"*Error:* {str(e)}\\n\"\n",
    "            f\"*Traceback:*\\n```{tb}```\"\n",
    "        )\n",
    "        send_slack_alert(error_message)\n",
    "\n",
    "        # Optionally, exit the script with a non-zero status\n",
    "        sys.exit(1)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1113978-1fab-4a8b-a2d7-24ef3ca0b567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
