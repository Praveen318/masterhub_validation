{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2515543-48bf-49a0-8061-d2ad04b3205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 12:14:07,865 - INFO - Slack client initialized successfully.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "2024-11-28 12:14:08,649 - INFO - BigQuery client initialized successfully.\n",
      "2024-11-28 12:14:10,211 - INFO - Found 174 tables in dataset 'Impetus_dev_prod'.\n",
      "2024-11-28 12:14:10,213 - INFO - Identified 2 common base names with 'master_hub_' and other specified prefixes.\n",
      "2024-11-28 12:14:10,214 - INFO - Starting comparison for base table 'supplier': 'master_hub_supplier' vs 'procuro_supplier'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-28 12:14:11,806 - INFO - Loaded data from table 'master_hub_supplier' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-28 12:14:12,991 - INFO - Loaded data from table 'procuro_supplier' into DataFrame.\n",
      "2024-11-28 12:14:12,992 - INFO - Filtered 'supplier' master table: 0 records excluded based on is_active = True.\n",
      "2024-11-28 12:14:13,185 - INFO - Retrieved schema for table 'master_hub_supplier'.\n",
      "2024-11-28 12:14:13,384 - INFO - Retrieved schema for table 'procuro_supplier'.\n",
      "2024-11-28 12:14:13,396 - INFO - Found 18 common columns, 16 unique to first table, 11 unique to second table.\n",
      "2024-11-28 12:14:13,397 - INFO - No important columns defined for 'supplier'. Applying checks to all columns except non_imp_columns: ['_boltic_mark_deleted', 'state', 'gstin', '_boltic_ingested_at', 'supplier_code', '_boltic_id', '_boltic_merged', 'pincode', 'is_msme', '_boltic_meta_id', '_boltic_pipe_id', 'city', 'address', '_boltic_updated_at', 'name', 'pan']\n",
      "2024-11-28 12:14:13,401 - INFO - Found 0 duplicate entries based on 'supplier_code'.\n",
      "2024-11-28 12:14:13,404 - INFO - Found 0 duplicate entries based on 'supplier_code'.\n",
      "2024-11-28 12:14:13,416 - INFO - Merged DataFrame has 822 records for mismatch comparison.\n",
      "2024-11-28 12:14:13,520 - INFO - Found 1050 mismatches between 'master_hub_supplier' and 'procuro_supplier'.\n",
      "2024-11-28 12:14:15,165 - INFO - Found 3360 null values in master table 'master_hub_supplier'.\n",
      "2024-11-28 12:14:15,165 - INFO - Found 267 null values in target table 'procuro_supplier'.\n",
      "2024-11-28 12:14:15,166 - INFO - Found 0 data type issues.\n",
      "2024-11-28 12:14:15,186 - INFO - Found 2 format issues.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-28 12:14:18,986 - INFO - Loaded reference pincode mapping from 'all_india_po_list' in 'analytics_data' dataset.\n",
      "2024-11-28 12:14:24,613 - INFO - Found 226 pincode mapping issues in master table 'master_hub_supplier'.\n",
      "2024-11-28 12:14:24,618 - INFO - Found 99 keys in source not in target and 0 keys in target not in source.\n",
      "2024-11-28 12:14:24,620 - INFO - Completed comparison for 'master_hub_supplier' vs 'procuro_supplier'.\n",
      "2024-11-28 12:14:25,100 - INFO - Message sent to C08310RS2PK: 1732776265.099599\n",
      "2024-11-28 12:14:25,101 - INFO - Starting comparison for base table 'supplier': 'master_hub_supplier' vs 'costing_engine_supplier'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-28 12:14:25,911 - INFO - Loaded data from table 'master_hub_supplier' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-28 12:14:27,439 - INFO - Loaded data from table 'costing_engine_supplier' into DataFrame.\n",
      "2024-11-28 12:14:27,440 - INFO - Filtered 'supplier' master table: 0 records excluded based on is_active = True.\n",
      "2024-11-28 12:14:27,559 - INFO - Retrieved schema for table 'master_hub_supplier'.\n",
      "2024-11-28 12:14:27,659 - INFO - Retrieved schema for table 'costing_engine_supplier'.\n",
      "2024-11-28 12:14:27,676 - INFO - Found 25 common columns, 9 unique to first table, 2 unique to second table.\n",
      "2024-11-28 12:14:27,677 - INFO - No important columns defined for 'supplier'. Applying checks to all columns except non_imp_columns: ['is_active', 'state', 'gstin', '_boltic_merged', 'pincode', 'is_msme', '_boltic_meta_id', '_boltic_pipe_id', '_boltic_id', 'pan', '_boltic_mark_deleted', 'is_sample_supplier', 'vendor_status', '_boltic_ingested_at', 'supplier_code', 'city', 'address', '_boltic_updated_at', 'name', 'email', 'country_of_origin']\n",
      "2024-11-28 12:14:27,679 - INFO - Found 0 duplicate entries based on 'supplier_code'.\n",
      "2024-11-28 12:14:28,552 - INFO - Found 757 duplicate entries based on 'supplier_code'.\n",
      "2024-11-28 12:14:28,554 - WARNING - Duplicate keys found in target table 'costing_engine_supplier'. These will be reported but not used in mismatch comparison.\n",
      "2024-11-28 12:14:28,557 - INFO - Merged DataFrame has 921 records for mismatch comparison.\n",
      "2024-11-28 12:14:28,608 - INFO - Found 154 mismatches between 'master_hub_supplier' and 'costing_engine_supplier'.\n",
      "2024-11-28 12:14:32,725 - INFO - Found 3360 null values in master table 'master_hub_supplier'.\n",
      "2024-11-28 12:14:32,725 - INFO - Found 3901 null values in target table 'costing_engine_supplier'.\n",
      "2024-11-28 12:14:32,726 - INFO - Found 0 data type issues.\n",
      "2024-11-28 12:14:32,747 - INFO - Found 2 format issues.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-28 12:14:35,681 - INFO - Loaded reference pincode mapping from 'all_india_po_list' in 'analytics_data' dataset.\n",
      "2024-11-28 12:14:41,622 - INFO - Found 226 pincode mapping issues in master table 'master_hub_supplier'.\n",
      "2024-11-28 12:14:41,628 - INFO - Found 0 keys in source not in target and 4 keys in target not in source.\n",
      "2024-11-28 12:14:41,630 - INFO - Completed comparison for 'master_hub_supplier' vs 'costing_engine_supplier'.\n",
      "2024-11-28 12:14:42,113 - INFO - Message sent to C08310RS2PK: 1732776282.103429\n",
      "2024-11-28 12:14:42,134 - INFO - Added Table of Contents to the Word document.\n",
      "2024-11-28 12:14:42,508 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:43,402 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:43,475 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:43,479 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:43,595 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:43,606 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:43,662 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:44,550 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:45,603 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:45,863 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:45,867 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:45,985 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:45,988 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:14:46,064 - INFO - Saved aggregated comparison report as 'supplier_comparison_report_aggregated_20241128_121445.docx'.\n",
      "2024-11-28 12:14:47,455 - INFO - File uploaded to Slack channel 'C08310RS2PK': https://gofynd.slack.com/files/U07TU8ERA1J/F082WS09T9S/supplier_comparison_report_aggregated_20241128_121445.docx\n",
      "2024-11-28 12:14:47,456 - INFO - Removed local report file 'supplier_comparison_report_aggregated_20241128_121445.docx'.\n",
      "2024-11-28 12:15:17,473 - INFO - Starting comparison for base table 'vendor_details': 'master_hub_supplier' vs 'scan_pack_vendor_details'.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-28 12:15:18,868 - INFO - Loaded data from table 'master_hub_supplier' into DataFrame.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-28 12:15:19,392 - INFO - Loaded data from table 'scan_pack_vendor_details' into DataFrame.\n",
      "2024-11-28 12:15:19,395 - INFO - Filtered 'vendor_details' master table: 0 records excluded based on is_active = True.\n",
      "2024-11-28 12:15:19,607 - INFO - Retrieved schema for table 'master_hub_supplier'.\n",
      "2024-11-28 12:15:19,825 - INFO - Retrieved schema for table 'scan_pack_vendor_details'.\n",
      "2024-11-28 12:15:19,835 - INFO - Found 17 common columns, 17 unique to first table, 9 unique to second table.\n",
      "2024-11-28 12:15:19,836 - INFO - No important columns defined for 'vendor_details'. Applying checks to all columns except non_imp_columns: ['pan', '_boltic_mark_deleted', 'state', '_boltic_ingested_at', '_boltic_id', '_boltic_merged', 'pincode', '_boltic_meta_id', 'city', '_boltic_pipe_id', 'address', '_boltic_updated_at', 'name']\n",
      "2024-11-28 12:15:19,843 - INFO - Found 0 duplicate entries based on 'supplier_code'.\n",
      "2024-11-28 12:15:19,846 - INFO - Found 0 duplicate entries based on 'vendor_code'.\n",
      "2024-11-28 12:15:19,853 - INFO - Merged DataFrame has 96 records for mismatch comparison.\n",
      "2024-11-28 12:15:19,860 - INFO - Found 452 mismatches between 'master_hub_supplier' and 'scan_pack_vendor_details'.\n",
      "2024-11-28 12:15:20,645 - INFO - Found 3360 null values in master table 'master_hub_supplier'.\n",
      "2024-11-28 12:15:20,645 - INFO - Found 777 null values in target table 'scan_pack_vendor_details'.\n",
      "2024-11-28 12:15:20,646 - INFO - Found 0 data type issues.\n",
      "2024-11-28 12:15:20,666 - INFO - Found 2 format issues.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "2024-11-28 12:15:24,101 - INFO - Loaded reference pincode mapping from 'all_india_po_list' in 'analytics_data' dataset.\n",
      "2024-11-28 12:15:29,602 - INFO - Found 226 pincode mapping issues in master table 'master_hub_supplier'.\n",
      "2024-11-28 12:15:29,606 - INFO - Found 825 keys in source not in target and 0 keys in target not in source.\n",
      "2024-11-28 12:15:29,609 - INFO - Completed comparison for 'master_hub_supplier' vs 'scan_pack_vendor_details'.\n",
      "2024-11-28 12:15:30,527 - INFO - Message sent to C08310RS2PK: 1732776330.263229\n",
      "2024-11-28 12:15:30,565 - INFO - Added Table of Contents to the Word document.\n",
      "2024-11-28 12:15:30,764 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:15:31,643 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:15:31,844 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:15:31,849 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:15:31,969 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:15:32,053 - INFO - Added table to the Word document.\n",
      "2024-11-28 12:15:32,094 - INFO - Saved aggregated comparison report as 'vendor_details_comparison_report_aggregated_20241128_121532.docx'.\n",
      "2024-11-28 12:15:33,421 - INFO - File uploaded to Slack channel 'C08310RS2PK': https://gofynd.slack.com/files/U07TU8ERA1J/F082RFJ9PDK/vendor_details_comparison_report_aggregated_20241128_121532.docx\n",
      "2024-11-28 12:15:33,527 - INFO - Removed local report file 'vendor_details_comparison_report_aggregated_20241128_121532.docx'.\n",
      "2024-11-28 12:16:04,748 - ERROR - Failed to upload 'fynd-jio-impetus-prod.analytics_data.error_logs' to BigQuery: 403 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/fynd-jio-impetus-prod/jobs?uploadType=multipart: Access Denied: Table fynd-jio-impetus-prod:analytics_data.error_logs: Permission bigquery.tables.update denied on table fynd-jio-impetus-prod:analytics_data.error_logs (or it may not exist).\n",
      "2024-11-28 12:16:04,753 - INFO - All comparisons completed.\n"
     ]
    }
   ],
   "source": [
    "#log fixed\n",
    "#mismatch fixed\n",
    "#main.py\n",
    "#count in each heading\n",
    "#Duplicate_table_fixed\n",
    "#Big_numeric_readibiliy\n",
    "#paragraph to heading\n",
    "#ToC added\n",
    "#format issue enchaned\n",
    "#pincode issue added and enchanced\n",
    "#enchanced scalabiltiy incase target_key changes acorrding to table\n",
    "# working error log\n",
    "#imp and non imp added\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "from docx import Document\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import traceback\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = 'fynd-jio-impetus-prod'       # Replace with your project ID\n",
    "DATASET_ID = 'Impetus_dev_prod'                 # Replace with your dataset ID\n",
    "\n",
    "# # Configuration\n",
    "# PROJECT_ID = 'fynd-jio-impetus-non-prod'       # Replace with your project ID\n",
    "# DATASET_ID = 'Impetus_dev_sit'                 # Replace with your dataset ID\n",
    "PREFIXES = ['procuro_', 'costing_engine_', 'scan_pack_', 'pigeon_']  # Define your prefixes\n",
    "# time_stamp,issue, error_message, tables_compared, issue_table, issue_column, unique_identifer\n",
    "ERROR_LOG_M = []\n",
    "\n",
    "# Get the current datetime\n",
    "now = datetime.now()\n",
    "\n",
    "Non_imp_columns = {\n",
    "    'supplier': ['id', '_id', 'updated_at', 'created_at'],\n",
    "    'vendor_details': ['id', '_id', 'updated_at', 'created_at']  # Add if applicable\n",
    "}\n",
    "\n",
    "\n",
    "# Define Important Columns\n",
    "Imp_columns = {\n",
    "    'brand': ['name', 'slug', 'code'],\n",
    "    'brick': ['name', 'id', 'brick_code', 'description', 'class_code'],\n",
    "    'config_buyer_brand_mapping': ['buyer_email', 'brand_code', 'id', 'buyer_id', 'is_active', 'buyer_name'],\n",
    "    'brand_pm_mapping': ['pm_id', 'brand_code', 'pm_email', 'is_active', 'pm_name', 'id'],\n",
    "    'coe_bom_element_type_mapping': ['is_active','coe_id','id','coe_name','coe_approver_email','element_type']\n",
    "}\n",
    "\n",
    "# Mapping of base table names to their master key and target keys per prefix\n",
    "BASE_TABLES = {\n",
    "    # 'brand': {\n",
    "    #     'master_key': 'code',\n",
    "    #     'targets': {\n",
    "    #         'procuro_': 'code',\n",
    "    #         'costing_engine_': 'code'\n",
    "    #     },\n",
    "    #     'active_filter': {\n",
    "    #         'column': 'is_active',\n",
    "    #         'value': True\n",
    "    #     },\n",
    "    #     'perform_checks': True  # Default behavior\n",
    "    # },\n",
    "    # 'brand_pm_mapping': {\n",
    "    #     'master_key': 'pm_id',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'pm_id'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    # 'brick': {\n",
    "    #     'master_key': 'brick_code',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'code'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    # 'coe_bom_element_type_mapping': {\n",
    "    #     'master_key': 'coe_name',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'coe_name'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    # 'event_log': {\n",
    "    #     'master_key': 'user_id',\n",
    "    #     'targets': {\n",
    "    #         'costing_engine_': 'user_id'\n",
    "    #     },\n",
    "    #     'perform_checks': True\n",
    "    # },\n",
    "    'supplier': {\n",
    "        'master_key': 'supplier_code',\n",
    "        'targets': {\n",
    "            'procuro_': 'supplier_code',\n",
    "            'costing_engine_': 'supplier_code'\n",
    "        },\n",
    "        'active_filter': {  # Apply active filter\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    'vendor_details': {  # Newly added entry\n",
    "        'master_key': 'supplier_code',  # Using supplier_code as the key\n",
    "        'master_table': 'master_hub_supplier',  # Specify the master table explicitly\n",
    "        'targets': {\n",
    "            'scan_pack_': 'vendor_code'\n",
    "        },\n",
    "        'active_filter': {  # Apply active filter\n",
    "            'column': 'is_active',\n",
    "            'value': True\n",
    "        },\n",
    "        'perform_checks': True\n",
    "    },\n",
    "    # 'hsn_tax_mapping': {  # Newly added base table for HSN Codes\n",
    "    #     'master_key': 'hsn_code',  # Assuming 'hsn_code' is the key column\n",
    "    #     'master_table': 'master_hub_hsn',\n",
    "    #     'targets': {\n",
    "    #         'procuro_': 'hsn_code',\n",
    "    #     },\n",
    "    #     'perform_checks': False  # Only perform key comparisons\n",
    "    # }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Slack configuration\n",
    "SLACK_TOKEN = \"xoxb-2151238541-7946286860052-5FCcfqBPem0xKigGlIcKdLgX\"\n",
    "SLACK_CHANNEL = \"C07UN19ETK5\"\n",
    "# SLACK_CHANNEL = \"C08310RS2PK\"\n",
    "\n",
    "# Initialize Slack client\n",
    "if SLACK_TOKEN and SLACK_CHANNEL:\n",
    "    slack_client = WebClient(token=SLACK_TOKEN)\n",
    "    logging.info(\"Slack client initialized successfully.\")\n",
    "else:\n",
    "    slack_client = None\n",
    "    logging.warning(\"Slack token or channel not found. Slack notifications will be disabled.\")\n",
    "\n",
    "\n",
    "def get_bigquery_client(project_id):\n",
    "    \"\"\"\n",
    "    Initialize and return a BigQuery client.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID.\n",
    "\n",
    "    Returns:\n",
    "        bigquery.Client: An initialized BigQuery client.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        logging.info(\"BigQuery client initialized successfully.\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize BigQuery client: {e}\")\n",
    "        raise\n",
    "\n",
    "def find_common_tables_with_master_hub(client, dataset_name, prefixes, base_tables):\n",
    "    \"\"\"\n",
    "    Find tables in the specified dataset that share the same base name after removing the 'master_hub_' prefix\n",
    "    and exist with other given prefixes.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset to search within.\n",
    "        prefixes (list): List of prefixes to compare with 'master_hub_'.\n",
    "        base_tables (dict): The BASE_TABLES dictionary containing base table configurations.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are base names and values are dictionaries showing which prefixes have tables.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Reference the dataset\n",
    "        dataset_ref = client.dataset(dataset_name)\n",
    "        \n",
    "        # List all tables in the dataset\n",
    "        tables = client.list_tables(dataset_ref)\n",
    "        table_names = [table.table_id for table in tables]\n",
    "        logging.info(f\"Found {len(table_names)} tables in dataset '{dataset_name}'.\")\n",
    "        \n",
    "        # Dictionary to hold base names and their corresponding tables\n",
    "        common_tables = {}\n",
    "        for base_name, config in base_tables.items():\n",
    "            # Determine the master table\n",
    "            master_table = config.get('master_table', f'master_hub_{base_name}')\n",
    "            if master_table in table_names:\n",
    "                common_tables[base_name] = {'master_hub_': master_table}\n",
    "                # Check for target tables with specified prefixes\n",
    "                for prefix, target_key in config.get('targets', {}).items():\n",
    "                    target_table = f\"{prefix}{base_name}\"\n",
    "                    if target_table in table_names:\n",
    "                        common_tables[base_name][prefix] = target_table\n",
    "            else:\n",
    "                logging.warning(f\"Master table '{master_table}' for base '{base_name}' not found in dataset.\")\n",
    "        \n",
    "        # Filter out base names that only have 'master_hub_' but no other matching prefixes\n",
    "        common_tables_with_prefixes = {base_name: tables for base_name, tables in common_tables.items() if len(tables) > 1}\n",
    "        \n",
    "        logging.info(f\"Identified {len(common_tables_with_prefixes)} common base names with 'master_hub_' and other specified prefixes.\")\n",
    "        return common_tables_with_prefixes\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Google API Error: {e.message}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return {}\n",
    "\n",
    "def get_table_schema(client, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Retrieve the schema of a specified BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        table_name (str): The name of the table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping column names to their data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table_ref = client.dataset(dataset_name).table(table_name)\n",
    "        table = client.get_table(table_ref)\n",
    "        schema = {field.name: field.field_type for field in table.schema}\n",
    "        logging.info(f\"Retrieved schema for table '{table_name}'.\")\n",
    "        return schema\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Failed to retrieve schema for table '{table_name}': {e.message}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while retrieving schema for table '{table_name}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def load_table_from_bigquery(client, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Load a table from BigQuery into a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        table_name (str): The name of the table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the table data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f\"SELECT * FROM `{PROJECT_ID}.{dataset_name}.{table_name}`\"\n",
    "        df = client.query(query).to_dataframe()\n",
    "        logging.info(f\"Loaded data from table '{table_name}' into DataFrame.\")\n",
    "        return df\n",
    "    except GoogleAPIError as e:\n",
    "        logging.error(f\"Failed to load table '{table_name}': {e.message}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while loading table '{table_name}': {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# def standardize_dataframe(df):\n",
    "#     \"\"\"\n",
    "#     Standardize string columns in the DataFrame by stripping whitespace and converting to lowercase.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The DataFrame to standardize.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Standardized DataFrame.\n",
    "#     \"\"\"\n",
    "#     df_copy = df.copy()\n",
    "#     for col in df_copy.columns:\n",
    "#         if pd.api.types.is_string_dtype(df_copy[col]):\n",
    "#             df_copy[col] = df_copy[col].astype(str).str.strip().str.lower()\n",
    "#     logging.info(\"Standardized DataFrame for comparison.\")\n",
    "#     return df_copy\n",
    "\n",
    "def standardize_dataframe(df, exclude_columns=[]):\n",
    "    \"\"\"\n",
    "    Standardize string columns in the DataFrame by stripping whitespace and converting to lowercase,\n",
    "    excluding specified columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to standardize.\n",
    "        exclude_columns (list): Columns to exclude from standardization.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Standardized DataFrame.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in df_copy.columns:\n",
    "        if col in exclude_columns:\n",
    "            continue  # Skip standardizing this column\n",
    "        if pd.api.types.is_string_dtype(df_copy[col]):\n",
    "            df_copy[col] = df_copy[col].astype(str).str.strip().str.lower()\n",
    "    logging.info(\"Standardized DataFrame for comparison.\")\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def find_common_and_non_common_columns(df1, df2):\n",
    "    \"\"\"\n",
    "    Identify common and unique columns between two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df1 (pd.DataFrame): First DataFrame.\n",
    "        df2 (pd.DataFrame): Second DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (common_columns, df1_unique_columns, df2_unique_columns)\n",
    "    \"\"\"\n",
    "    common_columns = list(set(df1.columns).intersection(set(df2.columns)))\n",
    "    df1_unique_columns = list(set(df1.columns) - set(df2.columns))\n",
    "    df2_unique_columns = list(set(df2.columns) - set(df1.columns))\n",
    "    logging.info(f\"Found {len(common_columns)} common columns, {len(df1_unique_columns)} unique to first table, {len(df2_unique_columns)} unique to second table.\")\n",
    "    return common_columns, df1_unique_columns, df2_unique_columns\n",
    "\n",
    "# def find_mismatches(df_master, df_target, common_columns, master_key, target_key, table1, table2, duplicates_master, duplicates_target, non_imp_columns):\n",
    "#     \"\"\"\n",
    "#     Identify mismatches between two DataFrames based on common columns and key columns.\n",
    "\n",
    "#     Args:\n",
    "#         df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "#         df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "#         common_columns (list): List of common columns to compare.\n",
    "#         master_key (str): The key column in the master table.\n",
    "#         target_key (str): The key column in the target table.\n",
    "#         table1 (str): Name of the source table.\n",
    "#         table2 (str): Name of the target table.\n",
    "#         duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "#         duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "\n",
    "#     Returns:\n",
    "#         list: List of dictionaries containing mismatch details.\n",
    "#     \"\"\"\n",
    "#     mismatches = []\n",
    "#     error_logs_m = []\n",
    "#     # Ensure key columns are present in both DataFrames\n",
    "#     if master_key not in df_master.columns or target_key not in df_target.columns:\n",
    "#         logging.error(f\"Key columns '{master_key}' or '{target_key}' not found in the respective tables.\")\n",
    "#         return mismatches\n",
    "\n",
    "#     # Rename target key to match master key for easier comparison\n",
    "#     df_target_renamed = df_target.rename(columns={target_key: master_key})\n",
    "\n",
    "#     # Merge DataFrames on the master_key\n",
    "#     merged_df = pd.merge(\n",
    "#         df_master.drop_duplicates(subset=master_key),\n",
    "#         df_target_renamed.drop_duplicates(subset=master_key),\n",
    "#         on=master_key,\n",
    "#         suffixes=(f'_{table1}', f'_{table2}'),\n",
    "#         how='inner'\n",
    "#     )\n",
    "\n",
    "#     logging.info(f\"Merged DataFrame has {len(merged_df)} records for mismatch comparison.\")\n",
    "\n",
    "#     for index, row in merged_df.iterrows():\n",
    "#         key = row[master_key]\n",
    "#         for column in common_columns:\n",
    "#             if column.startswith('_boltic_') or column in non_imp_columns:\n",
    "#                 continue  # Skip columns starting with '_boltic_'\n",
    "#             val_master = row.get(f\"{column}_{table1}\")\n",
    "#             val_target = row.get(f\"{column}_{table2}\")\n",
    "#             # Handle NaN values in comparison\n",
    "#             if pd.isna(val_master) and pd.isna(val_target):\n",
    "#                 continue  # Both are NaN, treat as equal\n",
    "#             elif pd.isna(val_master) or pd.isna(val_target) or val_master != val_target:\n",
    "#                 mismatch_detail = {\n",
    "#                     master_key: key,\n",
    "#                     'column': column,\n",
    "#                     f'{table1}_value': val_master,\n",
    "#                     f'{table2}_value': val_target\n",
    "#                 }\n",
    "#                 mismatches.append(mismatch_detail)\n",
    "#                 error_detail = {\n",
    "#                     'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "#                     'issue': 'mismatch',\n",
    "#                     'error_message': '',\n",
    "#                     'source_table': f'{table1}',\n",
    "#                     'target_table': f'{table2}',\n",
    "#                     'issue_column': column,\n",
    "#                     'unique_identifier': f'{master_key}: {key}'\n",
    "#                 }\n",
    "#                 error_logs_m.append(error_detail)\n",
    "\n",
    "    \n",
    "\n",
    "#     logging.info(f\"Found {len(mismatches)} mismatches between '{table1}' and '{table2}'.\")\n",
    "#     return mismatches, error_logs_m\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def find_mismatches(df_master, df_target, common_columns, master_key, target_key, table1, table2, duplicates_master, duplicates_target, non_imp_columns):\n",
    "    \"\"\"\n",
    "    Identify mismatches between two DataFrames based on common columns and key columns, with case-insensitive comparisons for string columns.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "        df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "        common_columns (list): List of common columns to compare.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        table1 (str): Name of the source table.\n",
    "        table2 (str): Name of the target table.\n",
    "        duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "        duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "        non_imp_columns (list): Columns to exclude from mismatch comparison.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - List of dictionaries with mismatch details.\n",
    "            - List of dictionaries with error log details.\n",
    "    \"\"\"\n",
    "    mismatches = []\n",
    "    error_logs_m = []\n",
    "    now = datetime.now()  # Initialize the current timestamp\n",
    "\n",
    "    # Ensure key columns are present in both DataFrames\n",
    "    if master_key not in df_master.columns or target_key not in df_target.columns:\n",
    "        logging.error(f\"Key columns '{master_key}' or '{target_key}' not found in the respective tables.\")\n",
    "        return mismatches, error_logs_m\n",
    "\n",
    "    # Rename target key to match master key for easier comparison\n",
    "    df_target_renamed = df_target.rename(columns={target_key: master_key})\n",
    "\n",
    "    # Drop duplicates based on the master_key to ensure unique keys for merging\n",
    "    df_master_unique = df_master.drop_duplicates(subset=master_key)\n",
    "    df_target_unique = df_target_renamed.drop_duplicates(subset=master_key)\n",
    "\n",
    "    # Merge DataFrames on the master_key\n",
    "    merged_df = pd.merge(\n",
    "        df_master_unique,\n",
    "        df_target_unique,\n",
    "        on=master_key,\n",
    "        suffixes=(f'_{table1}', f'_{table2}'),\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Merged DataFrame has {len(merged_df)} records for mismatch comparison.\")\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        key = row[master_key]\n",
    "        for column in common_columns:\n",
    "            if column.startswith('_boltic_') or column in non_imp_columns:\n",
    "                continue  # Skip columns starting with '_boltic_' or non-important columns\n",
    "\n",
    "            master_col = f\"{column}_{table1}\"\n",
    "            target_col = f\"{column}_{table2}\"\n",
    "\n",
    "            val_master = row.get(master_col)\n",
    "            val_target = row.get(target_col)\n",
    "\n",
    "            # Convert string values to lowercase for case-insensitive comparison\n",
    "            if isinstance(val_master, str):\n",
    "                val_master_cmp = val_master.lower()\n",
    "            else:\n",
    "                val_master_cmp = val_master\n",
    "\n",
    "            if isinstance(val_target, str):\n",
    "                val_target_cmp = val_target.lower()\n",
    "            else:\n",
    "                val_target_cmp = val_target\n",
    "\n",
    "            # Handle NaN values in comparison\n",
    "            if pd.isna(val_master_cmp) and pd.isna(val_target_cmp):\n",
    "                continue  # Both are NaN, treat as equal\n",
    "            elif pd.isna(val_master_cmp) or pd.isna(val_target_cmp) or val_master_cmp != val_target_cmp:\n",
    "                mismatch_detail = {\n",
    "                    master_key: key,\n",
    "                    'column': column,\n",
    "                    f'{table1}_value': val_master,\n",
    "                    f'{table2}_value': val_target\n",
    "                }\n",
    "                mismatches.append(mismatch_detail)\n",
    "                error_detail = {\n",
    "                    'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'mismatch',\n",
    "                    'error_message': '',\n",
    "                    'source_table': table1,\n",
    "                    'target_table': table2,\n",
    "                    'issue_column': column,\n",
    "                    'unique_identifier': f'{master_key}: {key}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(mismatches)} mismatches between '{table1}' and '{table2}'.\")\n",
    "    return mismatches, error_logs_m\n",
    "\n",
    "\n",
    "# def find_duplicates(df, key_column, table_name):\n",
    "#     \"\"\"\n",
    "#     Detect duplicate key_column entries in the DataFrame and identify differences.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The DataFrame to check.\n",
    "#         key_column (str): The key column to check for duplicates.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: DataFrame containing duplicated key_column values with differences.\n",
    "#     \"\"\"\n",
    "#     if key_column not in df.columns:\n",
    "#         logging.error(f\"Key column '{key_column}' not found in DataFrame.\")\n",
    "#         return pd.DataFrame()\n",
    "    \n",
    "#     # Get all duplicate entries (keep=False to get all duplicates)\n",
    "#     duplicates_df = df[df.duplicated(subset=key_column, keep=False)]\n",
    "    \n",
    "#     # Group by key_column\n",
    "#     grouped = duplicates_df.groupby(key_column)\n",
    "    \n",
    "#     duplicate_records = []\n",
    "\n",
    "#     error_logs_m = []\n",
    "\n",
    "#     for key, group in grouped:\n",
    "#         if len(group) <= 1:\n",
    "#             continue  # Not a duplicate\n",
    "        \n",
    "#         # Drop key_column and any columns starting with '_boltic_'\n",
    "#         group_non_key = group.drop(columns=[key_column] + [col for col in group.columns if col.startswith('_boltic_')])\n",
    "        \n",
    "#         # Check if all rows are identical\n",
    "#         if group_non_key.nunique().sum() == 0:\n",
    "#             difference = \"No difference exists\"\n",
    "#         else:\n",
    "#             # Find which columns have differences\n",
    "#             cols_with_diff = group_non_key.columns[group_non_key.nunique() > 1].tolist()\n",
    "#             # difference = ', '.join(cols_with_diff)\n",
    "#             difference = \"Difference in value of columns: \" + ', '.join(cols_with_diff)\n",
    "        \n",
    "#         duplicate_records.append({\n",
    "#             key_column: key,\n",
    "#             'Difference in value': difference\n",
    "#         })\n",
    "#         error_detail = {\n",
    "#             'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "#             'issue': 'duplicate',\n",
    "#             'error_message': f'{difference}',\n",
    "#             'source_table': f'{table_name}',\n",
    "#             'target_table': '',\n",
    "#             'issue_column': '',\n",
    "#             'unique_identifier': f'{key_column}: {key}'\n",
    "#         }\n",
    "#         error_logs_m.append(error_detail)\n",
    "    \n",
    "#     logging.info(f\"Found {len(duplicate_records)} duplicate entries based on '{key_column}'.\")\n",
    "#     return pd.DataFrame(duplicate_records), error_logs_m\n",
    "\n",
    "def find_duplicates(df, key_column, table_name):\n",
    "    \"\"\"\n",
    "    Detect duplicate key_column entries in the DataFrame and identify differences,\n",
    "    ignoring specified columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to check.\n",
    "        key_column (str): The key column to check for duplicates.\n",
    "        table_name (str): Name of the table being checked.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - pd.DataFrame containing duplicated key_column values with differences.\n",
    "            - list of error log dictionaries.\n",
    "    \"\"\"\n",
    "    if key_column not in df.columns:\n",
    "        logging.error(f\"Key column '{key_column}' not found in DataFrame.\")\n",
    "        return pd.DataFrame(), []\n",
    "    \n",
    "    # Define columns to ignore when checking for differences\n",
    "    ignore_columns = {'id', '_id', 'updated_at'}\n",
    "    \n",
    "    # Get all duplicate entries (keep=False to get all duplicates)\n",
    "    duplicates_df = df[df.duplicated(subset=key_column, keep=False)]\n",
    "    \n",
    "    # Group by key_column\n",
    "    grouped = duplicates_df.groupby(key_column)\n",
    "    \n",
    "    duplicate_records = []\n",
    "    error_logs_m = []\n",
    "    now = datetime.now()\n",
    "    \n",
    "    for key, group in grouped:\n",
    "        if len(group) <= 1:\n",
    "            continue  # Not a duplicate\n",
    "        \n",
    "        # Drop key_column and any columns starting with '_boltic_'\n",
    "        non_boltic_cols = [col for col in group.columns if not col.startswith('_boltic_')]\n",
    "        group_non_key = group[non_boltic_cols].drop(columns=[key_column])\n",
    "        \n",
    "        # Determine columns to check by excluding ignore_columns\n",
    "        columns_to_check = [col for col in group_non_key.columns if col not in ignore_columns]\n",
    "        \n",
    "        if not columns_to_check:\n",
    "            # If there are no columns to check after ignoring, treat as no difference\n",
    "            difference = \"No difference exists (only ignored columns differ)\"\n",
    "        else:\n",
    "            # Check if all rows are identical in the columns_to_check\n",
    "            subset = group_non_key[columns_to_check]\n",
    "            if subset.nunique().sum() == 0:\n",
    "                difference = \"No difference exists\"\n",
    "            else:\n",
    "                # Find which columns have differences\n",
    "                cols_with_diff = subset.columns[subset.nunique() > 1].tolist()\n",
    "                difference = \"Difference in value of columns: \" + ', '.join(cols_with_diff)\n",
    "        \n",
    "        # Only add to duplicate_records if differences exist outside ignored columns\n",
    "        if difference != \"No difference exists\" and difference != \"No difference exists (only ignored columns differ)\":\n",
    "            duplicate_records.append({\n",
    "                key_column: key,\n",
    "                'Difference in value': difference\n",
    "            })\n",
    "            error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'duplicate',\n",
    "                'error_message': difference,\n",
    "                'source_table': table_name,\n",
    "                'target_table': '',\n",
    "                'issue_column': '',\n",
    "                'unique_identifier': f'{key_column}: {key}'\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "    \n",
    "    logging.info(f\"Found {len(duplicate_records)} duplicate entries based on '{key_column}'.\")\n",
    "    return pd.DataFrame(duplicate_records), error_logs_m\n",
    "\n",
    "\n",
    "def validate_data_types(schema_master, schema_target, master_key, table1_name, table2_name,columns_to_check):\n",
    "    \"\"\"\n",
    "    Compare data types of common columns between master and target schemas.\n",
    "\n",
    "    Args:\n",
    "        schema_master (dict): Schema of the master table.\n",
    "        schema_target (dict): Schema of the target table.\n",
    "        master_key (str): The key column for reference.\n",
    "        table1_name (str): Name of the first table.\n",
    "        table2_name (str): Name of the second table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing data type discrepancies with table names in headers.\n",
    "    \"\"\"\n",
    "    data_type_issues = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    # # Identify common columns\n",
    "    # common_columns = set(schema_master.keys()).intersection(set(schema_target.keys()))\n",
    "    # Identify common columns to check\n",
    "    common_columns = set(columns_to_check).intersection(set(schema_master.keys()), set(schema_target.keys()))\n",
    "\n",
    "    for column in common_columns:\n",
    "        type_master = schema_master[column]\n",
    "        type_target = schema_target[column]\n",
    "        if type_master != type_target:\n",
    "            data_type_issues.append({\n",
    "                'column_name': column,\n",
    "                f'{table1_name}_data_type': type_master,\n",
    "                f'{table2_name}_data_type': type_target\n",
    "            })\n",
    "            error_detail = {\n",
    "            'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'data_type_issues',\n",
    "            'error_message': f'{table1_name}_data_type: {type_master} , {table2_name}_data_type: {type_target}',\n",
    "            'source_table': f'{table1_name}',\n",
    "            'target_table': f'{table2_name}',\n",
    "            'issue_column': column,\n",
    "            'unique_identifier': ''\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "\n",
    "    logging.info(f\"Found {len(data_type_issues)} data type issues.\")\n",
    "    return pd.DataFrame(data_type_issues), error_logs_m\n",
    "\n",
    "def validate_formats(df_master, df_target, key_column, target_key, target_table, master_table):\n",
    "    \"\"\"\n",
    "    Validate specific column formats using regular expressions and include corresponding target table values.\n",
    "    \n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing format issues with corresponding target table values.\n",
    "    \"\"\"\n",
    "    format_issues = pd.DataFrame(columns=[key_column, 'column', 'value', 'issue', f'{target_table}_value'])\n",
    "    error_logs_m = []\n",
    "    \n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "        \n",
    "        # GSTIN format validation\n",
    "        if 'gstin' in df_master.columns:\n",
    "            gstin = str(row['gstin']).strip()\n",
    "            if not re.match(r'^[0-9]{2}[A-Z]{5}[0-9]{4}[A-Z]{1}[A-Z0-9]{3}$', gstin):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['gstin'] if 'gstin' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                \n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'gstin',\n",
    "                    'value': row['gstin'],\n",
    "                    'issue': 'Invalid GSTIN format',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'format_issue',\n",
    "                'error_message': 'Invalid GSTIN format',\n",
    "                'source_table': f'{master_table}',\n",
    "                'target_table': '',\n",
    "                'issue_column': 'gstin',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "        \n",
    "        # Email format validation\n",
    "        if 'email' in df_master.columns:\n",
    "            email = str(row['email']).strip()\n",
    "            if not re.match(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', email):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['email'] if 'email' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                \n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'email',\n",
    "                    'value': row['email'],\n",
    "                    'issue': 'Invalid email format',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'format_issue',\n",
    "                'error_message': 'Invalid email format',\n",
    "                'source_table': f'{master_table}',\n",
    "                'target_table': '',\n",
    "                'issue_column': 'email',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "        \n",
    "        # Pincode format validation\n",
    "        if 'pincode' in df_master.columns:\n",
    "            pincode = str(row['pincode']).strip()\n",
    "            if not re.match(r'^\\d{6}$', pincode):\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['pincode'] if 'pincode' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                \n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'pincode',\n",
    "                    'value': row['pincode'],\n",
    "                    'issue': 'Pincode must be exactly 6 digits',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'format_issue',\n",
    "                'error_message': 'Pincode must be exactly 6 digits',\n",
    "                'source_table': f'{master_table}',\n",
    "                'target_table': '',\n",
    "                'issue_column': 'pincode',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "\n",
    "                error_logs_m.append(error_detail)\n",
    "\n",
    "        # Address length validation\n",
    "        if 'address' in df_master.columns:\n",
    "            address = str(row['address']).strip()\n",
    "            if len(address) > 100:\n",
    "                # Fetch corresponding target value\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row['address'] if 'address' in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "\n",
    "                format_issues = pd.concat([format_issues, pd.DataFrame([{\n",
    "                    key_column: key_value,\n",
    "                    'column': 'address',\n",
    "                    'value': address,\n",
    "                    'issue': 'Address exceeds 100 characters after stripping',\n",
    "                    f'{target_table}_value': target_value\n",
    "                }])], ignore_index=True)\n",
    "                error_detail = {\n",
    "                    'time_stamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'issue': 'format_issue',\n",
    "                    'error_message': 'Address exceeds 100 characters',\n",
    "                    'source_table': master_table,\n",
    "                    'target_table': '',\n",
    "                    'issue_column': 'address',\n",
    "                    'unique_identifier': f'{key_column}: {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Found {len(format_issues)} format issues.\")\n",
    "    return format_issues, error_logs_m\n",
    "\n",
    "\n",
    "def create_table(doc, data, column_names):\n",
    "    \"\"\"\n",
    "    Helper function to create a table in a docx document from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        data (list or list of dict): Data to populate the table.\n",
    "        column_names (list): List of column names for the table headers.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    table = doc.add_table(rows=1, cols=len(column_names))\n",
    "    table.style = 'Light List Accent 1'\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for i, col_name in enumerate(column_names):\n",
    "        hdr_cells[i].text = col_name\n",
    "\n",
    "    for row_data in data:\n",
    "        row_cells = table.add_row().cells\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            cell_value = str(row_data.get(col_name, '')).strip()\n",
    "            row_cells[i].text = cell_value\n",
    "    logging.info(\"Added table to the Word document.\")\n",
    "\n",
    "def add_non_matching_keys_section(doc, df1_only_keys, table1_name, df2_only_keys, table2_name, key_column_master, key_column_target):\n",
    "    \"\"\"\n",
    "    Add a section in the Word document for non-matching keys between two tables.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "        df1_only_keys (list): Keys present only in table1.\n",
    "        table1_name (str): Name of the first table.\n",
    "        df2_only_keys (list): Keys present only in table2.\n",
    "        table2_name (str): Name of the second table.\n",
    "        key_column_master (str): The key column in the master table.\n",
    "        key_column_target (str): The key column in the target table.\n",
    "    \"\"\"\n",
    "    if df1_only_keys or df2_only_keys:\n",
    "        if df1_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_master}' present only in '{table1_name}' and not in '{table2_name}' ({len(df1_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_master: key.strip()} for key in df1_only_keys], [key_column_master])\n",
    "        if df2_only_keys:\n",
    "            doc.add_heading(f\"'{key_column_target}' present only in '{table2_name}' and not in '{table1_name}' ({len(df2_only_keys)})\", level=2)\n",
    "            create_table(doc, [{key_column_target: key.strip()} for key in df2_only_keys], [key_column_target])\n",
    "    else:\n",
    "        doc.add_paragraph(\"No non-matching keys found.\")\n",
    "\n",
    "def add_table_of_contents(doc):\n",
    "    \"\"\"\n",
    "    Adds a Table of Contents to the Word document.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): The Word document object.\n",
    "    \"\"\"\n",
    "    paragraph = doc.add_paragraph()\n",
    "    run = paragraph.add_run()\n",
    "    fldChar_begin = OxmlElement('w:fldChar')  # creates a new element\n",
    "    fldChar_begin.set(qn('w:fldCharType'), 'begin')  # sets attribute on element\n",
    "    instrText = OxmlElement('w:instrText')\n",
    "    instrText.set(qn('xml:space'), 'preserve')  # sets attribute on element\n",
    "    instrText.text = 'TOC \\\\o \"1-2\" \\\\h \\\\z \\\\u'  # change to what you need\n",
    "    fldChar_separate = OxmlElement('w:fldChar')\n",
    "    fldChar_separate.set(qn('w:fldCharType'), 'separate')\n",
    "    fldChar_end = OxmlElement('w:fldChar')\n",
    "    fldChar_end.set(qn('w:fldCharType'), 'end')\n",
    "    run._r.append(fldChar_begin)\n",
    "    run._r.append(instrText)\n",
    "    run._r.append(fldChar_separate)\n",
    "    run._r.append(fldChar_end)\n",
    "    logging.info(\"Added Table of Contents to the Word document.\")\n",
    "\n",
    "def create_aggregated_document(all_results, base_name):\n",
    "    \"\"\"\n",
    "    Creates a single Word document that presents all comparison results for a base table.\n",
    "\n",
    "    Args:\n",
    "        all_results (list): List of comparison result dictionaries.\n",
    "        base_name (str): The base name of the table.\n",
    "\n",
    "    Returns:\n",
    "        str: The filepath of the saved report.\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "    doc.add_heading(f'{base_name.capitalize()} Tables Comparison Report', level=0)\n",
    "    doc.add_paragraph(f'Report generated on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "    \n",
    "    # Add Instruction for TOC Update\n",
    "    doc.add_paragraph(\n",
    "        \" **Note:** To update the Table of Contents and make the links clickable, go to Reference tab and click Update_Table or press F9 in Windows and Fn+F9 in mac, after opening after opening this document in Microsoft Word.\",\n",
    "        style='Intense Quote'\n",
    "    )\n",
    "        \n",
    "    # Add Table of Contents\n",
    "    doc.add_heading('Table of Contents', level=1)\n",
    "    add_table_of_contents(doc)\n",
    "    doc.add_page_break()\n",
    "\n",
    "    for result in all_results:\n",
    "        table1_name = result['table1_name']\n",
    "        table2_name = result['table2_name']\n",
    "        key_column_master = result['key_column_master']\n",
    "        key_column_target = result['key_column_target']\n",
    "        doc.add_heading(f'Comparison: {table1_name} vs {table2_name}', level=1)\n",
    "\n",
    "        # Mismatches\n",
    "        if result['mismatches']:\n",
    "            doc.add_heading(f'Mismatches ({len(result[\"mismatches\"])})', level=2)\n",
    "            column_names = [key_column_master, 'column', f'{table1_name}_value', f'{table2_name}_value']\n",
    "            create_table(doc, result['mismatches'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No mismatches found.\", level=2)\n",
    "\n",
    "        # Null values in master table\n",
    "        if not result['null_values_master'].empty:\n",
    "            count_null_master = len(result['null_values_master'])\n",
    "            doc.add_heading(f'Null values in {table1_name} ({count_null_master})', level=2)\n",
    "            column_names = [key_column_master, 'column', table2_name]\n",
    "            create_table(doc, result['null_values_master'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table1_name}.\", level=2)\n",
    "\n",
    "        # Null values in target table\n",
    "        if not result['null_values_target'].empty:\n",
    "            count_null_target = len(result['null_values_target'])\n",
    "            doc.add_heading(f'Null values in {table2_name} ({count_null_target})', level=2)\n",
    "            column_names = [key_column_target, 'column', table1_name]\n",
    "            create_table(doc, result['null_values_target'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No null values found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Duplicate keys in master table\n",
    "        if not result['duplicates_master'].empty:\n",
    "            count_dup_master = len(result['duplicates_master'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table1_name} ({count_dup_master})', level=2)\n",
    "            # Modified to include 'Difference in value' column\n",
    "            create_table(doc, result['duplicates_master'].to_dict('records'), [key_column_master, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(\"No duplicate keys found in master table.\", level=2)\n",
    "\n",
    "        # Duplicate keys in target table with actual table name\n",
    "        if not result['duplicates_target'].empty:\n",
    "            count_dup_target = len(result['duplicates_target'])\n",
    "            doc.add_heading(f'Duplicate Keys in {table2_name} ({count_dup_target})', level=2)\n",
    "            # Modified to include 'Difference in value' column\n",
    "            create_table(doc, result['duplicates_target'].to_dict('records'), [key_column_target, 'Difference in value'])\n",
    "        else:\n",
    "            doc.add_heading(f\"No duplicate keys found in {table2_name}.\", level=2)\n",
    "\n",
    "        # Data type issues\n",
    "        if not result['data_type_issues'].empty:\n",
    "            count_data_type_issues = len(result['data_type_issues'])\n",
    "            doc.add_heading(f'Data Type Issues ({count_data_type_issues})', level=2)\n",
    "            column_names = ['column_name', f'{table1_name}_data_type', f'{table2_name}_data_type']\n",
    "            create_table(doc, result['data_type_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No data type issues found.\", level=2)\n",
    "\n",
    "        # Format issues in master table with target values\n",
    "        if not result['format_issues_master'].empty:\n",
    "            count_format_issues_master = len(result['format_issues_master'])\n",
    "            doc.add_heading(f'Format Issues in {table1_name} ({count_format_issues_master})', level=2)\n",
    "            column_names_master = [key_column_master, 'column', 'value', 'issue', f'{table2_name}_value']\n",
    "            create_table(doc, result['format_issues_master'].to_dict('records'), column_names_master)\n",
    "        else:\n",
    "            doc.add_heading(f\"No format issues found in {table1_name}.\", level=2)\n",
    "\n",
    "        # # Pincode Mapping Issues\n",
    "        # if not result['pincode_mapping_issues'].empty:\n",
    "        #     count_pincode_issues = len(result['pincode_mapping_issues'])\n",
    "        #     doc.add_heading(f'Pincode Mapping Issues in {table1_name} ({count_pincode_issues})', level=2)\n",
    "        #     column_names = [key_column_master, 'pincode', 'state', 'city', 'issue']\n",
    "        #     create_table(doc, result['pincode_mapping_issues'].to_dict('records'), column_names)\n",
    "        # else:\n",
    "        #     doc.add_heading(\"No pincode mapping issues found.\", level=2)\n",
    "\n",
    "        # Pincode Mapping Issues with target details\n",
    "        if not result['pincode_mapping_issues'].empty:\n",
    "            count_pincode_issues = len(result['pincode_mapping_issues'])\n",
    "            doc.add_heading(f'Pincode Mapping Issues in {table1_name} ({count_pincode_issues})', level=2)\n",
    "            column_names = [\n",
    "                key_column_master, 'pincode', 'state', 'city', 'issue',\n",
    "                f'{table2_name}_details'\n",
    "            ]\n",
    "            create_table(doc, result['pincode_mapping_issues'].to_dict('records'), column_names)\n",
    "        else:\n",
    "            doc.add_heading(\"No pincode mapping issues found.\", level=2)\n",
    "\n",
    "        \n",
    "\n",
    "        # Non-matching keys in master DataFrame\n",
    "        if result['df_master_only_keys']:\n",
    "            count_master_only = len(result['df_master_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table1_name} ({count_master_only})', level=2)\n",
    "            column_names = [key_column_master]\n",
    "            create_table(doc, result['df_master_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table1_name}.\", level=2)\n",
    "\n",
    "        # Non-matching keys in target DataFrame\n",
    "        if result['df_target_only_keys']:\n",
    "            count_target_only = len(result['df_target_only_keys'])\n",
    "            doc.add_heading(f'Keys only in {table2_name} ({count_target_only})', level=2)\n",
    "            column_names = [key_column_target]\n",
    "            create_table(doc, result['df_target_only_keys'], column_names)\n",
    "        else:\n",
    "            doc.add_heading(f\"No keys found only in {table2_name}.\", level=2)\n",
    "\n",
    "\n",
    "        doc.add_page_break()  # Optional: Add a page break between comparisons\n",
    "\n",
    "    # Save the aggregated document to the current directory\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_filename = f\"{base_name}_comparison_report_aggregated_{timestamp}.docx\"\n",
    "    doc.save(report_filename)\n",
    "    logging.info(f\"Saved aggregated comparison report as '{report_filename}'.\")\n",
    "\n",
    "    return report_filename  # Return the filename for further processing\n",
    "\n",
    "\n",
    "\n",
    "def send_slack_alert(message):\n",
    "    \"\"\"\n",
    "    Send a message to a specified Slack channel.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to send.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping Slack notification.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = slack_client.chat_postMessage(\n",
    "            channel=SLACK_CHANNEL,\n",
    "            text=message\n",
    "        )\n",
    "        logging.info(f\"Message sent to {SLACK_CHANNEL}: {response['ts']}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Error sending message to Slack: {e.response['error']}\")\n",
    "\n",
    "def upload_file_to_slack(filepath, title=None):\n",
    "    \"\"\"\n",
    "    Upload a file to the specified Slack channel using files_upload_v2.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the file to upload.\n",
    "        title (str, optional): The title for the uploaded file. Defaults to the file's basename.\n",
    "    \"\"\"\n",
    "    if not slack_client:\n",
    "        logging.warning(\"Slack client is not initialized. Skipping file upload.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            response = slack_client.files_upload_v2(\n",
    "                channel=SLACK_CHANNEL,\n",
    "                file=f,\n",
    "                filename=os.path.basename(filepath),  # Explicitly set the filename with extension\n",
    "                title=title if title else os.path.basename(filepath),  # Set the title\n",
    "                initial_comment=title if title else \"File uploaded.\"  # Optional: Add an initial comment\n",
    "            )\n",
    "\n",
    "        # Verify if the upload was successful\n",
    "        if response.get('ok'):\n",
    "            file_permalink = response['file']['permalink']\n",
    "            logging.info(f\"File uploaded to Slack channel '{SLACK_CHANNEL}': {file_permalink}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to upload file to Slack: {response}\")\n",
    "    except SlackApiError as e:\n",
    "        logging.error(f\"Slack API Error during file upload: {e.response['error']}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during file upload: {e}\")\n",
    "\n",
    "\n",
    "def find_non_matching_keys(df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table):\n",
    "    \"\"\"\n",
    "    Identify keys present in df_master but not in df_target and vice versa, including duplicates.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame.\n",
    "        df_target (pd.DataFrame): Target DataFrame.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        duplicates_master (pd.DataFrame): Duplicate keys in master table.\n",
    "        duplicates_target (pd.DataFrame): Duplicate keys in target table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (master_only_keys, target_only_keys)\n",
    "            - master_only_keys (list of dict): Keys present only in master DataFrame.\n",
    "            - target_only_keys (list of dict): Keys present only in target DataFrame.\n",
    "    \"\"\"\n",
    "    error_logs_m = []\n",
    "    # Include all keys, including duplicates\n",
    "    keys_master = set(df_master[master_key].astype(str).str.strip())\n",
    "    keys_target = set(df_target[target_key].astype(str).str.strip())\n",
    "\n",
    "    # Keys present only in master\n",
    "    master_only = keys_master - keys_target\n",
    "    # Keys present only in target\n",
    "    target_only = keys_target - keys_master\n",
    "\n",
    "    logging.info(f\"Found {len(master_only)} keys in source not in target and {len(target_only)} keys in target not in source.\")\n",
    "\n",
    "    # Convert to list of dictionaries for consistency\n",
    "    master_only_keys = [{master_key: key} for key in master_only]\n",
    "    target_only_keys = [{target_key: key} for key in target_only]\n",
    "        # Log errors for keys only in master\n",
    "    for key in master_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{master_key}' with value '{key}' is present only in '{master_table}' and missing in '{target_table}'.\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table, \n",
    "            'issue_column': master_key,\n",
    "            'unique_identifier': f\"{master_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "    # Log errors for keys only in target\n",
    "    for key in target_only:\n",
    "        error_detail = {\n",
    "            'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'missing_key',\n",
    "            'error_message': f\"Key '{target_key}' with value '{key}' is present only in '{target_table}' and missing in '{master_table}'.\",\n",
    "            'source_table': target_table,\n",
    "            'target_table': master_table,\n",
    "            'issue_column': target_key,\n",
    "            'unique_identifier': f\"{target_key}: {key}\"\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "\n",
    "\n",
    "    return master_only_keys, target_only_keys, error_logs_m\n",
    "\n",
    "\n",
    "\n",
    "def find_detailed_nulls(df_master, df_target, master_key, target_key, master_table, target_table, columns_to_check):\n",
    "    \"\"\"\n",
    "    Identify null values in both master and target tables and fetch corresponding values or indicate missing keys.\n",
    "\n",
    "    Args:\n",
    "        df_master (pd.DataFrame): Source DataFrame (master_hub_ table).\n",
    "        df_target (pd.DataFrame): Target DataFrame (prefixed table).\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "        master_table (str): Name of the master table.\n",
    "        target_table (str): Name of the target table.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (null_values_master, null_values_target)\n",
    "    \"\"\"\n",
    "    null_values_master = []\n",
    "    null_values_target = []\n",
    "    error_logs_m = []\n",
    "\n",
    "    # Find nulls in master\n",
    "    # null_master = df_master[df_master.isnull().any(axis=1)]\n",
    "    null_master = df_master[df_master[columns_to_check].isnull().any(axis=1)]\n",
    "    for idx, row in null_master.iterrows():\n",
    "        key_value = str(row[master_key]).strip()\n",
    "        for column in df_master.columns:\n",
    "            if column == master_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and columns starting with '_boltic_'\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_target[target_key].astype(str).str.strip().values:\n",
    "                    target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    target_value = target_row[column] if column in target_row else \"Column not present\"\n",
    "                else:\n",
    "                    target_value = f\"'{target_key}' not present\"\n",
    "                null_record = {\n",
    "                    master_key: key_value,\n",
    "                    'column': column,\n",
    "                    target_table: target_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'null',\n",
    "                'error_message': 'Null in columns',\n",
    "                'source_table': f'{master_table}',\n",
    "                'target_table': '',\n",
    "                'issue_column': column,\n",
    "                'unique_identifier': f'{master_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_master.append(null_record)\n",
    "                \n",
    "\n",
    "    # Find nulls in target\n",
    "    null_target = df_target[df_target[columns_to_check].isnull().any(axis=1)]\n",
    "    null_target = df_target[df_target.isnull().any(axis=1)]\n",
    "    for idx, row in null_target.iterrows():\n",
    "        key_value = str(row[target_key]).strip()\n",
    "        for column in df_target.columns:\n",
    "            if column == target_key or column.startswith('_boltic_'):\n",
    "                continue  # Skip key column and columns starting with '_boltic_'\n",
    "            if pd.isnull(row[column]):\n",
    "                if key_value in df_master[master_key].astype(str).str.strip().values:\n",
    "                    master_row = df_master[df_master[master_key].astype(str).str.strip() == key_value].iloc[0]\n",
    "                    master_value = master_row[column] if column in master_row else \"Column not present\"\n",
    "                else:\n",
    "                    master_value = f\"'{master_key}' not present\"\n",
    "                null_record = {\n",
    "                    target_key: key_value,\n",
    "                    'column': column,\n",
    "                    master_table: master_value\n",
    "                }\n",
    "                error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'null',\n",
    "                'error_message': 'Null in columns',\n",
    "                'source_table': f'{target_table}',\n",
    "                'target_table': '',\n",
    "                'issue_column': column,\n",
    "                'unique_identifier': f'{target_key} : {key_value}'\n",
    "                }\n",
    "                error_logs_m.append(error_detail)\n",
    "                null_values_target.append(null_record)\n",
    "                \n",
    "\n",
    "    logging.info(f\"Found {len(null_values_master)} null values in master table '{master_table}'.\")\n",
    "    logging.info(f\"Found {len(null_values_target)} null values in target table '{target_table}'.\")\n",
    "    return null_values_master, null_values_target, error_logs_m\n",
    "\n",
    "\n",
    "def validate_pincode_mapping(df_master, df_target, key_column, target_key, target_table, client, master_table):\n",
    "    \"\"\"\n",
    "    Validate pincode mapping by comparing with the all_india_PO_list reference table.\n",
    "    If a pincode issue is found in the master table, then check the corresponding pincode in the target table.\n",
    "    \n",
    "    Args:\n",
    "        df_master (pd.DataFrame): The master DataFrame to validate.\n",
    "        df_target (pd.DataFrame): The target DataFrame to fetch corresponding values.\n",
    "        key_column (str): The key column in the master DataFrame.\n",
    "        target_key (str): The key column in the target DataFrame.\n",
    "        target_table (str): The name of the target table.\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        master_table (str): Name of the master table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing pincode mapping issues with corresponding target table details.\n",
    "        list: List of error log dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    error_logs_m = []\n",
    "    # Read the reference table from Analytics dataset\n",
    "    try:\n",
    "        reference_table = \"all_india_po_list\"\n",
    "        reference_dataset = \"analytics_data\"\n",
    "        # reference_table = \"all_india_PO_list\"\n",
    "        # reference_dataset = \"Analytics\"\n",
    "        query = f\"SELECT pincode, city, state FROM `{PROJECT_ID}.{reference_dataset}.{reference_table}`\"\n",
    "        reference_df = client.query(query).to_dataframe()\n",
    "        reference_df['pincode'] = reference_df['pincode'].astype(str).str.strip()\n",
    "        reference_df['city'] = reference_df['city'].astype(str).str.strip().str.lower()\n",
    "        reference_df['state'] = reference_df['state'].astype(str).str.strip().str.lower()\n",
    "        logging.info(f\"Loaded reference pincode mapping from '{reference_table}' in '{reference_dataset}' dataset.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load reference pincode mapping: {e}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "    \n",
    "    # Check if df_master has 'pincode', 'city', 'state' columns\n",
    "    required_columns = {'pincode', 'city', 'state'}\n",
    "    if not required_columns.issubset(df_master.columns):\n",
    "        logging.info(f\"DataFrame does not have required columns for pincode mapping validation: {required_columns}\")\n",
    "        return pd.DataFrame(), error_logs_m\n",
    "    \n",
    "    # Initialize the issues DataFrame with a single target table details column\n",
    "    pincode_mapping_issues = pd.DataFrame(columns=[\n",
    "        key_column, 'pincode', 'state', 'city', 'issue',\n",
    "        f'{target_table}_details'\n",
    "    ])\n",
    "    \n",
    "    # Iterate over each row in df_master to validate pincode mapping\n",
    "    for idx, row in df_master.iterrows():\n",
    "        key_value = str(row[key_column]).strip()\n",
    "        pincode = str(row['pincode']).strip()\n",
    "        city = str(row['city']).strip().lower()\n",
    "        state = str(row['state']).strip().lower()\n",
    "        \n",
    "        # Fetch corresponding target row if exists\n",
    "        target_row = df_target[df_target[target_key].astype(str).str.strip() == key_value]\n",
    "        if not target_row.empty:\n",
    "            target_row = target_row.iloc[0]\n",
    "            target_pincode = target_row['pincode'] if 'pincode' in target_row and pd.notnull(target_row['pincode']) else \"Pincode missing\"\n",
    "            target_state = target_row['state'] if 'state' in target_row and pd.notnull(target_row['state']) else \"State missing\"\n",
    "            target_city = target_row['city'] if 'city' in target_row and pd.notnull(target_row['city']) else \"City missing\"\n",
    "            target_details = f\"Pincode: {target_pincode}, State: {target_state}, City: {target_city}\"\n",
    "        else:\n",
    "            target_details = f\"Key '{key_column}' with value '{key_value}' not present in target table '{target_table}'.\"\n",
    "        \n",
    "        # Check if pincode exists in reference\n",
    "        ref_matches = reference_df[reference_df['pincode'] == pincode]\n",
    "        if ref_matches.empty:\n",
    "            issue = f\"Invalid pincode ({pincode}).\"\n",
    "            pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "                key_column: key_value,\n",
    "                'pincode': pincode,\n",
    "                'state': state,\n",
    "                'city': city,\n",
    "                'issue': issue,\n",
    "                f'{target_table}_details': target_details\n",
    "            }])], ignore_index=True)\n",
    "            error_detail = {\n",
    "                'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'issue': 'pincode_mapping',\n",
    "                'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "                'source_table': master_table,\n",
    "                'target_table': target_table,\n",
    "                'issue_column': 'pincode',\n",
    "                'unique_identifier': f'{key_column}: {key_value}'\n",
    "            }\n",
    "            error_logs_m.append(error_detail)\n",
    "            continue\n",
    "        \n",
    "        # Check if any of the reference entries match both the city and state\n",
    "        exact_match = ref_matches[\n",
    "            (ref_matches['city'] == city) & (ref_matches['state'] == state)\n",
    "        ]\n",
    "        if not exact_match.empty:\n",
    "            continue  # No issue, mapping is correct\n",
    "        \n",
    "        # Check for state mismatch\n",
    "        state_matches = ref_matches[ref_matches['state'] == state]\n",
    "        \n",
    "        # Check for city mismatch\n",
    "        city_matches = ref_matches[ref_matches['city'] == city]\n",
    "        \n",
    "        if state_matches.empty and city_matches.empty:\n",
    "            # Both state and city do not match\n",
    "            expected_entries = ref_matches[['state', 'city']].drop_duplicates()\n",
    "            expected_states = expected_entries['state'].tolist()\n",
    "            expected_cities = expected_entries['city'].tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}' and city '{city}'. Expected states: {expected_states_str}; Expected cities: {expected_cities_str}.\"\n",
    "        elif state_matches.empty:\n",
    "            # State does not match\n",
    "            expected_states = ref_matches['state'].unique().tolist()\n",
    "            expected_states_str = ', '.join(expected_states)\n",
    "            issue = f\"Pincode {pincode} does not match state '{state}'. Expected states: {expected_states_str}.\"\n",
    "        elif city_matches.empty:\n",
    "            # City does not match\n",
    "            expected_cities = state_matches['city'].unique().tolist()\n",
    "            expected_cities_str = ', '.join(expected_cities)\n",
    "            issue = f\"Pincode {pincode} does not match city '{city}'. Expected cities: {expected_cities_str}.\"\n",
    "        else:\n",
    "            # Other cases\n",
    "            issue = f\"Pincode {pincode} has a mapping inconsistency.\"\n",
    "        \n",
    "        pincode_mapping_issues = pd.concat([pincode_mapping_issues, pd.DataFrame([{\n",
    "            key_column: key_value,\n",
    "            'pincode': pincode,\n",
    "            'state': state,\n",
    "            'city': city,\n",
    "            'issue': issue,\n",
    "            f'{target_table}_details': target_details\n",
    "        }])], ignore_index=True)\n",
    "        error_detail = {\n",
    "            'time_stamp': now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'issue': 'pincode_mapping',\n",
    "            'error_message': f\"{issue}. {target_table} Details: {target_details}\",\n",
    "            'source_table': master_table,\n",
    "            'target_table': target_table,\n",
    "            'issue_column': 'pincode',\n",
    "            'unique_identifier': f'{key_column}: {key_value}'\n",
    "        }\n",
    "        error_logs_m.append(error_detail)\n",
    "    \n",
    "    logging.info(f\"Found {len(pincode_mapping_issues)} pincode mapping issues in master table '{master_table}'.\")\n",
    "    return pincode_mapping_issues, error_logs_m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compare_tables(client, dataset_name, base_name, master_table, target_table, master_key, target_key):\n",
    "    \"\"\"\n",
    "    Compare two tables and generate a report.\n",
    "\n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        base_name (str): The base name of the table.\n",
    "        master_table (str): Name of the master_hub_ table.\n",
    "        target_table (str): Name of the target prefixed table.\n",
    "        master_key (str): The key column in the master table.\n",
    "        target_key (str): The key column in the target table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing all comparison results.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting comparison for base table '{base_name}': '{master_table}' vs '{target_table}'.\")\n",
    "\n",
    "    # Initialize comparison results\n",
    "    mismatches = []\n",
    "    null_values_master = []\n",
    "    null_values_target = []\n",
    "    data_type_issues = pd.DataFrame()\n",
    "    format_issues_master = pd.DataFrame()\n",
    "    pincode_mapping_issues = pd.DataFrame()\n",
    "    duplicates_master = pd.DataFrame()\n",
    "    duplicates_target = pd.DataFrame()\n",
    "    master_only_keys = []\n",
    "    target_only_keys = []\n",
    "\n",
    "    # Load data\n",
    "    df_master = load_table_from_bigquery(client, dataset_name, master_table)\n",
    "    df_target = load_table_from_bigquery(client, dataset_name, target_table)\n",
    "\n",
    "    # # **Apply standardization to handle case insensitivity**\n",
    "    # df_master = standardize_dataframe(df_master, exclude_columns=[master_key])\n",
    "    # df_target = standardize_dataframe(df_target, exclude_columns=[target_key])\n",
    "\n",
    "\n",
    "    # **Apply configurable active filter if defined**\n",
    "    base_table_info = BASE_TABLES.get(base_name, {})\n",
    "    active_filter = base_table_info.get('active_filter')\n",
    "    # Determine whether to perform additional checks\n",
    "    perform_checks = base_table_info.get('perform_checks', True)\n",
    "\n",
    "    if active_filter:\n",
    "        column = active_filter.get('column')\n",
    "        value = active_filter.get('value')\n",
    "        if column and column in df_master.columns:\n",
    "            initial_count = len(df_master)\n",
    "            df_master = df_master[df_master[column] == value]\n",
    "            filtered_count = len(df_master)\n",
    "            logging.info(f\"Filtered '{base_name}' master table: {initial_count - filtered_count} records excluded based on {column} = {value}.\")\n",
    "        else:\n",
    "            logging.warning(f\"Active filter specified but column '{column}' not found in master table '{master_table}'.\")\n",
    "\n",
    "\n",
    "    if df_master.empty or df_target.empty:\n",
    "        logging.warning(f\"One of the tables '{master_table}' or '{target_table}' is empty. Skipping comparison.\")\n",
    "        return None\n",
    "\n",
    "    # Identify BigNumeric columns in master and target tables\n",
    "    schema_master = get_table_schema(client, dataset_name, master_table)\n",
    "    schema_target = get_table_schema(client, dataset_name, target_table)\n",
    "    bignumeric_columns_master = [col for col, dtype in schema_master.items() if dtype == 'BIGNUMERIC']\n",
    "    bignumeric_columns_target = [col for col, dtype in schema_target.items() if dtype == 'BIGNUMERIC']\n",
    "\n",
    "    # Format BigNumeric columns in master table\n",
    "    for col in bignumeric_columns_master:\n",
    "        if col in df_master.columns:\n",
    "            df_master[col] = df_master[col].apply(lambda x: format(x, '.0f') if pd.notnull(x) else x)\n",
    "\n",
    "    # Format BigNumeric columns in target table\n",
    "    for col in bignumeric_columns_target:\n",
    "        if col in df_target.columns:\n",
    "            df_target[col] = df_target[col].apply(lambda x: format(x, '.0f') if pd.notnull(x) else x)\n",
    "\n",
    "    # Identify common columns\n",
    "    common_columns, master_unique_cols, target_unique_cols = find_common_and_non_common_columns(df_master, df_target)\n",
    "\n",
    "\n",
    "    # Get imp_columns and non_imp_columns\n",
    "    imp_columns = Imp_columns.get(base_name, None)\n",
    "    non_imp_columns = Non_imp_columns.get(base_name, [])\n",
    "\n",
    "    # Determine columns to check based on imp_columns\n",
    "    if imp_columns:\n",
    "        columns_to_check = [col for col in imp_columns if col in common_columns]\n",
    "        logging.info(f\"Important columns defined for '{base_name}': {columns_to_check}\")\n",
    "    else:\n",
    "        columns_to_check = [col for col in common_columns if col not in non_imp_columns]\n",
    "        logging.info(f\"No important columns defined for '{base_name}'. Applying checks to all columns except non_imp_columns: {columns_to_check}\")\n",
    "    \n",
    "    \n",
    "    if perform_checks:\n",
    "        # Find duplicates in both tables\n",
    "        duplicates_master, error_logs_m = find_duplicates(df_master, master_key, master_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "        duplicates_target,  error_logs_m = find_duplicates(df_target, target_key, target_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    if not duplicates_master.empty:\n",
    "        logging.warning(f\"Duplicate keys found in source table '{master_table}'. These will be reported but not used in mismatch comparison.\")\n",
    "    if not duplicates_target.empty:\n",
    "        logging.warning(f\"Duplicate keys found in target table '{target_table}'. These will be reported but not used in mismatch comparison.\")\n",
    "\n",
    "    # # Identify common columns\n",
    "    # common_columns, master_unique_cols, target_unique_cols = find_common_and_non_common_columns(df_master, df_target)\n",
    "\n",
    "    if not common_columns:\n",
    "        logging.warning(f\"No common columns found between '{master_table}' and '{target_table}'. Skipping comparison.\")\n",
    "        return None\n",
    "    \n",
    "    # Retrieve non-important columns for the current base table\n",
    "    non_imp_columns = Non_imp_columns.get(base_name, [])\n",
    "\n",
    "    # print(non_imp_columns)\n",
    "    \n",
    "    # Perform mismatch comparison if allowed\n",
    "    if perform_checks:\n",
    "    # Find mismatches excluding duplicate keys\n",
    "        mismatches, error_logs_m = find_mismatches(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            columns_to_check,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            duplicates_master,\n",
    "            duplicates_target,\n",
    "            non_imp_columns\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "    # Find detailed null values in both tables if allowed\n",
    "    if perform_checks:    \n",
    "        # Find detailed null values in both tables\n",
    "        null_values_master, null_values_target, error_logs_m = find_detailed_nulls(\n",
    "            df_master,\n",
    "            df_target,\n",
    "            master_key,\n",
    "            target_key,\n",
    "            master_table,\n",
    "            target_table,\n",
    "            columns_to_check\n",
    "        )\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    if perform_checks:    \n",
    "        # Validate data types between master and target schemas\n",
    "        data_type_issues, error_logs_m = validate_data_types(schema_master, schema_target, master_key, master_table, target_table, columns_to_check)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    if perform_checks:        \n",
    "        # Validate formats in master table only and include target values\n",
    "        format_issues_master, error_logs_m = validate_formats(df_master, df_target, master_key, target_key, target_table, master_table)\n",
    "        ERROR_LOG_M.extend(error_logs_m)\n",
    "        \n",
    "        # # Validate pincode mapping if applicable\n",
    "        # pincode_mapping_issues = pd.DataFrame()\n",
    "        # if {'pincode', 'city', 'state'}.issubset(df_master.columns):\n",
    "        #     pincode_mapping_issues = validate_pincode_mapping(df_master, master_key, client)\n",
    "    \n",
    "        # Validate pincode mapping if applicable and include target values\n",
    "\n",
    "    if perform_checks:        \n",
    "        pincode_mapping_issues = pd.DataFrame()\n",
    "        if {'pincode', 'city', 'state'}.issubset(df_master.columns):\n",
    "            pincode_mapping_issues, error_logs_m = validate_pincode_mapping(\n",
    "                df_master, \n",
    "                df_target, \n",
    "                master_key, \n",
    "                target_key, \n",
    "                target_table, \n",
    "                client, master_table\n",
    "            )\n",
    "            ERROR_LOG_M.extend(error_logs_m)\n",
    "    \n",
    "        # if {'pincode', 'city', 'state'}.issubset(df_target.columns):\n",
    "        # # **Corrected Call: Swap DataFrames and Keys**\n",
    "        #     pincode_mapping_issues_target, error_logs_m = validate_pincode_mapping(\n",
    "        #         df_target,       # df_master becomes target DataFrame\n",
    "        #         df_master,       # df_target becomes master DataFrame\n",
    "        #         target_key,      # key_column is target_key\n",
    "        #         master_key,      # target_key is master_key\n",
    "        #         master_table,    # target_table is master_table\n",
    "        #         client, \n",
    "        #         target_table\n",
    "        #     )\n",
    "        #     pincode_mapping_issues = pd.concat([pincode_mapping_issues, pincode_mapping_issues_target], ignore_index=True)\n",
    "        #     ERROR_LOG_M.extend(error_logs_m)\n",
    "    \n",
    "           \n",
    "    # Find non-matching keys\n",
    "    master_only_keys, target_only_keys, error_logs_m = find_non_matching_keys(\n",
    "        df_master, df_target, master_key, target_key, duplicates_master, duplicates_target, master_table, target_table\n",
    "    )\n",
    "    ERROR_LOG_M.extend(error_logs_m)\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        'mismatches': mismatches,\n",
    "        'null_values_master': pd.DataFrame(null_values_master),\n",
    "        'null_values_target': pd.DataFrame(null_values_target),\n",
    "        'duplicates_master': duplicates_master,\n",
    "        'duplicates_target': duplicates_target,\n",
    "        'data_type_issues': data_type_issues,\n",
    "        'format_issues_master': format_issues_master,\n",
    "        'pincode_mapping_issues': pincode_mapping_issues,\n",
    "        'key_column_master': master_key,\n",
    "        'key_column_target': target_key,\n",
    "        'df_master_only_keys': master_only_keys,\n",
    "        'df_target_only_keys': target_only_keys,\n",
    "        'table1_name': master_table,\n",
    "        'table2_name': target_table,\n",
    "        'df_master': df_master,\n",
    "        'df_target': df_target\n",
    "    }\n",
    "\n",
    "    logging.info(f\"Completed comparison for '{master_table}' vs '{target_table}'.\")\n",
    "    return results\n",
    "\n",
    "def generate_string_schema(df):\n",
    "    \"\"\"\n",
    "    Generates a BigQuery schema with all fields as STRING.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame for which to generate the schema.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of SchemaField objects with type STRING.\n",
    "    \"\"\"\n",
    "    schema = [SchemaField(column, \"STRING\", mode=\"NULLABLE\") for column in df.columns]\n",
    "    return schema\n",
    "\n",
    "\n",
    "def _upload_dataframe_to_bigquery(client, analytics_dataset, table_name, df):\n",
    "    \"\"\"\n",
    "    Helper function to upload a DataFrame to BigQuery.\n",
    "    \n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        analytics_dataset (str): The name of the Analytics dataset.\n",
    "        table_name (str): The name of the table to upload.\n",
    "        df (pd.DataFrame): The DataFrame to upload.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logging.info(f\"No data to upload for '{table_name}'. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    # Convert all columns to string type\n",
    "    df = df.astype(str)\n",
    "    \n",
    "    # Generate BigQuery schema with all fields as STRING\n",
    "    schema = generate_string_schema(df)\n",
    "    \n",
    "    # Ensure table name doesn't exceed BigQuery's maximum length (1,024 characters)\n",
    "    if len(table_name) > 1024:\n",
    "        original_table_name = table_name\n",
    "        table_name = table_name[:1021] + '...'\n",
    "        logging.warning(f\"Table name truncated from '{original_table_name}' to '{table_name}' due to length constraints.\")\n",
    "    \n",
    "    # Define the full table ID\n",
    "    table_id = f\"{client.project}.{analytics_dataset}.{table_name}\"\n",
    "    \n",
    "    # Upload the DataFrame to BigQuery\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(\n",
    "            df,\n",
    "            table_id,\n",
    "            job_config=bigquery.LoadJobConfig(\n",
    "                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "                schema=schema  # Using the provided schema with all fields as STRING\n",
    "            )\n",
    "        )\n",
    "        job.result()  # Wait for the job to complete\n",
    "        logging.info(f\"Successfully uploaded '{table_id}' with {len(df)} records.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to upload '{table_id}' to BigQuery: {e}\")\n",
    "\n",
    "def upload_comparison_results_to_bigquery(client, analytics_dataset, ERROR_LOG_M):\n",
    "    \"\"\"\n",
    "    Uploads each part of the comparison_result to BigQuery as separate tables in the Analytics dataset.\n",
    "    The table names follow the format: 'table1_name_vs_table2_name_heading'.\n",
    "    \n",
    "    Args:\n",
    "        client (bigquery.Client): Initialized BigQuery client.\n",
    "        analytics_dataset (str): The name of the Analytics dataset.\n",
    "        table1_name (str): Name of the first table (e.g., 'master_hub_brand').\n",
    "        table2_name (str): Name of the second table (e.g., 'procuro_brand').\n",
    "        comparison_result (dict): The dictionary containing comparison results.\n",
    "        ERROR_LOG: The error log data, either a DataFrame or a list.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle ERROR_LOG separately\n",
    "    if ERROR_LOG_M is not None:\n",
    "        # Determine the DataFrame to upload\n",
    "        if isinstance(ERROR_LOG_M, pd.DataFrame):\n",
    "            error_df = ERROR_LOG_M\n",
    "        elif isinstance(ERROR_LOG_M, list):\n",
    "            error_df = pd.DataFrame(ERROR_LOG_M)\n",
    "        else:\n",
    "            logging.warning(\"Unsupported data type for ERROR_LOG. Skipping upload.\")\n",
    "            error_df = None\n",
    "        \n",
    "        if error_df is not None:\n",
    "            _upload_dataframe_to_bigquery(client, analytics_dataset, \"error_logs\", error_df)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the comparison of multiple base tables against their master_hub_ counterparts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize BigQuery client\n",
    "        try:\n",
    "            client = get_bigquery_client(PROJECT_ID)\n",
    "        except Exception:\n",
    "            logging.error(\"Exiting due to BigQuery client initialization failure.\")\n",
    "            return\n",
    "\n",
    "        # Find common tables with 'master_hub_' and other prefixes, passing BASE_TABLES\n",
    "        common_tables = find_common_tables_with_master_hub(client, DATASET_ID, PREFIXES, BASE_TABLES)\n",
    "        \n",
    "        if not common_tables:\n",
    "            logging.info(\"No common tables found with 'master_hub_' and the specified prefixes.\")\n",
    "            return\n",
    "\n",
    "        # Iterate over each base table and perform comparisons\n",
    "        for base_name, tables in common_tables.items():\n",
    "            base_table_info = BASE_TABLES.get(base_name)\n",
    "            if not base_table_info:\n",
    "                logging.warning(f\"No configuration found for base table '{base_name}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            master_key = base_table_info.get('master_key')\n",
    "            target_tables = base_table_info.get('targets', {})\n",
    "            \n",
    "            master_table = tables.get('master_hub_')\n",
    "            if not master_table:\n",
    "                logging.warning(f\"Master table 'master_hub_{base_name}' not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            all_results = []\n",
    "            \n",
    "            # Iterate through each prefix and its corresponding target_key\n",
    "            for prefix, target_key in target_tables.items():\n",
    "                target_table = tables.get(prefix)\n",
    "                if not target_table:\n",
    "                    logging.warning(f\"Target table with prefix '{prefix}' for base table '{base_name}' not found. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                comparison_result = compare_tables(\n",
    "                    client, \n",
    "                    DATASET_ID, \n",
    "                    base_name, \n",
    "                    master_table, \n",
    "                    target_table, \n",
    "                    master_key, \n",
    "                    target_key  # Pass the correct target_key per prefix\n",
    "                )\n",
    "                if comparison_result:\n",
    "                    all_results.append(comparison_result)\n",
    "                    \n",
    "                    # Prepare and send a separate Slack message for each comparison\n",
    "                    total_mismatches = len(comparison_result['mismatches'])\n",
    "                    total_nulls_master = len(comparison_result['null_values_master'])\n",
    "                    total_nulls_target = len(comparison_result['null_values_target'])\n",
    "                    total_dup_master = len(comparison_result['duplicates_master'])\n",
    "                    total_dup_target = len(comparison_result['duplicates_target'])\n",
    "                    total_data_type_issues = len(comparison_result['data_type_issues'])\n",
    "                    total_format_issues_master = len(comparison_result['format_issues_master'])\n",
    "                    total_pincode_issues = len(comparison_result['pincode_mapping_issues'])\n",
    "                    total_non_matching_source = len(comparison_result.get('df_master_only_keys', []))\n",
    "                    total_non_matching_target = len(comparison_result.get('df_target_only_keys', []))\n",
    "                    \n",
    "                    message = (\n",
    "                        f\" *Comparison Report Generated for `{base_name}`*\\n\"\n",
    "                        f\"*Tables Compared: `{comparison_result['table1_name']}` vs `{comparison_result['table2_name']}`*\\n\"\n",
    "                        f\"- *Total Mismatches between values of same column name of both tables : `{total_mismatches}`*\\n\"\n",
    "                        f\"- *Total Null Values in `{comparison_result['table1_name']}`: `{total_nulls_master}`*\\n\"\n",
    "                        f\"- *Total Null Values in `{comparison_result['table2_name']}`: `{total_nulls_target}`*\\n\"\n",
    "                        f\"- *Duplicate `{master_key}` in `{comparison_result['table1_name']}`: `{total_dup_master}`*\\n\"\n",
    "                        f\"- *Duplicate `{target_key}` in `{comparison_result['table2_name']}`: `{total_dup_target}`*\\n\"\n",
    "                        f\"- *Total Data Type Issues(mismatch between datatype in columns with same name of both tables): `{total_data_type_issues}`*\\n\"\n",
    "                        f\"- *Total Format/Value Issues(gstin, email, pincode) in `{comparison_result['table1_name']}`: `{total_format_issues_master}`*\\n\"\n",
    "                        f\"- *Total Pincode Mapping Issues in `{comparison_result['table1_name']}`: `{total_pincode_issues}`*\\n\"\n",
    "                         \"- *Non-Matching Keys*:\\n\"\n",
    "                        f\"--*`{master_key}` only in `{comparison_result['table1_name']}` and not in `{comparison_result['table2_name']}`:`{total_non_matching_source}`,*\\n\"\n",
    "                        f\"--*`{target_key}` only in `{comparison_result['table2_name']}` and not in `{comparison_result['table1_name']}`:`{total_non_matching_target}`*\"\n",
    "                    )\n",
    "                \n",
    "                    send_slack_alert(message)\n",
    "            \n",
    "            if all_results:\n",
    "                # Generate aggregated report for the base name and get the filepath\n",
    "                report_filepath = create_aggregated_document(all_results, base_name)\n",
    "                \n",
    "                # Upload the report to Slack using the updated function\n",
    "                upload_file_to_slack(report_filepath, title=f\"{base_name.capitalize()} Comparison Report\")\n",
    "                \n",
    "                # Remove the local report file after successful upload\n",
    "                try:\n",
    "                    os.remove(report_filepath)\n",
    "                    logging.info(f\"Removed local report file '{report_filepath}'.\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to remove local report file '{report_filepath}': {e}\")\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                logging.info(f\"No comparison results to report for base name '{base_name}'.\")\n",
    "        \n",
    "        # # Upload error logs to BigQuery after all comparisons\n",
    "        # upload_comparison_results_to_bigquery(\n",
    "        #     client, \n",
    "        #     'Analytics',\n",
    "        #     ERROR_LOG_M\n",
    "        #     )\n",
    "        upload_comparison_results_to_bigquery(\n",
    "            client, \n",
    "            'analytics_data',\n",
    "            ERROR_LOG_M\n",
    "            )\n",
    "        \n",
    "\n",
    "        logging.info(\"All comparisons completed.\")\n",
    "    except Exception as e:\n",
    "        # Capture the full traceback\n",
    "        tb = traceback.format_exc()\n",
    "        logging.error(\"An unexpected error occurred in the main process.\", exc_info=True)\n",
    "        \n",
    "        # Prepare a detailed error message for Slack\n",
    "        error_message = (\n",
    "            f\" *Comparison Process Failed*\\n\"\n",
    "            f\"*Error:* {str(e)}\\n\"\n",
    "            f\"*Traceback:*\\n```{tb}```\"\n",
    "        )\n",
    "        send_slack_alert(error_message)\n",
    "\n",
    "        # Optionally, exit the script with a non-zero status\n",
    "        sys.exit(1)\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2458ad-da08-4e4f-a2f3-a2f4ab5a8e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
